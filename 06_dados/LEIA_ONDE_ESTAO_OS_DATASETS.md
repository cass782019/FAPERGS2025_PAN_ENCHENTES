# üì• ONDE EST√ÉO OS DATASETS

## ‚ö†Ô∏è IMPORTANTE

Os arquivos CSV (datasets) **N√ÉO est√£o inclu√≠dos** neste reposit√≥rio GitHub devido ao tamanho total superior a 1 GB.

## üìÇ Download dos Dados

### Google Drive (Principal)

**Link de acesso:**
üëâ [https://drive.google.com/drive/folders/1j7OiuMJuQ8tu7trlZJ4Zbo5Attu01knM?usp=drive_link](https://drive.google.com/drive/folders/1j7OiuMJuQ8tu7trlZJ4Zbo5Attu01knM?usp=drive_link)

### Arquivos Dispon√≠veis

O Google Drive cont√©m todos os datasets processados necess√°rios para reproduzir o pipeline:

#### Pasta `processados/`
- ‚úÖ `6_empresas_rs_porte_sobreviveu_pandemia_enchente.csv` (~250 MB)
- ‚úÖ `7_dados_unidos.csv` (~50 MB)
- ‚úÖ `dataset_unificado.csv` (~600 MB)
- ‚úÖ `dataset_com_posts.csv` (~1 MB)
- ‚úÖ `dataset_sem_posts.csv` (~200 MB)

#### Pasta `amostras/`
- ‚úÖ `amostra_estabelecimentos_rs.csv` (~5 MB)

**Total:** ~1.1 GB

## üìã Como Usar

### 1. Baixar os Arquivos

1. Acesse o link do Google Drive acima
2. Fa√ßa download de todos os arquivos
3. Organize-os na estrutura abaixo

### 2. Estrutura de Pastas

Ap√≥s o download, organize os arquivos assim:

```
0_artigo/
‚îî‚îÄ‚îÄ 06_dados/
    ‚îú‚îÄ‚îÄ processados/
    ‚îÇ   ‚îú‚îÄ‚îÄ 6_empresas_rs_porte_sobreviveu_pandemia_enchente.csv
    ‚îÇ   ‚îú‚îÄ‚îÄ 7_dados_unidos.csv
    ‚îÇ   ‚îú‚îÄ‚îÄ dataset_unificado.csv
    ‚îÇ   ‚îú‚îÄ‚îÄ dataset_com_posts.csv
    ‚îÇ   ‚îî‚îÄ‚îÄ dataset_sem_posts.csv
    ‚îî‚îÄ‚îÄ amostras/
        ‚îî‚îÄ‚îÄ amostra_estabelecimentos_rs.csv
```

### 3. Verificar Integridade

Execute este script Python para verificar se todos os arquivos foram baixados corretamente:

```python
import pandas as pd
import os

arquivos = {
    '06_dados/processados/6_empresas_rs_porte_sobreviveu_pandemia_enchente.csv': 2685868,
    '06_dados/processados/7_dados_unidos.csv': None,  # Vari√°vel
    '06_dados/processados/dataset_unificado.csv': 2685868,
    '06_dados/processados/dataset_com_posts.csv': 2638,
    '06_dados/processados/dataset_sem_posts.csv': 2683230,
    '06_dados/amostras/amostra_estabelecimentos_rs.csv': 10001
}

print("Verificando integridade dos datasets...\n")
tudo_ok = True

for arquivo, linhas_esperadas in arquivos.items():
    if os.path.exists(arquivo):
        try:
            df = pd.read_csv(arquivo)
            tamanho = len(df)
            tamanho_mb = os.path.getsize(arquivo) / (1024 * 1024)
            
            status = "‚úÖ"
            if linhas_esperadas and abs(tamanho - linhas_esperadas) > 10:
                status = "‚ö†Ô∏è"
                tudo_ok = False
            
            print(f"{status} {arquivo}")
            print(f"   Linhas: {tamanho:,} | Tamanho: {tamanho_mb:.1f} MB")
        except Exception as e:
            print(f"‚ùå {arquivo}: ERRO ao ler - {e}")
            tudo_ok = False
    else:
        print(f"‚ùå {arquivo}: ARQUIVO N√ÉO ENCONTRADO")
        tudo_ok = False
    print()

if tudo_ok:
    print("üéâ Todos os datasets est√£o OK! Voc√™ pode prosseguir com o pipeline.")
else:
    print("‚ö†Ô∏è Alguns problemas foram encontrados. Verifique os arquivos.")
```

## üìä Descri√ß√£o dos Datasets

### 6_empresas_rs_porte_sobreviveu_pandemia_enchente.csv

**Descri√ß√£o:** Dataset principal com dados de empresas do RS e targets de sobreviv√™ncia

**Linhas:** 2.685.868  
**Colunas:** 20  
**Tamanho:** ~250 MB

**Principais colunas:**
- `cnpj_basico`: Identificador √∫nico (8 d√≠gitos)
- `porte`: MEI, MICRO, PEQUENA, MEDIA, GRANDE
- `sobreviveu_pandemia`: Target 1 (1/0)
- `sobreviveu_enchente`: Target 2 (1/0)
- `idade_empresa_anos`: Feature temporal
- `situacao_cadastral`: ATIVA, BAIXADA, etc.

### 7_dados_unidos.csv

**Descri√ß√£o:** Posts do Instagram de empresas do RS

**Empresas √∫nicas:** 2.638  
**Tamanho:** ~50 MB

**Principais colunas:**
- `cnpj`: CNPJ da empresa
- `followers_count`: Seguidores
- `like_count`: Curtidas
- `caption`: Texto do post
- `timestamp`: Data/hora

### dataset_unificado.csv

**Descri√ß√£o:** Dataset final combinando dados de empresas + features de posts agregadas

**Linhas:** 2.685.868  
**Colunas:** 43  
**Tamanho:** ~600 MB

**Uso:** Entrada principal para modelagem (Etapa 5)

### dataset_com_posts.csv / dataset_sem_posts.csv

**Descri√ß√£o:** Subsets para estrat√©gia "separate"

**Com posts:** 2.638 empresas  
**Sem posts:** 2.683.230 empresas

## üîê Licen√ßa e Uso dos Dados

### Dados da Receita Federal
- **Fonte:** Cadastro Nacional da Pessoa Jur√≠dica (CNPJ) - Dados P√∫blicos
- **Licen√ßa:** Uso livre, dados p√∫blicos
- **URL:** https://www.gov.br/receitafederal/dados-abertos

### Dados do Instagram
- **Anonimiza√ß√£o:** CNPJs mantidos, conte√∫do de posts an√¥nimo
- **Uso:** Apenas para pesquisa acad√™mica e cient√≠fica
- **Restri√ß√µes:** N√£o redistribuir comercialmente

## üìß Suporte

### Problemas com Download?

- **Link n√£o funciona:** Abra uma issue no GitHub
- **Arquivos corrompidos:** Tente baixar novamente
- **Acesso negado:** Verifique se o link est√° correto

### Perguntas?

- **GitHub Issues:** [https://github.com/cass782019/FAPERGS2025_PAN_ENCHENTES/issues](https://github.com/cass782019/FAPERGS2025_PAN_ENCHENTES/issues)
- **Documenta√ß√£o completa:** `06_dados/README_DADOS.md`

## ‚úÖ Checklist

Antes de executar o pipeline, confirme:

- [ ] Baixei todos os 6 arquivos CSV do Google Drive
- [ ] Organizei nas pastas `processados/` e `amostras/`
- [ ] Executei o script de verifica√ß√£o de integridade
- [ ] Li a documenta√ß√£o em `README_DADOS.md`
- [ ] Instalei as depend√™ncias (`08_codigo/requirements.txt`)

## üöÄ Pr√≥ximos Passos

Ap√≥s baixar os dados:

1. **Verificar integridade** (script acima)
2. **Ler documenta√ß√£o:** `06_dados/README_DADOS.md`
3. **Instalar depend√™ncias:** `pip install -r 08_codigo/requirements.txt`
4. **Executar pipeline:** Seguir `09_anexos/como_reproduzir.md`

---

**√öltima atualiza√ß√£o:** Dezembro 2024  
**Tamanho total dos dados:** ~1.1 GB  
**Tempo estimado de download:** 5-15 minutos (depende da conex√£o)

