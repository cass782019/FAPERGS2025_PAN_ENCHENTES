# 1.5 Etapa 4: Pipeline ML Base

## Visão Geral

O Pipeline ML Base processa dados de posts do Instagram, cria features agregadas por empresa e combina com dados administrativos. Esta etapa prepara os datasets finais para modelagem.

## Objetivos

1. Processar dados de posts do Instagram
2. Criar features agregadas por CNPJ
3. Combinar dados de posts com dados de empresas
4. Implementar estratégias de modelagem (unified/separate/hybrid)
5. Gerar datasets prontos para otimização

## Arquivo de Implementação

**Notebook:** `08_codigo/notebooks/4.1.ipynb`

**Saídas:**
- `06_dados/processados/dataset_unificado.csv`
- `06_dados/processados/dataset_com_posts.csv`
- `06_dados/processados/dataset_sem_posts.csv`

## Metodologia

### 1. Features de Posts do Instagram

**Features Derivadas:**
```python
# Comprimento de captions
df_posts['caption_length'] = df_posts['caption'].str.len()
df_posts['caption_words'] = df_posts['caption'].str.split().str.len()

# Taxa de engajamento
df_posts['engagement_rate'] = (
    df_posts['like_count'] / df_posts['followers_count'] * 100
)
```

**Agregações por CNPJ:**
```python
features_posts = df_posts.groupby('cnpj_basico').agg({
    'followers_count': ['mean', 'max', 'min', 'std'],
    'like_count': ['sum', 'mean', 'median', 'std', 'max'],
    'engagement_rate': ['mean', 'median', 'std', 'max'],
    'caption_length': ['mean', 'median', 'std', 'max'],
    'post_id': 'count'  # total_posts
})
```

### 2. Combinação de Datasets

**Merge Left Join:**
```python
df_final = pd.merge(
    df_empresas,
    features_posts,
    on='cnpj_basico',
    how='left'
)
```

**Resultado:**
- Empresas com posts: 2.638 (features completas)
- Empresas sem posts: 2.683.230 (features de posts = NaN)
- Total features: 40+

### 3. Estratégias de Modelagem

#### Unified (Unificado)
- Dataset único
- Imputação de NaNs com SimpleImputer
- Modelo único para todas empresas

#### Separate (Separado)
- Dataset COM posts
- Dataset SEM posts
- Modelos especializados

#### Hybrid (Híbrido) ⭐ RECOMENDADO
- Gera os 3 datasets
- Permite ensemble
- Melhor para desbalanceamento extremo

### 4. Pré-processamento

**Imputação:**
```python
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X)
```

**Normalização:**
```python
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imputed)
```

**Encoding:**
```python
# Categóricas
for col in categorical_cols:
    df[col] = pd.Categorical(df[col]).codes
```

## Processamento Escalável

**Chunks para Arquivos Grandes:**
```python
chunk_size = 10000
chunks = []
for chunk in pd.read_csv(file, chunksize=chunk_size):
    processed = process(chunk)
    chunks.append(processed)
df = pd.concat(chunks)
```

## Saídas

### Dataset Unificado
- **Linhas:** 2.685.868
- **Colunas:** 43
- **Features de posts:** Imputadas

### Dataset Com Posts
- **Linhas:** 2.638
- **Colunas:** 43
- **Features de posts:** Completas

### Dataset Sem Posts
- **Linhas:** 2.683.230
- **Colunas:** 16 (apenas features de empresas)
- **Features de posts:** Ausentes

## Próximo Passo

[1.6 Otimização e SHAP](1.6_otimizacao_shap.md)

---

**Documento:** 1.5_pipeline_ml_base.md  
**Versão:** 1.0  
**Data:** Dezembro 2024

