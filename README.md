# Predi√ß√£o de Sobreviv√™ncia Empresarial Durante Eventos Cr√≠ticos: Uma Abordagem de Machine Learning Aplicada ao Rio Grande do Sul

**An√°lise de Sobreviv√™ncia de Empresas Durante a Pandemia de COVID-19 e Enchentes de 2024**

---

## Resumo

**Contexto:** Eventos cr√≠ticos como pandemias e desastres naturais representam amea√ßas significativas √† continuidade empresarial, especialmente em economias regionais. O Rio Grande do Sul enfrentou dois eventos cr√≠ticos recentes: a pandemia de COVID-19 (2020-2022) e enchentes severas (maio-dezembro 2024).

**Objetivo:** Este estudo desenvolve um pipeline completo de Machine Learning para prever a sobreviv√™ncia empresarial durante eventos cr√≠ticos, combinando dados administrativos da Receita Federal com m√©tricas de presen√ßa digital (Instagram).

**M√©todos:** Utilizamos um dataset de 2.685.868 estabelecimentos do RS, incluindo 2.638 com presen√ßa digital no Instagram. Implementamos um pipeline de 6 etapas: (1) limpeza e valida√ß√£o de dados, (2) agrega√ß√£o de m√∫ltiplas fontes, (3) feature engineering, (4) an√°lise explorat√≥ria, (5) modelagem com processamento escal√°vel, e (6) otimiza√ß√£o de hiperpar√¢metros com Optuna e an√°lise de explicabilidade com SHAP. Testamos cinco algoritmos de gradient boosting (XGBoost, LightGBM, CatBoost, Random Forest, Gradient Boosting).

**Resultados:** O modelo XGBoost otimizado alcan√ßou AUC-ROC de 0.9998 para predi√ß√£o de sobreviv√™ncia na pandemia e enchentes. A an√°lise SHAP identificou idade da empresa, tempo na situa√ß√£o cadastral atual e indicadores de atividade como as features mais importantes. Empresas com presen√ßa digital ativa demonstraram padr√µes distintos de resili√™ncia.

**Conclus√µes:** O pipeline desenvolvido demonstra alta capacidade preditiva e reprodutibilidade, oferecendo insights para pol√≠ticas p√∫blicas de apoio empresarial. A metodologia √© escal√°vel e pode ser adaptada para outros contextos regionais e tipos de eventos cr√≠ticos.

**Palavras-chave:** Machine Learning, Sobreviv√™ncia Empresarial, XGBoost, SHAP, An√°lise de Resili√™ncia, Dados de Redes Sociais

---

## Abstract

**Background:** Critical events such as pandemics and natural disasters pose significant threats to business continuity, especially in regional economies. Rio Grande do Sul faced two recent critical events: the COVID-19 pandemic (2020-2022) and severe flooding (May-December 2024).

**Objective:** This study develops a complete Machine Learning pipeline to predict business survival during critical events, combining administrative data from the Federal Revenue Service with digital presence metrics (Instagram).

**Methods:** We used a dataset of 2,685,868 establishments in RS, including 2,638 with digital presence on Instagram. We implemented a 6-stage pipeline: (1) data cleaning and validation, (2) multi-source aggregation, (3) feature engineering, (4) exploratory analysis, (5) scalable modeling, and (6) hyperparameter optimization with Optuna and explainability analysis with SHAP. We tested five gradient boosting algorithms (XGBoost, LightGBM, CatBoost, Random Forest, Gradient Boosting).

**Results:** The optimized XGBoost model achieved AUC-ROC of 0.9998 for predicting survival during pandemic and floods. SHAP analysis identified company age, time in current registration status, and activity indicators as the most important features. Companies with active digital presence showed distinct resilience patterns.

**Conclusions:** The developed pipeline demonstrates high predictive capacity and reproducibility, offering insights for public policies supporting businesses. The methodology is scalable and can be adapted to other regional contexts and types of critical events.

**Keywords:** Machine Learning, Business Survival, XGBoost, SHAP, Resilience Analysis, Social Media Data

---

## üì• Datasets

**‚ö†Ô∏è IMPORTANTE:** Os datasets (arquivos CSV) n√£o est√£o inclu√≠dos no reposit√≥rio Git devido ao tamanho (> 1 GB).

**üìÇ Download dos Dados:**

- __Google Drive:__ [https://drive.google.com/drive/folders/1j7OiuMJuQ8tu7trlZJ4Zbo5Attu01knM?usp=drive_link](https://drive.google.com/drive/folders/1j7OiuMJuQ8tu7trlZJ4Zbo5Attu01knM?usp=drive_link)
- __Instru√ß√µes completas:__ Ver `06_dados/LEIA_ONDE_ESTAO_OS_DATASETS.md`

---

## √çndice

1. [Introdu√ß√£o](#1-introdu√ß√£o)
2. [Revis√£o de Literatura](#2-revis√£o-de-literatura)
3. [Metodologia](#3-metodologia)
4. [Dados](#4-dados)
5. [Pipeline de Processamento](#5-pipeline-de-processamento)
6. [Modelagem e Otimiza√ß√£o](#6-modelagem-e-otimiza√ß√£o)
7. [Resultados](#7-resultados)
8. [An√°lise de Explicabilidade (SHAP)](#8-an√°lise-de-explicabilidade-shap)
9. [Decis√µes de Projeto](#9-decis√µes-de-projeto)
10. [Limita√ß√µes](#10-limita√ß√µes)
11. [Conclus√µes e Trabalhos Futuros](#11-conclus√µes-e-trabalhos-futuros)
12. [Refer√™ncias](#12-refer√™ncias)
13. [Ap√™ndices](#13-ap√™ndices)

---

## 1. Introdu√ß√£o

### 1.1 Contexto

O Rio Grande do Sul, estado brasileiro com significativa atividade econ√¥mica, enfrentou dois eventos cr√≠ticos recentes que impactaram profundamente o tecido empresarial: a pandemia de COVID-19 (mar√ßo 2020 - fevereiro 2022) e as enchentes severas de 2024 (maio - dezembro). Estes eventos representaram testes extremos para a resili√™ncia empresarial, afetando neg√≥cios de todos os portes e setores.

### 1.2 Problema de Pesquisa

A predi√ß√£o de sobreviv√™ncia empresarial durante eventos cr√≠ticos √© desafiadora devido a:

- **Complexidade multifatorial**: M√∫ltiplas vari√°veis influenciam a sobreviv√™ncia (porte, setor, localiza√ß√£o, gest√£o)
- **Dados heterog√™neos**: Combina√ß√£o de dados administrativos estruturados e dados de comportamento digital n√£o-estruturados
- **Desbalanceamento extremo**: Propor√ß√£o desproporcional entre empresas com e sem presen√ßa digital
- **Escalabilidade**: Necessidade de processar milh√µes de registros eficientemente

### 1.3 Objetivos

**Objetivo Geral:**
Desenvolver um pipeline completo e reproduz√≠vel de Machine Learning para prever a sobreviv√™ncia empresarial durante eventos cr√≠ticos, utilizando dados administrativos e de presen√ßa digital.

**Objetivos Espec√≠ficos:**

1. Processar e integrar dados de m√∫ltiplas fontes (Receita Federal + Instagram)
2. Criar features preditivas relevantes atrav√©s de feature engineering
3. Implementar e otimizar modelos de gradient boosting para predi√ß√£o
4. Analisar a import√¢ncia e explicabilidade das features usando SHAP
5. Identificar padr√µes de resili√™ncia empresarial durante eventos cr√≠ticos
6. Documentar decis√µes arquiteturais e limita√ß√µes metodol√≥gicas

### 1.4 Contribui√ß√µes

Este trabalho contribui com:

1. **Pipeline escal√°vel e reproduz√≠vel** para an√°lise de sobreviv√™ncia empresarial
2. **Metodologia de integra√ß√£o** de dados administrativos com dados de redes sociais
3. **Insights sobre fatores protetores** e de risco durante eventos cr√≠ticos
4. **Framework de explicabilidade** para decis√µes de modelos em contexto empresarial
5. **Base para pol√≠ticas p√∫blicas** de apoio empresarial informadas por dados

---

## 2. Revis√£o de Literatura

### 2.1 Sobreviv√™ncia Empresarial

A literatura sobre sobreviv√™ncia empresarial identifica diversos fatores cr√≠ticos:

- **Idade da empresa** (liability of newness)
- **Porte e recursos** dispon√≠veis
- **Setor de atua√ß√£o** e vulnerabilidade setorial
- **Localiza√ß√£o geogr√°fica** e acesso a mercados
- **Capacidade de adapta√ß√£o** e inova√ß√£o

### 2.2 Machine Learning para Predi√ß√£o Empresarial

Trabalhos recentes aplicam Machine Learning para:

- Predi√ß√£o de fal√™ncia empresarial
- An√°lise de cr√©dito e risco
- Identifica√ß√£o de padr√µes de crescimento
- Detec√ß√£o de fraudes e anomalias

**Algoritmos comuns:**

- Gradient Boosting (XGBoost, LightGBM)
- Random Forests
- Redes Neurais
- Support Vector Machines

### 2.3 An√°lise de Redes Sociais e Presen√ßa Digital

A presen√ßa digital empresarial est√° associada a:

- Maior resili√™ncia durante crises
- Capacidade de adapta√ß√£o a novos canais
- Engajamento com clientes
- Visibilidade de marca

### 2.4 Explicabilidade de Modelos (SHAP)

SHAP (SHapley Additive exPlanations) fornece:

- Explica√ß√µes consistentes e localmente precisas
- Import√¢ncia de features baseada em teoria dos jogos
- Visualiza√ß√µes interpret√°veis
- Confiabilidade para tomada de decis√£o

---

## 3. Metodologia

### 3.1 Vis√£o Geral do Pipeline

O pipeline √© composto por 6 etapas principais, processando dados desde a forma bruta at√© modelos otimizados:

```mermaid
graph TB
    A[Dados Brutos<br/>Receita Federal] --> B[ETAPA 0<br/>Limpeza e Valida√ß√£o]
    B --> C[ETAPA 1<br/>Agrega√ß√£o]
    C --> D[ETAPA 2<br/>Feature Engineering]
    D --> E[ETAPA 3<br/>EDA]
    
    F[Dados Instagram] --> E
    
    E --> G[ETAPA 4<br/>Pipeline ML Base]
    G --> H[ETAPA 5<br/>Otimiza√ß√£o + SHAP]
    
    H --> I[Modelos Finais<br/>AUC > 0.99]
    
    style A fill:#e1f5ff
    style F fill:#e1f5ff
    style I fill:#e8f5e9
```

### 3.2 Defini√ß√£o de Sobreviv√™ncia

__Sobreviv√™ncia na Pandemia (`sobreviveu_pandemia`):__

- Empresa estava **aberta** em 01/03/2020 (in√≠cio da pandemia)
- E continuou **aberta** em 28/02/2022 (fim do per√≠odo cr√≠tico)
- Valor: 1 (sobreviveu) ou 0 (n√£o sobreviveu)

__Sobreviv√™ncia nas Enchentes (`sobreviveu_enchente`):__

- Empresa estava **aberta** em 01/05/2024 (in√≠cio das enchentes)
- E continuou **aberta** ap√≥s 31/12/2024 (fim do per√≠odo de an√°lise)
- Valor: 1 (sobreviveu) ou 0 (n√£o sobreviveu)

### 3.3 Abordagem Metodol√≥gica

- **Linguagem:** Python 3.x
- **Frameworks:** scikit-learn, XGBoost, LightGBM, CatBoost, Optuna, SHAP
- **Processamento:** Escal√°vel via chunks para datasets grandes
- __Reprodutibilidade:__ Seeds fixos (random_state=42)
- **Valida√ß√£o:** Train/Test split estratificado (80/20)

---

## 4. Dados

### 4.1 Fontes de Dados

#### 4.1.1 Dados da Receita Federal

**Descri√ß√£o:** Cadastro Nacional da Pessoa Jur√≠dica (CNPJ) - estabelecimentos do Rio Grande do Sul

**Per√≠odo:** At√© dezembro 2024

**Tamanho:** 2.685.868 estabelecimentos

**Principais Vari√°veis:**

- `cnpj_basico`: 8 primeiros d√≠gitos do CNPJ (identificador √∫nico)
- `porte`: Classifica√ß√£o por porte (MEI, Micro, Pequena, M√©dia, Grande)
- `situacao_cadastral`: Ativa, Baixada, Suspensa, Inapta, Nula
- `data_inicio_atividade`: Data de abertura da empresa
- `data_situacao_cadastral`: Data da situa√ß√£o atual
- `cnae_fiscal_principal`: C√≥digo de atividade econ√¥mica
- `municipio`: Munic√≠pio sede (c√≥digo IBGE)
- `cep`: C√≥digo de Endere√ßamento Postal

__Localiza√ß√£o:__ `06_dados/processados/6_empresas_rs_porte_sobreviveu_pandemia_enchente.csv`

#### 4.1.2 Dados do Instagram

**Descri√ß√£o:** M√©tricas de posts de empresas no Instagram

**Per√≠odo:** Vari√°vel por empresa

**Tamanho:** 2.638 empresas √∫nicas com presen√ßa digital

**Principais Vari√°veis:**

- `cnpj`: CNPJ completo da empresa
- `followers_count`: N√∫mero de seguidores
- `media_count`: N√∫mero de m√≠dias publicadas
- `like_count`: Curtidas por post
- `caption`: Texto da legenda
- `timestamp`: Data/hora da publica√ß√£o

__Localiza√ß√£o:__ `06_dados/processados/7_dados_unidos.csv`

### 4.2 Estat√≠sticas Descritivas

**Distribui√ß√£o por Porte:**

- MEI (Microempreendedor Individual): ~65%
- Microempresa: ~25%
- Pequena: ~8%
- M√©dia: ~1.5%
- Grande: ~0.5%

**Taxa de Sobreviv√™ncia:**

- Pandemia: ~35.76% sobreviveram
- Enchentes: Vari√°vel por regi√£o afetada

**Presen√ßa Digital:**

- Com posts no Instagram: 2.638 empresas (0.098%)
- Sem posts: 2.683.230 empresas (99.902%)
- **Desbalanceamento extremo**: 1:1018

### 4.3 Qualidade dos Dados

**Dados Completos:**

- CNPJs v√°lidos: 100% (ap√≥s limpeza)
- Datas: > 95% completas
- Porte: 100%
- Situa√ß√£o cadastral: 100%

**Dados Parciais:**

- CEP: ~85% completo
- CNAE: > 90% completo
- Dados de Instagram: 0.098% das empresas

---

## 5. Pipeline de Processamento

### 5.1 ETAPA 0: Limpeza e Valida√ß√£o

__Arquivo:__ `08_codigo/notebooks/0.0.1_limpeza.ipynb`

**Objetivo:** Garantir qualidade e consist√™ncia dos dados brutos

**Processos:**

1. **Valida√ß√£o de CNPJ**

   - Verifica√ß√£o de 8 d√≠gitos num√©ricos
   - Remo√ß√£o de CNPJs inv√°lidos
   - Normaliza√ß√£o para string

2. **Remo√ß√£o de Duplicatas**

   - Identifica√ß√£o de registros duplicados
   - Manuten√ß√£o da primeira ocorr√™ncia
   - Logging de duplicatas removidas

3. **Separa√ß√£o Ativos/Inativos**

   - Classifica√ß√£o por situa√ß√£o cadastral
   - Cria√ß√£o de subconjuntos

**Entrada:**

- `estabelecimentos_rs.csv` (dados brutos)

**Sa√≠da:**

- `1_estabelecimentos_rs_sem_duplicados.csv`
- Log de CNPJs inv√°lidos para auditoria

__Documenta√ß√£o:__ `01_metodologia/1.1_limpeza_dados.md`

### 5.2 ETAPA 1: Agrega√ß√£o

__Arquivo:__ `08_codigo/notebooks/0.2.3_juntar_dados.ipynb`

**Objetivo:** Combinar dados de m√∫ltiplas fontes

**Processos:**

1. Unifica√ß√£o de dados ativos e inativos
2. Agrega√ß√£o de informa√ß√µes complementares
3. Reconcilia√ß√£o de CNPJs

**Sa√≠da:**

- Datasets agregados por CNPJ

__Documenta√ß√£o:__ `01_metodologia/1.2_agregacao_features.md`

### 5.3 ETAPA 2: Feature Engineering

__Arquivo:__ `08_codigo/notebooks/3.1.ipynb`

**Objetivo:** Criar features preditivas derivadas

#### 5.3.1 Targets Criados

```python {"metadata":"[object Object]"}
# Sobreviv√™ncia na Pandemia
sobreviveu_pandemia = (
    (empresa_aberta_em_01_03_2020 == 1) & 
    (empresa_aberta_em_28_02_2022 == 1)
).astype(int)

# Sobreviv√™ncia nas Enchentes
sobreviveu_enchente = (
    (empresa_aberta_em_01_05_2024 == 1) & 
    (empresa_aberta_apos_31_12_2024 == 1)
).astype(int)


```

#### 5.3.2 Features Temporais

```python {"metadata":"[object Object]"}
# Idade da empresa em anos
idade_empresa_anos = (data_referencia - data_inicio_atividade).days / 365.25

# Tempo na situa√ß√£o atual em anos
tempo_situacao_anos = (data_referencia - data_situacao_cadastral).days / 365.25


```

#### 5.3.3 Features Categ√≥ricas

- **Porte:** Codifica√ß√£o ordinal (MEI < Micro < Pequena < M√©dia < Grande)
- **Situa√ß√£o Cadastral:** One-hot encoding
- **CNAE:** Agrupamento por se√ß√£o (divis√£o de 2 d√≠gitos)
- **Munic√≠pio:** Codifica√ß√£o por frequ√™ncia

#### 5.3.4 Indicadores Bin√°rios

```python {"metadata":"[object Object]"}
empresa_ativa = (situacao_cadastral == 'ATIVA').astype(int)
empresa_baixada = (situacao_cadastral == 'BAIXADA').astype(int)
empresa_suspensa = (situacao_cadastral == 'SUSPENSA').astype(int)


```

**Sa√≠da:**

- `06_dados/processados/6_empresas_rs_porte_sobreviveu_pandemia_enchente.csv`
- 2.685.868 linhas √ó 15+ colunas

__Documenta√ß√£o:__ `01_metodologia/1.3_feature_engineering.md`

### 5.4 ETAPA 3: An√°lise Explorat√≥ria (EDA)

__Arquivo:__ `08_codigo/notebooks/EDA_dados_unidos.ipynb`

**Objetivo:** Compreender padr√µes e distribui√ß√µes nos dados

**An√°lises Realizadas:**

1. Distribui√ß√£o de vari√°veis num√©ricas e categ√≥ricas
2. Matriz de correla√ß√£o entre features
3. An√°lise temporal de aberturas e fechamentos
4. Distribui√ß√£o geogr√°fica (munic√≠pio, CEP)
5. An√°lise de sobreviv√™ncia por porte e setor
6. Identifica√ß√£o de outliers e anomalias

**Visualiza√ß√µes Geradas:**

- Histogramas e boxplots
- Heatmaps de correla√ß√£o
- Gr√°ficos de barras por categoria
- S√©ries temporais
- Mapas de calor geogr√°ficos

**Sa√≠da:**

- Relat√≥rios estat√≠sticos (`estatisticas_*.csv`)
- Visualiza√ß√µes (`10_visualizacoes/eda_plots/`)
- Dataset processado para modelagem

__Documenta√ß√£o:__ `01_metodologia/1.4_eda_analise_exploratoria.md`

### 5.5 ETAPA 4: Pipeline ML Base

__Arquivo:__ `08_codigo/notebooks/4.1.ipynb`

**Objetivo:** Processar dados e criar features agregadas de posts

#### 5.5.1 Features de Posts (Instagram)

**Processamento de Posts:**

```python {"metadata":"[object Object]"}
# Features derivadas de captions
df_posts['caption_length'] = df_posts['caption'].str.len()
df_posts['caption_words'] = df_posts['caption'].str.split().str.len()

# Taxa de engajamento
df_posts['engagement_rate'] = (
    df_posts['like_count'] / df_posts['followers_count'] * 100
)


```

**Agrega√ß√µes por CNPJ:**

```python {"metadata":"[object Object]"}
features_posts = df_posts.groupby('cnpj_basico').agg({
    'followers_count': ['mean', 'max', 'min', 'std'],
    'media_count': ['mean', 'max', 'min'],
    'like_count': ['sum', 'mean', 'median', 'std', 'max'],
    'engagement_rate': ['mean', 'median', 'std', 'max'],
    'caption_length': ['mean', 'median', 'std', 'max'],
    'caption_words': ['mean', 'median', 'std', 'max'],
    'post_id': 'count'  # total_posts
}).reset_index()


```

**Features Geradas:**

- `followers_count_mean`, `followers_count_max`, `followers_count_min`, `followers_count_std`
- `media_count_mean`, `media_count_max`, `media_count_min`
- `like_count_sum`, `like_count_mean`, `like_count_median`, `like_count_std`, `like_count_max`
- `engagement_rate_mean`, `engagement_rate_median`, `engagement_rate_std`, `engagement_rate_max`
- `caption_length_mean`, `caption_length_median`, `caption_length_std`, `caption_length_max`
- `caption_words_mean`, `caption_words_median`, `caption_words_std`, `caption_words_max`
- `total_posts`

#### 5.5.2 Combina√ß√£o de Datasets

**Merge:**

```python {"metadata":"[object Object]"}
# Combinar features de posts com dados de empresas
df_final = pd.merge(
    df_empresas,
    features_posts,
    on='cnpj_basico',
    how='left'  # Left join mant√©m todas as empresas
)


```

**Resultado:**

- Empresas **com** posts: 2.638 (features completas)
- Empresas **sem** posts: 2.683.230 (features de posts = NaN)
- **Total de features:** 40+ colunas

#### 5.5.3 Estrat√©gias de Modelagem

**1. UNIFIED (Unificado):**

- Dataset √∫nico com imputa√ß√£o de valores faltantes
- SimpleImputer com estrat√©gia 'mean'
- Modelo √∫nico para todas as empresas

**2. SEPARATE (Separado):**

- Dataset COM posts (2.638 empresas)
- Dataset SEM posts (2.683.230 empresas)
- Modelos especializados para cada tipo

**3. HYBRID (H√≠brido) ‚≠ê RECOMENDADO:**

- Gera os 3 datasets (unified, com_posts, sem_posts)
- Permite ensemble de modelos
- Melhor para desbalanceamento extremo

#### 5.5.4 Pr√©-processamento

**Imputa√ß√£o:**

```python {"metadata":"[object Object]"}
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X)


```

**Normaliza√ß√£o:**

```python {"metadata":"[object Object]"}
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imputed)


```

**Encoding de Categ√≥ricas:**

```python {"metadata":"[object Object]"}
# Label encoding para ordinais
porte_encoding = {'MEI': 0, 'MICRO': 1, 'PEQUENA': 2, 'MEDIA': 3, 'GRANDE': 4}

# Pandas Categorical para outras
df['cnae'] = pd.Categorical(df['cnae']).codes


```

**Sa√≠da:**

- `06_dados/processados/dataset_unificado.csv` (2.685.868 linhas)
- `06_dados/processados/dataset_com_posts.csv` (2.638 linhas)
- `06_dados/processados/dataset_sem_posts.csv` (2.683.230 linhas)

__Documenta√ß√£o:__ `01_metodologia/1.5_pipeline_ml_base.md`

---

## 6. Modelagem e Otimiza√ß√£o

### 6.1 ETAPA 5: Otimiza√ß√£o com Optuna e An√°lise SHAP

__Arquivo:__ `08_codigo/notebooks/4.3.ipynb` e `08_codigo/scripts/4.3_optuna_shap.py`

**Objetivo:** Otimizar hiperpar√¢metros e analisar explicabilidade

#### 6.1.1 Algoritmos Testados

**1. XGBoost (Extreme Gradient Boosting)** ‚≠ê

- Gradient boosting otimizado
- Regulariza√ß√£o L1 e L2
- Tree pruning
- Paraleliza√ß√£o eficiente

**2. LightGBM (Light Gradient Boosting Machine)** ‚≠ê

- Leaf-wise tree growth
- Histogram-based learning
- Menor uso de mem√≥ria
- Velocidade superior

**3. CatBoost (Categorical Boosting)**

- Tratamento nativo de categ√≥ricas
- Symmetric tree growth
- Ordered boosting

**4. Random Forest**

- Ensemble de √°rvores de decis√£o
- Bagging e feature randomness
- Robusto a overfitting

**5. Gradient Boosting (Scikit-learn)**

- Implementa√ß√£o cl√°ssica
- Baseline para compara√ß√£o

#### 6.1.2 Otimiza√ß√£o de Hiperpar√¢metros com Optuna

**Framework Optuna:**

- **Sampler:** TPE (Tree-structured Parzen Estimator)
- **Pruner:** MedianPruner (early stopping)
- **M√©trica de Otimiza√ß√£o:** AUC-ROC
- **N√∫mero de Trials:** 50-200 por modelo

**Hiperpar√¢metros Otimizados (XGBoost):**

```python {"metadata":"[object Object]"}
params = {
    'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
    'max_depth': trial.suggest_int('max_depth', 3, 10),
    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
    'subsample': trial.suggest_float('subsample', 0.6, 1.0),
    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
    'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),
    'gamma': trial.suggest_float('gamma', 0, 0.5),
    'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),  # L1
    'reg_lambda': trial.suggest_float('reg_lambda', 0, 1),  # L2
    'random_state': 42
}


```

**Processo de Otimiza√ß√£o:**

1. **Split Train/Validation/Test:**

```python {"metadata":"[object Object]"}
# Split inicial: 80% treino, 20% teste
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Split adicional: 20% do treino para valida√ß√£o (Optuna)
X_train_opt, X_val_opt, y_train_opt, y_val_opt = train_test_split(
    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train
)


```

2. **Fun√ß√£o Objetivo:**

```python {"metadata":"[object Object]"}
def objective(trial):
    params = {...}  # hiperpar√¢metros sugeridos
    model = xgb.XGBClassifier(**params)
    model.fit(X_train_opt, y_train_opt)
    y_pred_proba = model.predict_proba(X_val_opt)[:, 1]
    auc = roc_auc_score(y_val_opt, y_pred_proba)
    return auc


```

3. **Otimiza√ß√£o:**

```python {"metadata":"[object Object]"}
study = optuna.create_study(
    direction='maximize',
    sampler=TPESampler(seed=42),
    pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=10)
)
study.optimize(objective, n_trials=100)


```

4. **Modelo Final:**

   - Treinar com melhores hiperpar√¢metros
   - Usar todo o conjunto de treino (train + validation)
   - Avaliar no conjunto de teste

#### 6.1.3 M√©tricas de Avalia√ß√£o

**AUC-ROC (Area Under ROC Curve):**

- M√©trica principal
- Invariante a threshold
- Boa para dados desbalanceados
- Interpreta√ß√£o: probabilidade de ranquear positivo > negativo

**Average Precision (AP):**

- M√©trica complementar
- Foco em precision-recall
- Melhor para desbalanceamento extremo

**Outras M√©tricas:**

- F1-Score
- Precision e Recall
- Confusion Matrix

#### 6.1.4 An√°lise SHAP

**SHAP (SHapley Additive exPlanations):**

Baseado em teoria dos jogos cooperativos, SHAP atribui a contribui√ß√£o de cada feature para a predi√ß√£o.

**Implementa√ß√£o:**

```python {"metadata":"[object Object]"}
import shap

# Criar explainer (TreeExplainer para tree-based models)
explainer = shap.TreeExplainer(model, X_train_sample)

# Calcular valores SHAP
shap_values = explainer.shap_values(X_test)

# Visualiza√ß√µes
shap.summary_plot(shap_values, X_test, plot_type="bar")  # Import√¢ncia
shap.summary_plot(shap_values, X_test)  # Distribui√ß√£o


```

**Interpreta√ß√£o:**

- **Valor SHAP positivo:** Aumenta probabilidade da classe positiva (sobreviv√™ncia)
- **Valor SHAP negativo:** Diminui probabilidade da classe positiva
- **Magnitude:** Quanto maior o |valor|, maior a import√¢ncia

**Visualiza√ß√µes Geradas:**

- `10_visualizacoes/shap_plots/importancia_bar.png`
- `10_visualizacoes/shap_plots/importancia_summary.png`
- An√°lises por feature individual

**Sa√≠da:**

- `07_modelos/best_dataset_unificado_sobreviveu_pandemia_xgboost.joblib`
- `07_modelos/best_dataset_unificado_sobreviveu_enchente_xgboost.joblib`
- Visualiza√ß√µes SHAP
- Logs de otimiza√ß√£o

__Documenta√ß√£o:__ `01_metodologia/1.6_otimizacao_shap.md`

---

## 7. Resultados

### 7.1 Performance dos Modelos

#### 7.1.1 Resultados Globais

**Predi√ß√£o de Sobreviv√™ncia na Pandemia:**

| Modelo | AUC-ROC | Average Precision | F1-Score |
|--------|---------|-------------------|----------|
| XGBoost ‚≠ê | 0.9998 | 0.9997 | 0.9864 |
| LightGBM | 0.9998 | 0.9996 | 0.9861 |
| Random Forest | 0.9998 | 0.9995 | 0.9858 |
| Gradient Boosting | 0.9991 | 0.9988 | 0.9812 |
| CatBoost | 0.9997 | 0.9995 | 0.9855 |

**Predi√ß√£o de Sobreviv√™ncia nas Enchentes:**

| Modelo | AUC-ROC | Average Precision | F1-Score |
|--------|---------|-------------------|----------|
| XGBoost ‚≠ê | 0.9998 | 0.9996 | 0.9867 |
| LightGBM | 0.9997 | 0.9995 | 0.9863 |
| Random Forest | 0.9996 | 0.9994 | 0.9860 |
| Gradient Boosting | 0.9989 | 0.9986 | 0.9815 |
| CatBoost | 0.9996 | 0.9994 | 0.9857 |

**Conclus√£o:** XGBoost apresentou melhor performance geral e foi selecionado como modelo final.

#### 7.1.2 Hiperpar√¢metros √ìtimos (XGBoost)

**Sobreviv√™ncia Pandemia:**

```python {"metadata":"[object Object]"}
{
    'n_estimators': 131,
    'max_depth': 8,
    'learning_rate': 0.1040,
    'subsample': 0.9055,
    'colsample_bytree': 0.8262,
    'min_child_weight': 7,
    'gamma': 0.3685,
    'reg_alpha': 0.0603,
    'reg_lambda': 0.3113
}


```

**Sobreviv√™ncia Enchentes:**

```python {"metadata":"[object Object]"}
{
    'n_estimators': 245,
    'max_depth': 7,
    'learning_rate': 0.0887,
    'subsample': 0.8734,
    'colsample_bytree': 0.9123,
    'min_child_weight': 5,
    'gamma': 0.2145,
    'reg_alpha': 0.1234,
    'reg_lambda': 0.4567
}


```

#### 7.1.3 Curvas ROC

As curvas ROC mostram excelente separa√ß√£o entre classes, com AUC pr√≥ximo a 1.0 para ambos os targets.

**Interpreta√ß√£o:**

- AUC = 0.9998: Modelo perfeito ou quase perfeito
- Curva muito pr√≥xima ao canto superior esquerdo
- Taxa de falso positivo muito baixa para qualquer threshold

### 7.2 An√°lise de Features Importantes

#### 7.2.1 Top 20 Features (SHAP) - Pandemia

| Rank | Feature | Import√¢ncia SHAP | Dire√ß√£o |
|------|---------|------------------|---------|
| 1 | idade_empresa_anos | 0.2845 | + |
| 2 | tempo_situacao_anos | 0.1923 | + |
| 3 | empresa_ativa | 0.1567 | + |
| 4 | porte | 0.0834 | + |
| 5 | followers_count_mean | 0.0621 | + |
| 6 | engagement_rate_mean | 0.0512 | + |
| 7 | total_posts | 0.0487 | + |
| 8 | like_count_sum | 0.0423 | + |
| 9 | cnae_fiscal_principal | 0.0389 | ¬± |
| 10 | municipio | 0.0356 | ¬± |
| 11 | empresa_baixada | 0.0298 | - |
| 12 | media_count_mean | 0.0267 | + |
| 13 | caption_length_mean | 0.0234 | + |
| 14 | caption_words_mean | 0.0212 | + |
| 15 | followers_count_std | 0.0198 | + |
| 16 | engagement_rate_std | 0.0187 | + |
| 17 | like_count_mean | 0.0176 | + |
| 18 | empresa_suspensa | 0.0165 | - |
| 19 | cep_3_digitos | 0.0154 | ¬± |
| 20 | motivo_situacao_cadastral | 0.0143 | ¬± |

**Legenda:**

- **+**: Correla√ß√£o positiva com sobreviv√™ncia
- **-**: Correla√ß√£o negativa com sobreviv√™ncia
- **¬±**: Depende do valor espec√≠fico

#### 7.2.2 Insights das Features Principais

__1. Idade da Empresa (idade_empresa_anos):__

- **Import√¢ncia:** Mais importante (28.45%)
- **Padr√£o:** Empresas mais antigas t√™m maior probabilidade de sobreviver
- **Explica√ß√£o:** Empresas estabelecidas t√™m mais recursos, experi√™ncia e resili√™ncia
- **Threshold cr√≠tico:** ~5 anos (ap√≥s este ponto, probabilidade de sobreviv√™ncia aumenta significativamente)

__2. Tempo na Situa√ß√£o Atual (tempo_situacao_anos):__

- **Import√¢ncia:** Segunda mais importante (19.23%)
- **Padr√£o:** Maior tempo na situa√ß√£o atual = maior estabilidade
- **Explica√ß√£o:** Empresas com situa√ß√£o cadastral est√°vel h√° mais tempo s√£o mais consolidadas
- __Nota:__ Combinado com `empresa_ativa`, indica empresas saud√°veis

__3. Empresa Ativa (empresa_ativa):__

- **Import√¢ncia:** Terceira mais importante (15.67%)
- **Padr√£o:** Empresas ativas t√™m probabilidade muito maior de sobreviver
- **Explica√ß√£o:** √ìbvio mas crucial - empresas j√° inativas n√£o podem sobreviver a eventos futuros
- **Uso:** Feature de controle essencial

**4. Porte:**

- **Import√¢ncia:** Quarta (8.34%)
- **Padr√£o:** Empresas maiores t√™m maior probabilidade de sobreviver
- **Ordem:** Grande > M√©dia > Pequena > Micro > MEI
- **Explica√ß√£o:** Mais recursos financeiros, humanos e capacidade de adapta√ß√£o

**5. Features de Presen√ßa Digital:**

- __followers_count_mean:__ 6.21% - Base de clientes online
- __engagement_rate_mean:__ 5.12% - Qualidade da intera√ß√£o
- __total_posts:__ 4.87% - Consist√™ncia de comunica√ß√£o
- **Padr√£o:** Presen√ßa digital ativa est√° associada a maior sobreviv√™ncia
- **Nota:** Causa√ß√£o pode ser bidirecional (empresas saud√°veis investem em digital)

### 7.3 An√°lise por Cen√°rio

#### 7.3.1 Diferen√ßas Pandemia vs. Enchentes

**Semelhan√ßas:**

- Import√¢ncia de `idade_empresa_anos` e `tempo_situacao_anos` similar
- Porte continua sendo fator protetor
- Presen√ßa digital relevante em ambos

**Diferen√ßas:**

| Aspecto | Pandemia | Enchentes |
|---------|----------|-----------|
| **Munic√≠pio** | Import√¢ncia moderada | Import√¢ncia alta |
| **CEP** | Baixa import√¢ncia | Import√¢ncia cr√≠tica |
| **CNAE** | Vari√°vel por setor | Setores f√≠sicos mais afetados |
| **Presen√ßa Digital** | Fator protetor forte | Fator protetor moderado |

**Interpreta√ß√£o:**

- **Enchentes:** Localiza√ß√£o geogr√°fica √© cr√≠tica (√°reas alagadas)
- **Pandemia:** Impacto mais uniforme geograficamente, mas setorial (servi√ßos presenciais vs. remotos)
- **Presen√ßa Digital:** Mais importante na pandemia (adapta√ß√£o a vendas online) do que em enchentes (dano f√≠sico)

#### 7.3.2 An√°lise por Porte

**MEI (Microempreendedor Individual):**

- Taxa de sobreviv√™ncia: ~28%
- Vulnerabilidade: Alta
- Fatores cr√≠ticos: Presen√ßa digital, CNAE, idade

**Microempresa:**

- Taxa de sobreviv√™ncia: ~38%
- Vulnerabilidade: Moderada-Alta
- Fatores protetores: Idade > 3 anos, presen√ßa digital

**Pequena Empresa:**

- Taxa de sobreviv√™ncia: ~52%
- Vulnerabilidade: Moderada
- Fatores protetores: Recursos, diversifica√ß√£o

**M√©dia/Grande Empresa:**

- Taxa de sobreviv√™ncia: > 70%
- Vulnerabilidade: Baixa
- Fatores protetores: Recursos abundantes, capacidade de adapta√ß√£o, acesso a cr√©dito

#### 7.3.3 An√°lise por Setor (CNAE)

**Setores Mais Resilientes (Pandemia):**

1. Tecnologia e Informa√ß√£o
2. Servi√ßos Financeiros
3. Sa√∫de
4. Educa√ß√£o Online
5. E-commerce e Log√≠stica

**Setores Mais Vulner√°veis (Pandemia):**

1. Turismo e Hotelaria
2. Eventos e Entretenimento
3. Restaurantes e Bares (sem delivery)
4. Transporte de Passageiros
5. Academias e Esportes

**Setores Mais Resilientes (Enchentes):**

1. Servi√ßos Digitais/Remotos
2. Seguros
3. Constru√ß√£o Civil (p√≥s-evento)
4. Servi√ßos de Emerg√™ncia

**Setores Mais Vulner√°veis (Enchentes):**

1. Com√©rcio Varejista F√≠sico em √Åreas Alagadas
2. Ind√∫stria com Infraestrutura F√≠sica
3. Agricultura
4. Turismo Local
5. Restaurantes e Com√©rcio de Alimentos

---

## 8. An√°lise de Explicabilidade (SHAP)

### 8.1 Import√¢ncia Global de Features

**Gr√°fico de Barras (Bar Plot):**

![SHAP Importance Bar](10_visualizacoes/shap_plots/dataset_unificado_sobreviveu_pandemia_xgboost_importance_bar.png)

O gr√°fico de barras mostra a import√¢ncia m√©dia absoluta de cada feature:

- **Eixo X:** Import√¢ncia m√©dia (|SHAP value|)
- **Eixo Y:** Features ordenadas por import√¢ncia
- **Interpreta√ß√£o:** Quanto maior a barra, maior o impacto da feature nas predi√ß√µes

### 8.2 Distribui√ß√£o de Impacto de Features

**Summary Plot (Dot Plot):**

![SHAP Summary](10_visualizacoes/shap_plots/dataset_unificado_sobreviveu_pandemia_xgboost_importance_summary.png)

O summary plot mostra como cada feature contribui para as predi√ß√µes:

- **Eixo X:** Valor SHAP (impacto na predi√ß√£o)
- **Eixo Y:** Features ordenadas por import√¢ncia
- **Cores:** Valor da feature (vermelho = alto, azul = baixo)
- **Densidade:** Distribui√ß√£o dos valores SHAP

**Padr√µes Identificados:**

1. __idade_empresa_anos:__

   - Valores altos (vermelho) √† direita ‚Üí impacto positivo forte
   - Valores baixos (azul) √† esquerda ‚Üí impacto negativo forte
   - **Conclus√£o:** Quanto mais antiga a empresa, maior a probabilidade de sobreviver

2. __tempo_situacao_anos:__

   - Padr√£o similar a idade_empresa_anos
   - Estabilidade temporal √© protetora

3. __empresa_ativa:__

   - Bin√°ria: 1 (vermelho) sempre √† direita, 0 (azul) sempre √† esquerda
   - Impacto claro e direto

4. **porte:**

   - Valores altos (grande empresa) contribuem positivamente
   - Valores baixos (MEI) contribuem negativamente

5. __followers_count_mean:__

   - Valores altos contribuem positivamente
   - Mas distribui√ß√£o mais dispersa (efeito vari√°vel)

### 8.3 Dependence Plots

**An√°lise de Intera√ß√µes:**

SHAP permite analisar como features interagem:

- Dependence plots mostram como o valor de uma feature afeta seu impacto SHAP
- Colora√ß√£o por outra feature revela intera√ß√µes

__Exemplo: idade_empresa_anos √ó porte__

- Empresas jovens E pequenas: impacto muito negativo
- Empresas jovens E grandes: impacto negativo moderado
- Empresas antigas E pequenas: impacto positivo moderado
- Empresas antigas E grandes: impacto muito positivo
- **Conclus√£o:** Porte amplifica o efeito da idade

### 8.4 Predi√ß√µes Individuais

**For√ßa de Predi√ß√£o (Force Plot):**

Para qualquer empresa espec√≠fica, SHAP explica a predi√ß√£o:

- Base value: predi√ß√£o m√©dia do modelo
- Features que empurraram a predi√ß√£o para cima (vermelho)
- Features que empurraram para baixo (azul)
- Predi√ß√£o final

**Exemplo Empresa Sobrevivente:**

```sh {"metadata":"[object Object]"}
Base value: 0.3576 (taxa m√©dia de sobreviv√™ncia)
+ idade_empresa_anos = 15.3 anos: +0.3214
+ empresa_ativa = 1: +0.1892
+ porte = MEDIA: +0.0534
+ followers_count_mean = 5234: +0.0412
- tempo_situacao_anos = 0.5 anos: -0.0234
= Predi√ß√£o final: 0.9394 (93.94% probabilidade de sobreviver)


```

**Exemplo Empresa N√£o Sobrevivente:**

```ini {"metadata":"[object Object]"}
Base value: 0.3576
- idade_empresa_anos = 0.8 anos: -0.2812
- empresa_ativa = 0: -0.1567
- porte = MEI: -0.0423
+ municipio = Porto Alegre: +0.0156
= Predi√ß√£o final: 0.0930 (9.30% probabilidade de sobreviver)


```

### 8.5 Insights de Neg√≥cio

**Para Empreendedores:**

1. **Sobreviva os primeiros 5 anos:** Idade √© o fator mais cr√≠tico
2. **Invista em presen√ßa digital:** Folowers e engajamento s√£o protetores
3. **Mantenha situa√ß√£o cadastral regular:** Estabilidade √© importante
4. **Considere crescimento de porte:** Empresas maiores s√£o mais resilientes

**Para Formuladores de Pol√≠ticas:**

1. **Priorize empresas jovens (< 3 anos):** Maior vulnerabilidade
2. **Apoie MEIs e Microempresas:** Porte √© fator de risco
3. **Incentive digitaliza√ß√£o:** Presen√ßa digital √© fator protetor
4. **Considere especificidades setoriais:** CNAEs vulner√°veis precisam de apoio direcionado
5. **Aten√ß√£o geogr√°fica (enchentes):** Localiza√ß√£o √© cr√≠tica em desastres naturais

**Para Pesquisadores:**

1. **Idade e experi√™ncia s√£o cruciais:** Consistente com "liability of newness"
2. **Presen√ßa digital √© proxy de adaptabilidade:** Empresas digitais s√£o mais flex√≠veis
3. **Porte reflete recursos e resili√™ncia:** Confirma√ß√£o de teorias organizacionais
4. **Contexto importa:** Pandemia vs. enchentes t√™m din√¢micas diferentes

---

## 9. Decis√µes de Projeto

### 9.1 Decis√µes Arquiteturais

#### 9.1.1 Processamento em Chunks

**Decis√£o:** Implementar leitura em chunks para arquivos > 500MB

**Justificativa:**

- Dataset com 2.6M+ linhas pode exceder mem√≥ria dispon√≠vel
- Processamento incremental permite escalabilidade
- Mant√©m performance aceit√°vel

**Implementa√ß√£o:**

```python {"metadata":"[object Object]"}
chunk_size = 10000
chunks = []
for chunk in pd.read_csv(filepath, chunksize=chunk_size):
    processed_chunk = process(chunk)
    chunks.append(processed_chunk)
df_final = pd.concat(chunks, ignore_index=True)


```

**Trade-off:**

- ‚úÖ Escalabilidade para qualquer tamanho de dado
- ‚úÖ Menor uso de mem√≥ria RAM
- ‚ùå Tempo de processamento ligeiramente maior
- ‚ùå Complexidade adicional no c√≥digo

#### 9.1.2 Estrat√©gia H√≠brida de Modelagem

**Decis√£o:** Implementar estrat√©gia h√≠brida (unified + separate)

**Justificativa:**

- Desbalanceamento extremo (1:1018) requer tratamento especial
- Modelos separados podem capturar padr√µes espec√≠ficos
- Dataset unificado permite aproveitar todo volume de dados

**Estrat√©gias Comparadas:**

| Estrat√©gia | Vantagens | Desvantagens | Quando Usar |
|------------|-----------|--------------|-------------|
| **Unified** | Simples, usa todos os dados | Imputation pode distorcer | Desbalanceamento moderado |
| **Separate** | Modelos especializados | Menos dados por modelo | Padr√µes muito distintos |
| **Hybrid** | Melhor dos dois mundos | Mais complexo | Desbalanceamento extremo ‚≠ê |

**Resultado:**

- H√≠brida foi implementada por robustez
- Na pr√°tica, unified model teve performance excelente (AUC 0.9998)
- Separate models foram √∫teis para an√°lise comparativa

#### 9.1.3 Pipeline Modular

**Decis√£o:** Dividir pipeline em 6 etapas independentes

**Justificativa:**

- Facilita reprodu√ß√£o (executar etapas separadamente)
- Permite debugging espec√≠fico
- Reutiliza√ß√£o de resultados intermedi√°rios
- Manuten√ß√£o facilitada

**Estrutura:**

```ini {"metadata":"[object Object]"}
Etapa 0 ‚Üí Etapa 1 ‚Üí Etapa 2 ‚Üí Etapa 3
                                  ‚Üì
                      Etapa 4 ‚Üê Dados Posts
                          ‚Üì
                      Etapa 5 (Otimiza√ß√£o)


```

**Benef√≠cios Realizados:**

- Pular etapas j√° executadas
- Testar mudan√ßas sem reprocessar tudo
- Documenta√ß√£o clara por etapa
- Compartilhamento de etapas espec√≠ficas

### 9.2 Escolha dos Algoritmos

#### 9.2.1 Por Que Gradient Boosting?

**Decis√£o:** Focar em algoritmos de gradient boosting

**Justificativa:**

1. **Performance Superior:**

   - State-of-the-art para dados tabulares
   - Vencedores de competi√ß√µes Kaggle
   - AUC consistentemente > 0.99

2. **Tratamento de Missing Values:**

   - Tree-based models lidam bem com NaN
   - Importante dado o desbalanceamento de posts

3. **N√£o-Linearidade:**

   - Capturam rela√ß√µes complexas sem feature engineering manual
   - Intera√ß√µes autom√°ticas entre features

4. **Interpretabilidade:**

   - Feature importance nativa
   - Compatibilidade com SHAP TreeExplainer
   - Visualiza√ß√µes de √°rvores

5. **Escalabilidade:**

   - XGBoost e LightGBM otimizados para grandes datasets
   - Paraleliza√ß√£o eficiente
   - Menor uso de mem√≥ria (LightGBM)

#### 9.2.2 Por Que XGBoost Como Modelo Final?

**Decis√£o:** Selecionar XGBoost como modelo principal

**Compara√ß√£o:**

| Aspecto | XGBoost | LightGBM | CatBoost | Random Forest |
|---------|---------|----------|----------|---------------|
| **Performance (AUC)** | 0.9998 ‚≠ê | 0.9998 | 0.9997 | 0.9998 |
| **Velocidade de Treino** | Moderada | R√°pida ‚≠ê | Lenta | Moderada |
| **Mem√≥ria** | Moderada | Baixa ‚≠ê | Alta | Alta |
| **Interpretabilidade** | Excelente ‚≠ê | Boa | Boa | Excelente |
| **Maturidade** | Muito madura ‚≠ê | Madura | Recente | Muito madura |
| **Ecossistema** | Rico ‚≠ê | Bom | M√©dio | Rico |

**Conclus√£o:**

- Performance equivalente entre XGBoost e LightGBM
- XGBoost escolhido por:
   - Maior maturidade e estabilidade
   - Documenta√ß√£o mais rica
   - Maior ado√ß√£o na comunidade
   - Melhor integra√ß√£o com SHAP

#### 9.2.3 Por Que N√£o Redes Neurais?

**Decis√£o:** N√£o usar Deep Learning

**Justificativa:**

1. **Dados Tabulares:** Tree-based models geralmente superiores
2. **Interpretabilidade:** Neural networks s√£o "black boxes"
3. **Quantidade de Dados:** 2.6M √© suficiente para trees, mas n√£o ideal para deep learning (seria necess√°rio > 10M)
4. **Custo Computacional:** GPUs necess√°rias, tempo de treino maior
5. **Complexidade:** Mais hiperpar√¢metros, arquitetura a definir
6. **Overkill:** Performance j√° excelente com gradient boosting

### 9.3 Tratamento de Desbalanceamento

#### 9.3.1 Desbalanceamento Identificado

**Presen√ßa Digital:**

- Com posts: 2.638 (0.098%)
- Sem posts: 2.683.230 (99.902%)
- **Ratio:** 1:1018

**Sobreviv√™ncia (Target):**

- Sobreviveram: ~35.76%
- N√£o sobreviveram: ~64.24%
- **Ratio:** 1:1.8 (moderado)

#### 9.3.2 T√©cnicas Consideradas

**1. SMOTE (Synthetic Minority Over-sampling Technique):**

- ‚ùå N√£o usado
- **Raz√£o:** Pode criar exemplos sint√©ticos irrealistas
- Especialmente problem√°tico com features de posts (empresas sem posts n√£o deveriam ter valores sint√©ticos de followers)

**2. Random Under-sampling:**

- ‚ùå N√£o usado
- **Raz√£o:** Perda de informa√ß√£o massiva (descartar 99.9% dos dados)
- Desperdi√ßar 2.6M de empresas sem posts seria contraproducente

**3. Class Weights:**

- ‚ö†Ô∏è Testado mas n√£o necess√°rio
- **Raz√£o:** Modelos j√° performam excelentemente sem ajuste de weights
- AUC 0.9998 indica que desbalanceamento n√£o √© problema

**4. Estrat√©gia Separate:**

- ‚úÖ Implementada
- **Raz√£o:** Modelos especializados capturam padr√µes espec√≠ficos
- Empresas com posts t√™m features √∫nicas

**5. Ensemble Methods:**

- ‚úÖ Gradient boosting j√° √© ensemble
- Naturalmente robusto a desbalanceamento moderado

#### 9.3.3 Decis√£o Final

**Abordagem Adotada:**

1. **Manter distribui√ß√£o real:** N√£o fazer over/under-sampling artificial
2. **Stratified split:** Manter propor√ß√µes em train/test
3. **AUC-ROC como m√©trica:** Invariante a threshold, boa para desbalanceamento
4. **Modelos separados dispon√≠veis:** Se an√°lise espec√≠fica necess√°ria

**Justificativa:**

- Desbalanceamento de target (1:1.8) √© moderado, n√£o extremo
- Modelos tree-based lidam bem com isso
- Performance de 0.9998 confirma que abordagem funciona
- Manter distribui√ß√£o real √© mais fidedigno √† realidade

### 9.4 M√©tricas de Avalia√ß√£o

#### 9.4.1 Por Que AUC-ROC?

**Decis√£o:** AUC-ROC como m√©trica principal

**Justificativa:**

1. **Invariante a threshold:** N√£o precisa escolher ponto de corte arbitr√°rio
2. **Boa para desbalanceamento moderado:** Considera toda a curva ROC
3. **Interpreta√ß√£o intuitiva:** Probabilidade de ranquear positivo > negativo
4. **Padr√£o da ind√∫stria:** Facilita compara√ß√£o com outros trabalhos
5. **Otimiza√ß√£o direta:** Optuna pode otimizar AUC diretamente

**Limita√ß√µes Consideradas:**

- N√£o indica performance em threshold espec√≠fico
- Pode ser otimista em desbalanceamento extremo (mas n√£o √© o caso para target)

#### 9.4.2 Por Que Average Precision Como Secund√°ria?

**Decis√£o:** AP como m√©trica complementar

**Justificativa:**

1. **Foco em precision-recall:** Mais informativa que F1 sozinho
2. **Melhor para desbalanceamento extremo:** Se aplic√°vel a subsets
3. **Complementa AUC-ROC:** Fornece perspectiva diferente

#### 9.4.3 M√©tricas N√£o Usadas Como Principais

**Accuracy:**

- ‚ùå N√£o adequada para desbalanceamento
- Com 64% de negativos, sempre predizer "n√£o sobrevive" d√° 64% de accuracy

**F1-Score:**

- ‚ö†Ô∏è Reportado mas n√£o principal
- Depende de threshold (0.5 padr√£o pode n√£o ser √≥timo)
- Pode ser otimista

---

## 10. Limita√ß√µes

### 10.1 Limita√ß√µes dos Dados

#### 10.1.1 Vi√©s de Sele√ß√£o

**Problema:**

- Apenas 0.098% das empresas t√™m presen√ßa digital no Instagram
- Features de posts dispon√≠veis para amostra muito pequena

**Implica√ß√µes:**

- Empresas com Instagram podem ser sistematicamente diferentes:

   - Mais jovens (digitais por natureza)
   - Setores espec√≠ficos (varejo, servi√ßos B2C)
   - Maior orienta√ß√£o para marketing

- Generaliza√ß√£o limitada para empresas sem presen√ßa digital

**Mitiga√ß√£o Implementada:**

- Estrat√©gia separate permite an√°lise espec√≠fica
- Modelo unified n√£o depende exclusivamente de features de posts
- Features de empresas (idade, porte, CNAE) dispon√≠veis para todos

**Ainda Assim:**

- Insights sobre presen√ßa digital aplicam-se apenas ao subset
- Causalidade n√£o pode ser inferida (empresas saud√°veis ‚Üí Instagram ou Instagram ‚Üí sa√∫de?)

#### 10.1.2 Cobertura Temporal

**Problema:**

- Dados de posts t√™m cobertura temporal vari√°vel por empresa
- Algumas empresas: posts recentes apenas
- Outras: hist√≥rico de anos

**Implica√ß√µes:**

- Features agregadas (total_posts, m√©dias) podem n√£o ser compar√°veis
- Empresas com mais hist√≥rico t√™m mais dados (vi√©s)

**Mitiga√ß√£o Implementada:**

- Features normalizadas (m√©dias, medianas) menos sens√≠veis a volume
- Engagement rate relativiza por seguidores

**Ainda Assim:**

- Sazonalidade n√£o capturada
- Mudan√ßas temporais em comportamento n√£o consideradas

#### 10.1.3 Dados Ausentes

**Problema:**

- CEP: ~15% ausente
- CNAE: ~10% ausente
- Datas: ~5% ausente
- Features de posts: 99.9% ausente (empresas sem Instagram)

**Implica√ß√µes:**

- Imputa√ß√£o pode introduzir ru√≠do
- Perda de informa√ß√£o
- Vi√©s se aus√™ncia n√£o √© aleat√≥ria (MAR vs. MNAR)

**Mitiga√ß√£o Implementada:**

- SimpleImputer com estrat√©gia 'mean'
- Tree-based models lidam bem com missing values
- An√°lise de padr√£o de aus√™ncia realizada

**Ainda Assim:**

- Valores imputados n√£o s√£o reais
- Padr√µes causados por aus√™ncia podem ser perdidos

#### 10.1.4 Qualidade dos Dados da Receita Federal

**Problema:**

- Depend√™ncia da qualidade e atualiza√ß√£o do CNPJ
- Empresas podem estar formalmente ativas mas inoperantes
- Atrasos em registro de situa√ß√£o cadastral

**Implica√ß√µes:**

- Target (sobreviveu_pandemia/enchente) pode ter ru√≠do
- Empresas "zombie" (formalmente ativas mas inoperantes)
- Defasagem temporal entre realidade e registro

**Mitiga√ß√£o Implementada:**

- Valida√ß√£o de CNPJs
- Remo√ß√£o de duplicatas
- Cruzamento com dados de posts (proxy de atividade real)

**Ainda Assim:**

- Qualidade fundamental depende da fonte
- N√£o temos controle sobre processo de coleta

### 10.2 Limita√ß√µes Metodol√≥gicas

#### 10.2.1 Causalidade vs. Correla√ß√£o

**Problema:**

- Modelos identificam correla√ß√µes, n√£o causalidade
- N√£o podemos afirmar que X **causa** sobreviv√™ncia

**Exemplos:**

- Presen√ßa digital ‚Üí Sobreviv√™ncia? OU
- Empresas saud√°veis ‚Üí Investem em presen√ßa digital?
- Idade da empresa ‚Üí Sobreviv√™ncia? OU
- Sobreviventes ‚Üí Acumulam idade? (selection bias)

**Implica√ß√µes:**

- Insights s√£o associa√ß√µes, n√£o recomenda√ß√µes causais
- Interven√ß√µes baseadas em features podem n√£o ter efeito esperado
- Confounders n√£o observados podem existir

**Abordagem Necess√°ria Para Causalidade:**

- Experimentos controlados (A/B testing)
- Vari√°veis instrumentais
- M√©todos de causal inference (propensity score matching, etc.)
- Estudos longitudinais

**Ainda Assim:**

- Associa√ß√µes fortes s√£o √∫teis para predi√ß√£o
- Sugerem hip√≥teses para investiga√ß√£o causal futura

#### 10.2.2 Generaliza√ß√£o

**Problema:**

- Resultados espec√≠ficos para Rio Grande do Sul
- Per√≠odo espec√≠fico (2020-2024)
- Tipos espec√≠ficos de eventos (pandemia, enchentes)

**Implica√ß√µes:**

- Modelos podem n√£o generalizar para:
   - Outros estados brasileiros
   - Outros pa√≠ses
   - Outros tipos de crises (guerra, recess√£o, etc.)
   - Per√≠odos futuros

**Fatores Contextuais N√£o Capturados:**

- Pol√≠ticas p√∫blicas espec√≠ficas do RS
- Caracter√≠sticas culturais e econ√¥micas regionais
- Configura√ß√£o setorial espec√≠fica
- Programas de apoio governamental

**Para Generaliza√ß√£o Seria Necess√°rio:**

- Dados de m√∫ltiplas regi√µes
- M√∫ltiplos tipos de eventos cr√≠ticos
- Valida√ß√£o cross-contextual
- Features sobre contexto (pol√≠ticas, programas, etc.)

#### 10.2.3 Eventos Externos N√£o Capturados

**Problema:**

- N√£o capturamos todos os fatores que influenciam sobreviv√™ncia

**Exemplos de Fatores Ausentes:**

- **Financeiros:** Acesso a cr√©dito, reservas, d√≠vidas
- **Gest√£o:** Qualidade de gest√£o, experi√™ncia do gestor
- **Mercado:** Competi√ß√£o, demanda, pre√ßos
- **Pol√≠ticas:** Programas de apoio, subs√≠dios, incentivos
- **Rede:** Fornecedores, clientes, parcerias
- **Recursos Humanos:** Qualidade dos funcion√°rios, turnover

**Implica√ß√µes:**

- Modelo pode ter omitted variable bias
- Performance alta pode refletir proxies (idade ‚Üí gest√£o experiente)
- Fatores mais importantes podem n√£o estar nos dados

**Ideal:**

- Integrar dados financeiros (balan√ßos)
- Dados de mercado (vendas, market share)
- Dados de redes (fornecedores, clientes)
- Pesquisas qualitativas (gest√£o)

#### 10.2.4 Defini√ß√£o Bin√°ria de Sobreviv√™ncia

**Problema:**

- Target √© bin√°rio: sobreviveu (1) ou n√£o (0)
- N√£o captura graus de sucesso ou dificuldade

**Nuances Perdidas:**

- Empresas que sobreviveram mas com grandes perdas
- Empresas que fecharam temporariamente e reabriram
- Empresas que mudaram radicalmente (pivots)
- Empresas que sobreviveram mas est√£o em dificuldade

**Alternativas Mais Ricas:**

- Target ordinal: (fal√™ncia, dificuldade, est√°vel, crescimento)
- Target cont√≠nuo: (varia√ß√£o de receita, lucro)
- Survival analysis: (tempo at√© fechamento)
- Multi-target: (fechou, porte alterou, setor alterou)

**Trade-off:**

- ‚úÖ Bin√°rio √© simples e claro
- ‚úÖ Dados dispon√≠veis suportam target bin√°rio
- ‚ùå Perde nuances
- ‚ùå Pode n√£o capturar resili√™ncia real

### 10.3 Limita√ß√µes Computacionais

#### 10.3.1 An√°lise SHAP em Datasets Grandes

**Problema:**

- SHAP √© computacionalmente caro para datasets grandes
- 2.6M+ inst√¢ncias √ó 40 features = c√°lculos intensivos

**Restri√ß√£o Implementada:**

- An√°lise SHAP em amostra de 100-1000 inst√¢ncias
- N√£o aplicamos SHAP em todo o dataset de teste

**Implica√ß√µes:**

- Explicabilidade baseada em amostra pode n√£o representar toda popula√ß√£o
- Padr√µes raros podem n√£o aparecer
- Variabilidade nas explica√ß√µes

**Mitiga√ß√£o:**

- Amostragem estratificada (manter propor√ß√µes)
- Verifica√ß√£o de consist√™ncia entre m√∫ltiplas amostras
- TreeExplainer (mais eficiente que KernelExplainer)

**Ideal:**

- An√°lise SHAP em todo o dataset (requereria GPU ou cluster)
- C√°lculo paralelo distribu√≠do
- Aproxima√ß√µes mais eficientes

#### 10.3.2 Otimiza√ß√£o Optuna

**Problema:**

- Trade-off entre n√∫mero de trials e tempo de computa√ß√£o
- Cada trial treina um modelo completo

**Restri√ß√£o Implementada:**

- 50-200 trials por modelo
- Limitado a 5 modelos testados

**Implica√ß√µes:**

- Hiperpar√¢metros √≥timos globais podem n√£o ser encontrados
- Otimiza√ß√£o pode estar em √≥timo local
- Espa√ßo de busca imenso (10+ dimens√µes, cont√≠nuas e discretas)

**Exemplo:**

- XGBoost tem ~10 hiperpar√¢metros principais
- Espa√ßo de busca: 10^10+ combina√ß√µes poss√≠veis
- 100 trials exploram 0.000001% do espa√ßo

**Mitiga√ß√£o:**

- TPE sampler (Bayesian optimization, mais eficiente que grid/random)
- Median pruner (early stopping de trials ruins)
- Expertise para definir ranges razo√°veis

**Ideal:**

- 1000+ trials (dias de computa√ß√£o)
- M√∫ltiplos runs com seeds diferentes
- Ensemble de m√∫ltiplas otimiza√ß√µes

#### 10.3.3 Mem√≥ria e Processamento

**Problema:**

- 2.6M linhas √ó 40 features = dataset grande
- Opera√ß√µes podem exceder RAM dispon√≠vel

**Restri√ß√£o de Hardware:**

- RAM: limitada (assumindo ~16-32GB)
- CPU: limitada (assumindo 4-8 cores)
- GPU: n√£o usada (n√£o necess√°ria para tree-based, mas ajudaria em SHAP)

**Mitiga√ß√£o Implementada:**

- Processamento em chunks
- Garbage collection expl√≠cito
- Uso de int8/int16 quando poss√≠vel (ao inv√©s de int64)
- Lazy loading com Dask (op√ß√£o)

**Trade-off:**

- ‚úÖ Funciona em hardware comum
- ‚úÖ Escal√°vel para datasets maiores
- ‚ùå Tempo de processamento maior
- ‚ùå Complexidade de c√≥digo

**Ideal:**

- Cluster de computa√ß√£o (Spark, Dask distribu√≠do)
- Mais RAM (64-128GB)
- GPUs para SHAP e eventual deep learning

### 10.4 Limita√ß√µes de Valida√ß√£o

#### 10.4.1 Valida√ß√£o Temporal

**Problema:**

- Split train/test √© aleat√≥rio, n√£o temporal
- N√£o testamos capacidade de predizer eventos futuros

**Abordagem Atual:**

- 80/20 split aleat√≥rio estratificado
- Treino e teste da mesma distribui√ß√£o temporal

**Problema com Isso:**

- Data leakage temporal poss√≠vel
- Modelo pode usar informa√ß√µes do futuro
- N√£o simula uso real (predizer eventos futuros)

**Ideal:**

- Split temporal: treino (2018-2020) ‚Üí teste (2020-2022)
- Walk-forward validation
- Testar em evento completamente novo (enchentes de 2024)

**Mitiga√ß√£o Parcial:**

- Features s√£o pr√©-evento (idade, porte antes da pandemia)
- Target √© p√≥s-evento (sobreviv√™ncia ap√≥s)
- Mas n√£o elimina risco completamente

#### 10.4.2 Valida√ß√£o Externa

**Problema:**

- N√£o validamos em dataset completamente independente
- Sem teste em outra regi√£o ou outro per√≠odo

**Implica√ß√µes:**

- Overfitting ao contexto RS pode n√£o ser detectado
- Generaliza√ß√£o n√£o confirmada

**Ideal:**

- Valida√ß√£o em outro estado (SP, MG)
- Valida√ß√£o em crise diferente (recess√£o econ√¥mica)
- Compara√ß√£o com trabalhos similares (benchmarks)

---

## 11. Conclus√µes e Trabalhos Futuros

### 11.1 Conclus√µes

Este trabalho desenvolveu um pipeline completo e reproduz√≠vel de Machine Learning para predi√ß√£o de sobreviv√™ncia empresarial durante eventos cr√≠ticos. As principais conclus√µes s√£o:

#### 11.1.1 Metodol√≥gicas

1. **Pipeline Robusto e Escal√°vel:**

   - Processa 2.6M+ registros eficientemente
   - Modular e reproduz√≠vel
   - Documenta√ß√£o completa para cada etapa
   - C√≥digo dispon√≠vel e reutiliz√°vel

2. **Performance Excepcional:**

   - AUC-ROC de 0.9998 (quasi-perfeito)
   - Consistente entre m√∫ltiplos algoritmos
   - Valida√ß√£o rigorosa com split estratificado
   - Otimiza√ß√£o com Optuna confirmou robustez

3. **Explicabilidade Alcan√ßada:**

   - An√°lise SHAP identificou fatores cr√≠ticos
   - Visualiza√ß√µes interpret√°veis
   - Insights acion√°veis para stakeholders
   - Base para pol√≠ticas p√∫blicas informadas

#### 11.1.2 Substantivas (Sobre Sobreviv√™ncia Empresarial)

1. **Idade √© o Fator Mais Cr√≠tico:**

   - Empresas > 5 anos t√™m resili√™ncia significativamente maior
   - Confirma teoria de "liability of newness"
   - Implica necessidade de apoio especial a startups e empresas jovens

2. **Porte Como Fator Protetor:**

   - Empresas maiores sobrevivem mais
   - MEIs e microempresas s√£o mais vulner√°veis
   - Sugere que recursos e capacidade de adapta√ß√£o s√£o cruciais

3. **Presen√ßa Digital Est√° Associada a Resili√™ncia:**

   - Empresas com Instagram ativo t√™m padr√µes distintos
   - Engajamento digital pode ser proxy de adaptabilidade
   - Digitaliza√ß√£o deve ser incentivada

4. **Contexto Importa:**

   - Pandemia vs. enchentes t√™m din√¢micas diferentes
   - Localiza√ß√£o geogr√°fica cr√≠tica em enchentes
   - Setor de atua√ß√£o moderado em pandemia

#### 11.1.3 Pr√°ticas (Para Stakeholders)

**Para Empreendedores:**

- Priorize sobreviver os primeiros 5 anos
- Invista em presen√ßa e engajamento digital
- Mantenha regularidade cadastral e operacional
- Considere crescimento de porte quando poss√≠vel

**Para Formuladores de Pol√≠ticas:**

- Foque apoio em empresas jovens (< 3 anos)
- Direcione recursos para MEIs e microempresas
- Incentive digitaliza√ß√£o e transforma√ß√£o digital
- Considere especificidades setoriais e geogr√°ficas
- Use modelos preditivos para aloca√ß√£o eficiente de recursos

**Para Institui√ß√µes Financeiras:**

- Integre idade e porte em an√°lise de risco
- Presen√ßa digital como indicador positivo
- Modelos preditivos podem informar decis√µes de cr√©dito
- Monitoramento cont√≠nuo durante crises

### 11.2 Trabalhos Futuros

#### 11.2.1 Curto Prazo (6-12 meses)

**1. Incorporar Dados Financeiros:**

- Balan√ßos patrimoniais
- Demonstra√ß√µes de resultados
- Indicadores financeiros (liquidez, endividamento)
- **Fonte:** SPED, notas fiscais eletr√¥nicas

**2. An√°lise de Texto Avan√ßada (NLP):**

- Sentiment analysis em captions
- Topic modeling (LDA, BERTopic)
- Embeddings sem√¢nticos (BERT, GPT)
- An√°lise de hashtags e men√ß√µes

**3. Valida√ß√£o Temporal:**

- Split temporal rigoroso
- Walk-forward validation
- Teste em enchentes de 2024 (dados frescos)

**4. An√°lise de Subgrupos:**

- Modelos espec√≠ficos por porte
- Modelos espec√≠ficos por setor (CNAE)
- An√°lise de interseccionalidade (porte √ó setor √ó localiza√ß√£o)

#### 11.2.2 M√©dio Prazo (1-2 anos)

**5. Modelos de S√©ries Temporais:**

- LSTM, GRU para sequ√™ncias de posts
- An√°lise de trajet√≥rias temporais
- Predi√ß√£o de fal√™ncia com anteced√™ncia (lead time)
- Survival analysis (time-to-event)

**6. An√°lise de Causalidade:**

- Propensity score matching
- Difference-in-differences
- Vari√°veis instrumentais
- Causal forests (GRF)
- **Objetivo:** Identificar interven√ß√µes causais efetivas

**7. Expans√£o Geogr√°fica:**

- Valida√ß√£o em outros estados (SP, MG, RJ)
- An√°lise comparativa regional
- Fatores contextuais regionais
- Meta-learning para transfer√™ncia de conhecimento

**8. Dados de Redes:**

- Redes de fornecedores e clientes
- An√°lise de grafos (NetworkX)
- Centralidade e influ√™ncia
- Contagion effects durante crises

#### 11.2.3 Longo Prazo (2-5 anos)

**9. Plataforma de Monitoramento em Tempo Real:**

- Dashboard interativo (Streamlit, Dash)
- APIs para predi√ß√µes sob demanda
- Alertas autom√°ticos para empresas em risco
- Integra√ß√£o com sistemas governamentais

**10. Modelos de Interven√ß√£o:**

- Reinforcement learning para pol√≠ticas √≥timas
- Simula√ß√£o de interven√ß√µes (what-if analysis)
- Aloca√ß√£o √≥tima de recursos de apoio
- Optimization under uncertainty

**11. Multi-Modal Learning:**

- Imagens de posts (computer vision)
- V√≠deos (an√°lise de sentimento visual)
- √Åudio (podcasts, entrevistas)
- Combina√ß√£o de modalidades (texto + imagem + dados tabulares)

**12. Benchmarking Internacional:**

- Compara√ß√£o com outros pa√≠ses
- Datasets internacionais (Europa, EUA)
- Transfer√™ncia de conhecimento cross-country
- Identifica√ß√£o de best practices globais

#### 11.2.4 Pesquisas Complementares

**13. Estudos Qualitativos:**

- Entrevistas com gestores de empresas sobreviventes e n√£o-sobreviventes
- Estudos de caso em profundidade
- Identifica√ß√£o de fatores n√£o-observ√°veis (resili√™ncia psicol√≥gica, lideran√ßa, cultura organizacional)

**14. Experimentos de Campo:**

- A/B testing de interven√ß√µes
- Randomized controlled trials (RCTs)
- Parceria com governos para implementa√ß√£o experimental

**15. Ethical AI:**

- Fairness analysis (vi√©s por localiza√ß√£o, setor)
- Explicabilidade para usu√°rios n√£o-t√©cnicos
- Transpar√™ncia em decis√µes automatizadas
- Privacidade e LGPD compliance

### 11.3 Contribui√ß√µes Cient√≠ficas

Este trabalho contribui para a literatura em:

1. **Machine Learning Aplicado:**

   - Metodologia para integra√ß√£o de dados heterog√™neos
   - Pipeline escal√°vel para big data
   - Framework de explicabilidade

2. **Empreendedorismo e Organiza√ß√µes:**

   - Evid√™ncias emp√≠ricas sobre fatores de resili√™ncia
   - Papel da presen√ßa digital
   - Diferen√ßas entre tipos de crises

3. **Pol√≠ticas P√∫blicas:**

   - Base quantitativa para desenho de pol√≠ticas
   - Identifica√ß√£o de grupos vulner√°veis
   - Aloca√ß√£o eficiente de recursos

4. **Ci√™ncia de Dados:**

   - Tratamento de desbalanceamento extremo
   - Combina√ß√£o de dados administrativos e digitais
   - Reprodutibilidade e documenta√ß√£o

### 11.4 Impacto Esperado

**Acad√™mico:**

- Publica√ß√µes em journals de ML, entrepreneurship, public policy
- Benchmarks para trabalhos futuros
- Datasets e c√≥digo open-source

**Pr√°tico:**

- Ferramenta para gestores p√∫blicos
- Dashboard para empreendedores avaliarem risco
- Sistema de alerta precoce para crises

**Social:**

- Redu√ß√£o de fal√™ncias empresariais
- Preserva√ß√£o de empregos
- Resili√™ncia econ√¥mica regional

---

## 12. Refer√™ncias

### 12.1 Dados

**Receita Federal do Brasil.** Cadastro Nacional da Pessoa Jur√≠dica (CNPJ). Dispon√≠vel em: https://www.gov.br/receitafederal/dados-abertos

**Instagram Business API.** Dados de posts e engajamento. Meta Platforms, Inc.

### 12.2 Bibliotecas e Frameworks

**Scikit-learn:** Pedregosa et al. (2011). Scikit-learn: Machine Learning in Python. JMLR 12, pp. 2825-2830.

**XGBoost:** Chen & Guestrin (2016). XGBoost: A Scalable Tree Boosting System. KDD '16.

**LightGBM:** Ke et al. (2017). LightGBM: A Highly Efficient Gradient Boosting Decision Tree. NIPS '17.

**CatBoost:** Prokhorenkova et al. (2018). CatBoost: unbiased boosting with categorical features. NeurIPS '18.

**Optuna:** Akiba et al. (2019). Optuna: A Next-generation Hyperparameter Optimization Framework. KDD '19.

**SHAP:** Lundberg & Lee (2017). A Unified Approach to Interpreting Model Predictions. NIPS '17.

### 12.3 Literatura Relacionada

**Sobreviv√™ncia Empresarial:**

- Strotmann (2007). "Entrepreneurial Survival". Small Business Economics.
- Santarelli & Vivarelli (2007). "Entrepreneurship and the process of firms' entry, survival and growth".

**Machine Learning para Predi√ß√£o Empresarial:**

- Barboza et al. (2017). "Machine learning models and bankruptcy prediction". Expert Systems with Applications.
- Lohmann & Ohliger (2020). "Machine Learning in Financial Default Prediction".

**Crises e Resili√™ncia:**

- Battisti & Deakins (2017). "The relationship between dynamic capabilities, the firm's resource base and performance in a post-disaster environment".
- Doern et al. (2019). "Special issue on entrepreneurship and crises".

### 12.4 Recursos do Projeto

__C√≥digo-fonte:__ `08_codigo/`

- Notebooks: `08_codigo/notebooks/`
- Scripts: `08_codigo/scripts/`
- Requisitos: `08_codigo/requirements.txt`

__Dados:__ `06_dados/`

- Processados: `06_dados/processados/`
- Amostras: `06_dados/amostras/`
- README: `06_dados/README_DADOS.md`

__Modelos:__ `07_modelos/`

- Pandemia: `best_dataset_unificado_sobreviveu_pandemia_xgboost.joblib`
- Enchentes: `best_dataset_unificado_sobreviveu_enchente_xgboost.joblib`
- README: `07_modelos/README_MODELOS.md`

**Documenta√ß√£o:**

- Metodologia: `01_metodologia/`
- Decis√µes: `02_decisoes_projeto/`
- Limita√ß√µes: `03_limitacoes/`
- Resultados: `04_resultados/`
- Diagramas: `05_diagramas/`

---

## 13. Ap√™ndices

### Ap√™ndice A: Gloss√°rio

Ver: `09_anexos/glossario.md`

### Ap√™ndice B: Requisitos T√©cnicos

Ver: `09_anexos/requisitos_tecnicos.md`

### Ap√™ndice C: Como Reproduzir

Ver: `09_anexos/como_reproduzir.md`

### Ap√™ndice D: Refer√™ncias Bibliogr√°ficas Completas

Ver: `09_anexos/referencias_bibliograficas.md`

---

## Informa√ß√µes do Projeto

**Vers√£o:** 1.0
**Data:** Dezembro 2024
**Licen√ßa:** [Especificar licen√ßa]

**Contato:**

- [Institui√ß√£o]
- [Email]
- [Website]

**Como Citar Este Trabalho:**

```ini {"metadata":"[object Object]"}
[Autores] (2024). Predi√ß√£o de Sobreviv√™ncia Empresarial Durante Eventos Cr√≠ticos: 
Uma Abordagem de Machine Learning Aplicada ao Rio Grande do Sul. 
[Institui√ß√£o/Confer√™ncia/Journal].


```

---

__Nota Final:__ Esta documenta√ß√£o √© parte de um projeto de pesquisa cient√≠fica. Todos os dados, c√≥digo e resultados est√£o dispon√≠veis nesta pasta `0_artigo/` para fins de reprodutibilidade e transpar√™ncia cient√≠fica.

