{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2064bbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linhas lidas: 1000\n",
      "Colunas: ['cnpj    cep cnae_primario   username    name    followers_count media_count profile_picture_url biography   caption like_count  timestamp   business_account']\n",
      "  cnpj    cep cnae_primario   username    name    followers_count media_count profile_picture_url biography   caption like_count  timestamp   business_account\n",
      "0  06.303.914/0001-08     gruporezor  Grupo Rezor...                                                                                                          \n",
      "1  06.303.914/0001-08     gruporezor  Grupo Rezor...                                                                                                          \n",
      "2  06.303.914/0001-08     gruporezor  Grupo Rezor...                                                                                                          \n",
      "3  06.303.914/0001-08     gruporezor  Grupo Rezor...                                                                                                          \n",
      "4  06.303.914/0001-08     gruporezor  Grupo Rezor...                                                                                                          \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ler as 1000 primeiras linhas\n",
    "df = pd.read_csv('inativos/postsfull2.csv', delimiter='|', nrows=1000)\n",
    "# Ver informações básicas\n",
    "print(f\"Linhas lidas: {len(df)}\")\n",
    "print(f\"Colunas: {df.columns.tolist()}\")\n",
    "# Ver as primeiras 5 linhas\n",
    "print(df.head())\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "# Opcional: salvar em novo arquivo\n",
    "df.to_csv('primeiras_1000_inativos.csv', sep='|', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d726a19b",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 767: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Detectar o delimitador atual\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprimeiras_1000_inativos.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m----> 6\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     delimiter_atual \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mSniffer()\u001b[38;5;241m.\u001b[39msniff(sample)\u001b[38;5;241m.\u001b[39mdelimiter\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDelimitador atual: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdelimiter_atual\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\cass7\\.conda\\envs\\jupyter-gpu\\lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 767: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Detectar o delimitador atual\n",
    "with open('primeiras_1000_inativos.csv', 'r') as file:\n",
    "    sample = file.read(1024)\n",
    "    delimiter_atual = csv.Sniffer().sniff(sample).delimiter\n",
    "    print(f\"Delimitador atual: '{delimiter_atual}'\")\n",
    "\n",
    "# Ler com o delimitador detectado\n",
    "df = pd.read_csv('primeiras_1000_inativos.csv', sep=delimiter_atual)\n",
    "\n",
    "# Salvar com pipe\n",
    "df.to_csv('primeiras_1000_inativos_pipe.csv', sep='|', index=False)\n",
    "\n",
    "print(\"Arquivo convertido com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b1da002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delimitador: ' '\n"
     ]
    }
   ],
   "source": [
    "def detectar_delimitador(arquivo, bytes_amostra=4096):\n",
    "    \"\"\"\n",
    "    Detecta o delimitador mais provável de um arquivo CSV\n",
    "    \"\"\"\n",
    "    import csv\n",
    "    \n",
    "    # Delimitadores comuns para testar\n",
    "    delimitadores = [',', ';', '\\t', '|', ':', ' ']\n",
    "    \n",
    "    with open(arquivo, 'r', encoding='utf-8') as f:\n",
    "        amostra = f.read(bytes_amostra)\n",
    "    \n",
    "    # Conta ocorrências de cada delimitador\n",
    "    contagem = {}\n",
    "    for delim in delimitadores:\n",
    "        contagem[delim] = amostra.count(delim)\n",
    "    \n",
    "    # Retorna o delimitador mais frequente\n",
    "    delimitador_provavel = max(contagem, key=contagem.get)\n",
    "    \n",
    "    # Verifica com csv.Sniffer para confirmar\n",
    "    try:\n",
    "        sniffer = csv.Sniffer()\n",
    "        delimitador_sniffed = sniffer.sniff(amostra).delimiter\n",
    "        if delimitador_sniffed in delimitadores:\n",
    "            return delimitador_sniffed\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return delimitador_provavel\n",
    "\n",
    "# Usar a função\n",
    "delimitador = detectar_delimitador('inativos/postsfull2.csv')\n",
    "print(f\"Delimitador: '{delimitador}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d1644a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 84 fields in line 3, saw 157\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minativos/postsfull2.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcep\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcnae_primario\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124musername\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Ver informações básicas\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cass7\\.conda\\envs\\jupyter-gpu\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cass7\\.conda\\envs\\jupyter-gpu\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cass7\\.conda\\envs\\jupyter-gpu\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\cass7\\.conda\\envs\\jupyter-gpu\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mpandas/_libs/parsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/parsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/parsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/parsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/parsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 84 fields in line 3, saw 157\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('inativos/postsfull2.csv', delimiter=' ')\n",
    "\n",
    "df = df.drop(['cep', 'cnae_primario', 'username', 'name'], axis=1)\n",
    "# Ver informações básicas\n",
    "print(f\"Linhas lidas: {len(df)}\")\n",
    "print(f\"Colunas: {df.columns.tolist()}\")\n",
    "# Ver as primeiras 5 linhas\n",
    "print(df.head())\n",
    "    \n",
    "df.to_csv('inativos_limpo.csv', sep='|', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecf3f453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linhas lidas: 1000\n",
      "Colunas: ['cnpj', 'cep', 'cnae_primario', 'username', 'name', 'followers_count', 'media_count', 'profile_picture_url', 'biography', 'caption', 'like_count', 'timestamp', 'business_account', 'arquivo_origem', 'sobreviveu']\n",
      "                 cnpj       cep  cnae_primario    username         name  \\\n",
      "0  07.118.629/0001-80  95555000        4110700  grupopessi  GRUPO PESSI   \n",
      "1  07.118.629/0001-80  95555000        4110700  grupopessi  GRUPO PESSI   \n",
      "2  07.118.629/0001-80  95555000        4110700  grupopessi  GRUPO PESSI   \n",
      "3  07.118.629/0001-80  95555000        4110700  grupopessi  GRUPO PESSI   \n",
      "4  07.118.629/0001-80  95555000        4110700  grupopessi  GRUPO PESSI   \n",
      "\n",
      "   followers_count  media_count  \\\n",
      "0          17404.0        691.0   \n",
      "1          17404.0        691.0   \n",
      "2          17404.0        691.0   \n",
      "3          17404.0        691.0   \n",
      "4          17404.0        691.0   \n",
      "\n",
      "                                 profile_picture_url  \\\n",
      "0  https://scontent.fpoa4-1.fna.fbcdn.net/v/t51.2...   \n",
      "1  https://scontent.fpoa4-1.fna.fbcdn.net/v/t51.2...   \n",
      "2  https://scontent.fpoa4-1.fna.fbcdn.net/v/t51.2...   \n",
      "3  https://scontent.fpoa4-1.fna.fbcdn.net/v/t51.2...   \n",
      "4  https://scontent.fpoa4-1.fna.fbcdn.net/v/t51.2...   \n",
      "\n",
      "                                           biography  \\\n",
      "0  Incorporadora com 23 anos de atuação em Capão ...   \n",
      "1  Incorporadora com 23 anos de atuação em Capão ...   \n",
      "2  Incorporadora com 23 anos de atuação em Capão ...   \n",
      "3  Incorporadora com 23 anos de atuação em Capão ...   \n",
      "4  Incorporadora com 23 anos de atuação em Capão ...   \n",
      "\n",
      "                                             caption  like_count  \\\n",
      "0  No Markho Life Complex, cada novo dia é um con...        17.0   \n",
      "1  Tecnologia, mobilidade e inteligência a serviç...       115.0   \n",
      "2  A proximidade entre o Rio Grande do Sul e a Ar...        86.0   \n",
      "3  Alfredo Pessi, CEO do Grupo Pessi, participou ...        63.0   \n",
      "4  O Markho Life Complex vem despertando o intere...       128.0   \n",
      "\n",
      "                  timestamp  business_account        arquivo_origem  \\\n",
      "0  2025-05-06T14:40:40+0000              True  postsfull1_limpo.csv   \n",
      "1  2025-05-01T15:21:35+0000              True  postsfull1_limpo.csv   \n",
      "2  2025-04-29T20:51:34+0000              True  postsfull1_limpo.csv   \n",
      "3  2025-04-25T13:27:46+0000              True  postsfull1_limpo.csv   \n",
      "4  2025-04-24T15:04:56+0000              True  postsfull1_limpo.csv   \n",
      "\n",
      "   sobreviveu  \n",
      "0           1  \n",
      "1           1  \n",
      "2           1  \n",
      "3           1  \n",
      "4           1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ler as 1000 primeiras linhas\n",
    "df = pd.read_csv('1_dados_limpos/completo_ativos_com_sobreviveu.csv', delimiter='|', nrows=1000)\n",
    "# Ver informações básicas\n",
    "print(f\"Linhas lidas: {len(df)}\")\n",
    "print(f\"Colunas: {df.columns.tolist()}\")\n",
    "# Ver as primeiras 5 linhas\n",
    "print(df.head())\n",
    "\n",
    "# Opcional: salvar em novo arquivo\n",
    "df.to_csv('primeiras_1000_ativos.csv', sep='|', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e3c75e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linhas lidas: 1000\n",
      "Colunas: ['cnpj    cep cnae_primario   username    name    followers_count media_count profile_picture_url biography   caption like_count  timestamp   business_account', 'sobreviveu']\n",
      "  cnpj    cep cnae_primario   username    name    followers_count media_count profile_picture_url biography   caption like_count  timestamp   business_account  \\\n",
      "0  06.303.914/0001-08     gruporezor  Grupo Rezor...                                                                                                             \n",
      "1  06.303.914/0001-08     gruporezor  Grupo Rezor...                                                                                                             \n",
      "2  06.303.914/0001-08     gruporezor  Grupo Rezor...                                                                                                             \n",
      "3  06.303.914/0001-08     gruporezor  Grupo Rezor...                                                                                                             \n",
      "4  06.303.914/0001-08     gruporezor  Grupo Rezor...                                                                                                             \n",
      "\n",
      "   sobreviveu  \n",
      "0           0  \n",
      "1           0  \n",
      "2           0  \n",
      "3           0  \n",
      "4           0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ler as 1000 primeiras linhas\n",
    "df = pd.read_csv('1_dados_limpos/completo_inativos_nao_sobreviveu_pipe.csv', delimiter='|', nrows=1000)\n",
    "# Ver informações básicas\n",
    "print(f\"Linhas lidas: {len(df)}\")\n",
    "print(f\"Colunas: {df.columns.tolist()}\")\n",
    "# Ver as primeiras 5 linhas\n",
    "print(df.head())\n",
    "\n",
    "# Opcional: salvar em novo arquivo\n",
    "df.to_csv('primeiras_1000_inativos.csv', sep='|', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
