{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a988655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# VERIFICADOR E UNIFICADOR DE ARQUIVOS CSV\n",
    "# ============================================\n",
    "\n",
    "class CSVMerger:\n",
    "    \"\"\"Classe para verificar e unir arquivos CSV\"\"\"\n",
    "    \n",
    "    def __init__(self, arquivo1, arquivo2, separador='|'):\n",
    "        \"\"\"\n",
    "        Inicializa o verificador/unificador\n",
    "        \n",
    "        Args:\n",
    "            arquivo1: Caminho do primeiro arquivo CSV\n",
    "            arquivo2: Caminho do segundo arquivo CSV\n",
    "            separador: Separador usado nos CSVs (padr√£o: |)\n",
    "        \"\"\"\n",
    "        self.arquivo1 = arquivo1\n",
    "        self.arquivo2 = arquivo2\n",
    "        self.separador = separador\n",
    "        self.df1 = None\n",
    "        self.df2 = None\n",
    "        self.df_merged = None\n",
    "        self.compativel = False\n",
    "        \n",
    "    def carregar_arquivos(self):\n",
    "        \"\"\"Carrega os dois arquivos CSV\"\"\"\n",
    "        print(\"üìÇ CARREGANDO ARQUIVOS...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        try:\n",
    "            # Tentar diferentes encodings se necess√°rio\n",
    "            encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']\n",
    "            \n",
    "            for encoding in encodings:\n",
    "                try:\n",
    "                    self.df1 = pd.read_csv(self.arquivo1, sep=self.separador, encoding=encoding)\n",
    "                    print(f\"‚úÖ Arquivo 1 carregado com encoding {encoding}\")\n",
    "                    print(f\"   üìÅ {self.arquivo1}\")\n",
    "                    print(f\"   üìä Dimens√µes: {len(self.df1)} linhas x {len(self.df1.columns)} colunas\")\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "            for encoding in encodings:\n",
    "                try:\n",
    "                    self.df2 = pd.read_csv(self.arquivo2, sep=self.separador, encoding=encoding)\n",
    "                    print(f\"‚úÖ Arquivo 2 carregado com encoding {encoding}\")\n",
    "                    print(f\"   üìÅ {self.arquivo2}\")\n",
    "                    print(f\"   üìä Dimens√µes: {len(self.df2)} linhas x {len(self.df2.columns)} colunas\")\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "            if self.df1 is None or self.df2 is None:\n",
    "                raise Exception(\"N√£o foi poss√≠vel carregar os arquivos\")\n",
    "                \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao carregar arquivos: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def analisar_estrutura(self):\n",
    "        \"\"\"Analisa e compara a estrutura dos dois CSVs\"\"\"\n",
    "        print(\"\\nüìã AN√ÅLISE DA ESTRUTURA\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Colunas\n",
    "        cols1 = set(self.df1.columns)\n",
    "        cols2 = set(self.df2.columns)\n",
    "        \n",
    "        print(\"\\nüî§ COLUNAS:\")\n",
    "        print(f\"Arquivo 1: {len(cols1)} colunas\")\n",
    "        print(f\"Arquivo 2: {len(cols2)} colunas\")\n",
    "        \n",
    "        # Colunas em comum\n",
    "        colunas_comuns = cols1.intersection(cols2)\n",
    "        print(f\"\\n‚úÖ Colunas em comum: {len(colunas_comuns)}\")\n",
    "        if len(colunas_comuns) <= 20:\n",
    "            for col in sorted(colunas_comuns):\n",
    "                print(f\"   ‚Ä¢ {col}\")\n",
    "        else:\n",
    "            for i, col in enumerate(sorted(colunas_comuns)[:10]):\n",
    "                print(f\"   ‚Ä¢ {col}\")\n",
    "            print(f\"   ... e mais {len(colunas_comuns) - 10} colunas\")\n",
    "        \n",
    "        # Colunas exclusivas\n",
    "        exclusivas1 = cols1 - cols2\n",
    "        exclusivas2 = cols2 - cols1\n",
    "        \n",
    "        if exclusivas1:\n",
    "            print(f\"\\n‚ö†Ô∏è Colunas exclusivas do Arquivo 1: {len(exclusivas1)}\")\n",
    "            for col in sorted(exclusivas1)[:10]:\n",
    "                print(f\"   ‚Ä¢ {col}\")\n",
    "            if len(exclusivas1) > 10:\n",
    "                print(f\"   ... e mais {len(exclusivas1) - 10} colunas\")\n",
    "                \n",
    "        if exclusivas2:\n",
    "            print(f\"\\n‚ö†Ô∏è Colunas exclusivas do Arquivo 2: {len(exclusivas2)}\")\n",
    "            for col in sorted(exclusivas2)[:10]:\n",
    "                print(f\"   ‚Ä¢ {col}\")\n",
    "            if len(exclusivas2) > 10:\n",
    "                print(f\"   ... e mais {len(exclusivas2) - 10} colunas\")\n",
    "        \n",
    "        # Verificar compatibilidade\n",
    "        print(\"\\nüîç VERIFICA√á√ÉO DE COMPATIBILIDADE:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        if cols1 == cols2:\n",
    "            print(\"‚úÖ Estruturas ID√äNTICAS - Uni√£o simples poss√≠vel\")\n",
    "            self.compativel = True\n",
    "            self.tipo_uniao = \"concat\"\n",
    "        elif len(colunas_comuns) > 0:\n",
    "            print(f\"‚ö†Ô∏è Estruturas PARCIALMENTE compat√≠veis\")\n",
    "            print(f\"   {len(colunas_comuns)} colunas em comum podem ser unidas\")\n",
    "            self.compativel = True\n",
    "            self.tipo_uniao = \"merge\"\n",
    "        else:\n",
    "            print(\"‚ùå Estruturas INCOMPAT√çVEIS - Nenhuma coluna em comum\")\n",
    "            self.compativel = False\n",
    "            self.tipo_uniao = None\n",
    "            \n",
    "        return self.compativel\n",
    "    \n",
    "    def verificar_tipos_dados(self):\n",
    "        \"\"\"Verifica compatibilidade dos tipos de dados nas colunas comuns\"\"\"\n",
    "        print(\"\\nüìä TIPOS DE DADOS NAS COLUNAS COMUNS:\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        colunas_comuns = set(self.df1.columns).intersection(set(self.df2.columns))\n",
    "        \n",
    "        incompativeis = []\n",
    "        for col in sorted(colunas_comuns)[:20]:  # Verificar primeiras 20 colunas\n",
    "            tipo1 = self.df1[col].dtype\n",
    "            tipo2 = self.df2[col].dtype\n",
    "            \n",
    "            if tipo1 != tipo2:\n",
    "                incompativeis.append({\n",
    "                    'coluna': col,\n",
    "                    'tipo_arquivo1': str(tipo1),\n",
    "                    'tipo_arquivo2': str(tipo2)\n",
    "                })\n",
    "                print(f\"‚ö†Ô∏è {col}:\")\n",
    "                print(f\"   Arquivo 1: {tipo1}\")\n",
    "                print(f\"   Arquivo 2: {tipo2}\")\n",
    "        \n",
    "        if not incompativeis:\n",
    "            print(\"‚úÖ Todos os tipos de dados s√£o compat√≠veis!\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è {len(incompativeis)} colunas com tipos diferentes\")\n",
    "            print(\"   (Ser√£o convertidas automaticamente na uni√£o)\")\n",
    "            \n",
    "        return incompativeis\n",
    "    \n",
    "    def estatisticas_detalhadas(self):\n",
    "        \"\"\"Mostra estat√≠sticas detalhadas dos dois arquivos\"\"\"\n",
    "        print(\"\\nüìà ESTAT√çSTICAS DETALHADAS:\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for i, df in enumerate([self.df1, self.df2], 1):\n",
    "            print(f\"\\nüìÅ ARQUIVO {i}:\")\n",
    "            print(\"-\" * 40)\n",
    "            print(f\"Total de linhas: {len(df):,}\")\n",
    "            print(f\"Total de colunas: {len(df.columns)}\")\n",
    "            print(f\"Total de c√©lulas: {df.size:,}\")\n",
    "            print(f\"Mem√≥ria utilizada: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
    "            \n",
    "            # Valores nulos\n",
    "            nulos = df.isnull().sum().sum()\n",
    "            if nulos > 0:\n",
    "                print(f\"Valores nulos: {nulos:,} ({nulos/df.size*100:.2f}%)\")\n",
    "            \n",
    "            # Valores duplicados\n",
    "            duplicados = df.duplicated().sum()\n",
    "            if duplicados > 0:\n",
    "                print(f\"Linhas duplicadas: {duplicados:,} ({duplicados/len(df)*100:.2f}%)\")\n",
    "            \n",
    "            # Amostra de dados\n",
    "            print(f\"\\nüîç Primeiras 3 linhas:\")\n",
    "            print(df.head(3).to_string(max_cols=5))\n",
    "    \n",
    "    def unir_arquivos(self, metodo='auto', remover_duplicados=True):\n",
    "        \"\"\"\n",
    "        Une os dois arquivos CSV\n",
    "        \n",
    "        Args:\n",
    "            metodo: 'auto', 'concat', 'merge', 'outer'\n",
    "            remover_duplicados: Remove linhas duplicadas ap√≥s uni√£o\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame unificado\n",
    "        \"\"\"\n",
    "        if not self.compativel:\n",
    "            print(\"‚ùå Arquivos incompat√≠veis para uni√£o!\")\n",
    "            return None\n",
    "            \n",
    "        print(\"\\nüîó UNINDO ARQUIVOS...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        try:\n",
    "            if metodo == 'auto':\n",
    "                metodo = self.tipo_uniao\n",
    "            \n",
    "            if metodo == 'concat' or set(self.df1.columns) == set(self.df2.columns):\n",
    "                # Uni√£o simples (estruturas id√™nticas)\n",
    "                print(\"üìå M√©todo: Concatena√ß√£o simples (append)\")\n",
    "                # Adicionar coluna de origem\n",
    "                self.df1['origem_arquivo'] = 'arquivo_1'\n",
    "                self.df2['origem_arquivo'] = 'arquivo_2'\n",
    "                self.df_merged = pd.concat([self.df1, self.df2], ignore_index=True)\n",
    "                \n",
    "            else:\n",
    "                # Uni√£o com merge (estruturas diferentes)\n",
    "                print(\"üìå M√©todo: Uni√£o externa (outer join)\")\n",
    "                # Adicionar coluna de origem antes do merge\n",
    "                self.df1['origem_arquivo'] = 'arquivo_1'\n",
    "                self.df2['origem_arquivo'] = 'arquivo_2'\n",
    "                self.df_merged = pd.concat([self.df1, self.df2], ignore_index=True, sort=False)\n",
    "            \n",
    "            print(f\"‚úÖ Uni√£o conclu√≠da!\")\n",
    "            print(f\"   Total de linhas: {len(self.df_merged):,}\")\n",
    "            print(f\"   Total de colunas: {len(self.df_merged.columns)}\")\n",
    "            \n",
    "            # Remover duplicados se solicitado\n",
    "            if remover_duplicados:\n",
    "                antes = len(self.df_merged)\n",
    "                # N√£o considerar a coluna origem_arquivo para detectar duplicados\n",
    "                cols_para_verificar = [col for col in self.df_merged.columns if col != 'origem_arquivo']\n",
    "                self.df_merged = self.df_merged.drop_duplicates(subset=cols_para_verificar, keep='first')\n",
    "                depois = len(self.df_merged)\n",
    "                if antes > depois:\n",
    "                    print(f\"   üóëÔ∏è {antes - depois:,} linhas duplicadas removidas\")\n",
    "            \n",
    "            # Estat√≠sticas finais\n",
    "            print(f\"\\nüìä RESULTADO FINAL:\")\n",
    "            print(f\"   Linhas do arquivo 1: {len(self.df1):,}\")\n",
    "            print(f\"   Linhas do arquivo 2: {len(self.df2):,}\")\n",
    "            print(f\"   Linhas no arquivo unificado: {len(self.df_merged):,}\")\n",
    "            print(f\"   Colunas no arquivo unificado: {len(self.df_merged.columns)}\")\n",
    "            \n",
    "            return self.df_merged\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao unir arquivos: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def salvar_resultado(self, nome_arquivo=\"ativos e inativos_full.csv\", separador='|'):\n",
    "        \"\"\"\n",
    "        Salva o arquivo unificado\n",
    "        \n",
    "        Args:\n",
    "            nome_arquivo: Nome do arquivo de sa√≠da\n",
    "            separador: Separador a usar no arquivo de sa√≠da\n",
    "        \"\"\"\n",
    "        if self.df_merged is None:\n",
    "            print(\"‚ùå Nenhum arquivo unificado para salvar!\")\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            print(f\"\\nüíæ SALVANDO ARQUIVO UNIFICADO...\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            self.df_merged.to_csv(nome_arquivo, sep=separador, index=False, encoding='utf-8')\n",
    "            \n",
    "            # Verificar arquivo salvo\n",
    "            tamanho = Path(nome_arquivo).stat().st_size / 1024 / 1024\n",
    "            \n",
    "            print(f\"‚úÖ Arquivo salvo com sucesso!\")\n",
    "            print(f\"   üìÅ Nome: {nome_arquivo}\")\n",
    "            print(f\"   üìä Tamanho: {tamanho:.2f} MB\")\n",
    "            print(f\"   üìù Linhas: {len(self.df_merged):,}\")\n",
    "            print(f\"   üìã Colunas: {len(self.df_merged.columns)}\")\n",
    "            print(f\"   üî§ Separador: '{separador}'\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao salvar arquivo: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def executar_processo_completo(self):\n",
    "        \"\"\"Executa o processo completo de verifica√ß√£o e uni√£o\"\"\"\n",
    "        print(\"üöÄ INICIANDO PROCESSO DE VERIFICA√á√ÉO E UNI√ÉO DE CSVs\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # 1. Carregar arquivos\n",
    "        if not self.carregar_arquivos():\n",
    "            return False\n",
    "        \n",
    "        # 2. Analisar estrutura\n",
    "        if not self.analisar_estrutura():\n",
    "            print(\"\\n‚ùå Processo interrompido: arquivos incompat√≠veis\")\n",
    "            return False\n",
    "        \n",
    "        # 3. Verificar tipos de dados\n",
    "        self.verificar_tipos_dados()\n",
    "        \n",
    "        # 4. Mostrar estat√≠sticas\n",
    "        self.estatisticas_detalhadas()\n",
    "        \n",
    "        # 5. Unir arquivos\n",
    "        if self.unir_arquivos():\n",
    "            # 6. Salvar resultado\n",
    "            self.salvar_resultado()\n",
    "            print(\"\\n‚ú® PROCESSO CONCLU√çDO COM SUCESSO!\")\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "# ============================================\n",
    "# FUN√á√ïES AUXILIARES\n",
    "# ============================================\n",
    "\n",
    "def verificar_e_unir_csvs(arquivo1, arquivo2, separador='|', nome_saida=\"ativos e inativos_full.csv\"):\n",
    "    \"\"\"\n",
    "    Fun√ß√£o simplificada para verificar e unir dois CSVs\n",
    "    \n",
    "    Args:\n",
    "        arquivo1: Caminho do primeiro arquivo\n",
    "        arquivo2: Caminho do segundo arquivo\n",
    "        separador: Separador usado nos CSVs\n",
    "        nome_saida: Nome do arquivo de sa√≠da\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame unificado ou None se incompat√≠vel\n",
    "    \"\"\"\n",
    "    merger = CSVMerger(arquivo1, arquivo2, separador)\n",
    "    if merger.executar_processo_completo():\n",
    "        return merger.df_merged\n",
    "    return None\n",
    "\n",
    "def analisar_multiplos_csvs(*arquivos, separador='|'):\n",
    "    \"\"\"\n",
    "    Analisa m√∫ltiplos arquivos CSV para verificar compatibilidade\n",
    "    \n",
    "    Args:\n",
    "        *arquivos: Lista de caminhos de arquivos\n",
    "        separador: Separador usado nos CSVs\n",
    "    \"\"\"\n",
    "    print(\"üìä AN√ÅLISE DE M√öLTIPLOS ARQUIVOS CSV\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    dataframes = []\n",
    "    estruturas = []\n",
    "    \n",
    "    for i, arquivo in enumerate(arquivos, 1):\n",
    "        try:\n",
    "            df = pd.read_csv(arquivo, sep=separador)\n",
    "            dataframes.append(df)\n",
    "            estruturas.append(set(df.columns))\n",
    "            print(f\"‚úÖ Arquivo {i}: {arquivo}\")\n",
    "            print(f\"   Linhas: {len(df):,}, Colunas: {len(df.columns)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro no arquivo {i}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Verificar compatibilidade geral\n",
    "    colunas_comuns = set.intersection(*estruturas)\n",
    "    print(f\"\\nüìã Colunas em comum em TODOS os arquivos: {len(colunas_comuns)}\")\n",
    "    \n",
    "    if len(colunas_comuns) > 0:\n",
    "        print(\"‚úÖ Arquivos podem ser unidos!\")\n",
    "        return dataframes\n",
    "    else:\n",
    "        print(\"‚ùå Arquivos incompat√≠veis para uni√£o\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "178fe6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù EXEMPLO DE USO:\n",
      "============================================================\n",
      "\n",
      "# Para usar com seus arquivos, execute:\n",
      "\n",
      "merger = CSVMerger(r\"D:\\____________________FAPERGS\\DADOS\u0001_dados_limpos\\completo_ativos_com_sobreviveu.csv\", r\"D:\\____________________FAPERGS\\DADOS\u0001_dados_limpos\\completo_inativos_nao_sobreviveu_pipe.csv\", separador='|')\n",
      "merger.executar_processo_completo()\n",
      "\n",
      "# Ou use a fun√ß√£o simplificada:\n",
      "df_unificado = verificar_e_unir_csvs(\n",
      "    'caminho/para/arquivo1.csv',\n",
      "    'caminho/para/arquivo2.csv',\n",
      "    separador='|',\n",
      "    nome_saida='ativos e inativos_full.csv'\n",
      ")\n",
      "\n",
      "üöÄ INICIANDO PROCESSO DE VERIFICA√á√ÉO E UNI√ÉO DE CSVs\n",
      "============================================================\n",
      "üìÇ CARREGANDO ARQUIVOS...\n",
      "============================================================\n",
      "‚úÖ Arquivo 1 carregado com encoding utf-8\n",
      "   üìÅ D:\\____________________FAPERGS\\DADOS\\1_dados_limpos\\completo_ativos_com_sobreviveu.csv\n",
      "   üìä Dimens√µes: 1292775 linhas x 15 colunas\n",
      "‚úÖ Arquivo 2 carregado com encoding utf-8\n",
      "   üìÅ D:\\____________________FAPERGS\\DADOS\\1_dados_limpos\\completo_inativos_nao_sobreviveu_pipe.csv\n",
      "   üìä Dimens√µes: 239713 linhas x 2 colunas\n",
      "\n",
      "üìã AN√ÅLISE DA ESTRUTURA\n",
      "============================================================\n",
      "\n",
      "üî§ COLUNAS:\n",
      "Arquivo 1: 15 colunas\n",
      "Arquivo 2: 2 colunas\n",
      "\n",
      "‚úÖ Colunas em comum: 1\n",
      "   ‚Ä¢ sobreviveu\n",
      "\n",
      "‚ö†Ô∏è Colunas exclusivas do Arquivo 1: 14\n",
      "   ‚Ä¢ arquivo_origem\n",
      "   ‚Ä¢ biography\n",
      "   ‚Ä¢ business_account\n",
      "   ‚Ä¢ caption\n",
      "   ‚Ä¢ cep\n",
      "   ‚Ä¢ cnae_primario\n",
      "   ‚Ä¢ cnpj\n",
      "   ‚Ä¢ followers_count\n",
      "   ‚Ä¢ like_count\n",
      "   ‚Ä¢ media_count\n",
      "   ... e mais 4 colunas\n",
      "\n",
      "‚ö†Ô∏è Colunas exclusivas do Arquivo 2: 1\n",
      "   ‚Ä¢ cnpj    cep cnae_primario   username    name    followers_count media_count profile_picture_url biography   caption like_count  timestamp   business_account\n",
      "\n",
      "üîç VERIFICA√á√ÉO DE COMPATIBILIDADE:\n",
      "----------------------------------------\n",
      "‚ö†Ô∏è Estruturas PARCIALMENTE compat√≠veis\n",
      "   1 colunas em comum podem ser unidas\n",
      "\n",
      "üìä TIPOS DE DADOS NAS COLUNAS COMUNS:\n",
      "============================================================\n",
      "‚úÖ Todos os tipos de dados s√£o compat√≠veis!\n",
      "\n",
      "üìà ESTAT√çSTICAS DETALHADAS:\n",
      "============================================================\n",
      "\n",
      "üìÅ ARQUIVO 1:\n",
      "----------------------------------------\n",
      "Total de linhas: 1,292,775\n",
      "Total de colunas: 15\n",
      "Total de c√©lulas: 19,391,625\n",
      "Mem√≥ria utilizada: 3335.19 MB\n",
      "Valores nulos: 95,783 (0.49%)\n",
      "Linhas duplicadas: 25 (0.00%)\n",
      "\n",
      "üîç Primeiras 3 linhas:\n",
      "                 cnpj       cep  ...        arquivo_origem sobreviveu\n",
      "0  07.118.629/0001-80  95555000  ...  postsfull1_limpo.csv          1\n",
      "1  07.118.629/0001-80  95555000  ...  postsfull1_limpo.csv          1\n",
      "2  07.118.629/0001-80  95555000  ...  postsfull1_limpo.csv          1\n",
      "\n",
      "üìÅ ARQUIVO 2:\n",
      "----------------------------------------\n",
      "Total de linhas: 239,713\n",
      "Total de colunas: 2\n",
      "Total de c√©lulas: 479,426\n",
      "Mem√≥ria utilizada: 892.20 MB\n",
      "Linhas duplicadas: 447 (0.19%)\n",
      "\n",
      "üîç Primeiras 3 linhas:\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              cnpj    cep cnae_primario   username    name    followers_count media_count profile_picture_url biography   caption like_count  timestamp   business_account  sobreviveu\n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                06.303.914/0001-08     gruporezor  Grupo Rezor  955   387   https://scontent.fpoa4-1.fna.fbcdn.net/v/t51.2885-15/468994426_958969389469326_1637646702774260357_n.jpg?_nc_cat=105&ccb=1-7&_nc_sid=7d201b&_nc_eui2=AeHROGuoCx5SVC9aJ1bxc7AsrH8ez1IMdGisfx7PUgx0aGlBqprB3Z0yqye7X6OZIj4haF-oT9isIq5IWkqI4keH&_nc_ohc=YJLPjcHk-rwQ7kNvwFL48i0&_nc_oc=AdnIqRmsEs4a8aBeRAAU8Jt5ka6uoR4e2fa4uD2RwOqr9gnWvwROqA-6G71y1G-Xk2Q&_nc_zt=23&_nc_ht=scontent.fpoa4-1.fna&edm=AL-3X8kEAAAA&oh=00_AfNygrmhOYijvlb3LHOIJPg0CBPKT2CYYLSndncMOEiybg&oe=684D04BF   üéØ Consultoria com contabilidade üå± Agro üöÄ Startups üí∞ BPO Financeiro üìù Educa√ß√£o  üìçResultados que constroem legado    Dessa vez, nem Madame Simone teve resposta. üòÇüî•  Apenas com expertise, estrat√©gia e determina√ß√£o, √© poss√≠vel tomar decis√µes assertivas para n√£o ter problemas com a contabilidade do seu neg√≥cio.  - #gruporezor #madamesimone #contabilidade #santamaria #contabil  40    2025-06-04T18:04:48+0000 true           0\n",
      "1  06.303.914/0001-08     gruporezor  Grupo Rezor  955   387   https://scontent.fpoa4-1.fna.fbcdn.net/v/t51.2885-15/468994426_958969389469326_1637646702774260357_n.jpg?_nc_cat=105&ccb=1-7&_nc_sid=7d201b&_nc_eui2=AeHROGuoCx5SVC9aJ1bxc7AsrH8ez1IMdGisfx7PUgx0aGlBqprB3Z0yqye7X6OZIj4haF-oT9isIq5IWkqI4keH&_nc_ohc=YJLPjcHk-rwQ7kNvwFL48i0&_nc_oc=AdnIqRmsEs4a8aBeRAAU8Jt5ka6uoR4e2fa4uD2RwOqr9gnWvwROqA-6G71y1G-Xk2Q&_nc_zt=23&_nc_ht=scontent.fpoa4-1.fna&edm=AL-3X8kEAAAA&oh=00_AfNygrmhOYijvlb3LHOIJPg0CBPKT2CYYLSndncMOEiybg&oe=684D04BF   üéØ Consultoria com contabilidade üå± Agro üöÄ Startups üí∞ BPO Financeiro üìù Educa√ß√£o  üìçResultados que constroem legado    üì£ Aten√ß√£o, empresas e profissionais da √°rea fiscal!  A partir de janeiro de 2026, notas fiscais sem o detalhamento de IBS e CBS ser√£o rejeitadas. As mudan√ßas fazem parte do novo layout da nota fiscal eletr√¥nica e exigem aten√ß√£o desde j√°!  üîé O que muda? ‚úî Detalhamento dos tributos por item ‚úî Total dos tributos e total geral da nota ‚úî Inclus√£o de novos campos obrigat√≥rios, como al√≠quota efetiva, cr√©dito presumido e nota de cr√©dito  üóì Cronograma: Homologa√ß√£o: julho/2025 Produ√ß√£o: outubro/2025 Obrigatoriedade: janeiro/2026  üö® Fique por dentro, prepare sua equipe e garanta conformidade com as novas exig√™ncias!  #GrupoRezor #ReformaTribut√°ria #NotaFiscalEletr√¥nica #IBS #CBS #Contabilidade #ComplianceFiscal #Brasil2026  9    2025-05-27T15:51:46+0000 true           0\n",
      "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                           06.303.914/0001-08     gruporezor  Grupo Rezor  955   387   https://scontent.fpoa4-1.fna.fbcdn.net/v/t51.2885-15/468994426_958969389469326_1637646702774260357_n.jpg?_nc_cat=105&ccb=1-7&_nc_sid=7d201b&_nc_eui2=AeHROGuoCx5SVC9aJ1bxc7AsrH8ez1IMdGisfx7PUgx0aGlBqprB3Z0yqye7X6OZIj4haF-oT9isIq5IWkqI4keH&_nc_ohc=YJLPjcHk-rwQ7kNvwFL48i0&_nc_oc=AdnIqRmsEs4a8aBeRAAU8Jt5ka6uoR4e2fa4uD2RwOqr9gnWvwROqA-6G71y1G-Xk2Q&_nc_zt=23&_nc_ht=scontent.fpoa4-1.fna&edm=AL-3X8kEAAAA&oh=00_AfNygrmhOYijvlb3LHOIJPg0CBPKT2CYYLSndncMOEiybg&oe=684D04BF   üéØ Consultoria com contabilidade üå± Agro üöÄ Startups üí∞ BPO Financeiro üìù Educa√ß√£o  üìçResultados que constroem legado    TU DEIXOU PARA A √öLTIMA HORA, N√ÉO √â? üò±üìÜ  O prazo para entrega do IRPF 2025 se encerra no dia 30 de maio.  Corra que ainda d√° tempo! Entre e contato com o Grupo Rezor que podemos te ajudar.   - #IRPF #impostoderenda #santamaria #gruporezor #contabil #contabilidade  25    2025-05-22T12:01:30+0000 true           0\n",
      "\n",
      "üîó UNINDO ARQUIVOS...\n",
      "============================================================\n",
      "üìå M√©todo: Uni√£o externa (outer join)\n",
      "‚úÖ Uni√£o conclu√≠da!\n",
      "   Total de linhas: 1,532,488\n",
      "   Total de colunas: 17\n",
      "   üóëÔ∏è 472 linhas duplicadas removidas\n",
      "\n",
      "üìä RESULTADO FINAL:\n",
      "   Linhas do arquivo 1: 1,292,775\n",
      "   Linhas do arquivo 2: 239,713\n",
      "   Linhas no arquivo unificado: 1,532,016\n",
      "   Colunas no arquivo unificado: 17\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_65432\\3501673708.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     21\u001b[0m \"\"\")\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mmerger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCSVMerger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"D:\\____________________FAPERGS\\DADOS\\1_dados_limpos\\completo_ativos_com_sobreviveu.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mr\"D:\\____________________FAPERGS\\DADOS\\1_dados_limpos\\completo_inativos_nao_sobreviveu_pipe.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseparador\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'|'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mmerger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecutar_processo_completo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# ============================================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_65432\\3112696976.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[1;31m# 4. Mostrar estat√≠sticas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestatisticas_detalhadas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m         \u001b[1;31m# 5. Unir arquivos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munir_arquivos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    309\u001b[0m             \u001b[1;31m# 6. Salvar resultado\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msalvar_resultado\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n‚ú® PROCESSO CONCLU√çDO COM SUCESSO!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cass7\\.conda\\envs\\jupyter-gpu\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1575\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1576\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1577\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m   1578\u001b[0m             \u001b[1;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1579\u001b[0m             \u001b[1;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1580\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# EXEMPLO DE USO\n",
    "# ============================================\n",
    "\n",
    "# Exemplo b√°sico - SUBSTITUA COM SEUS ARQUIVOS\n",
    "print(\"üìù EXEMPLO DE USO:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "# Para usar com seus arquivos, execute:\n",
    "\n",
    "merger = CSVMerger(r\"D:\\____________________FAPERGS\\DADOS\\1_dados_limpos\\completo_ativos_com_sobreviveu.csv\", r\"D:\\____________________FAPERGS\\DADOS\\1_dados_limpos\\completo_inativos_nao_sobreviveu_pipe.csv\", separador='|')\n",
    "merger.executar_processo_completo()\n",
    "\n",
    "# Ou use a fun√ß√£o simplificada:\n",
    "df_unificado = verificar_e_unir_csvs(\n",
    "    'caminho/para/arquivo1.csv',\n",
    "    'caminho/para/arquivo2.csv',\n",
    "    separador='|',\n",
    "    nome_saida='ativos e inativos_full.csv'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "merger = CSVMerger(r\"D:\\____________________FAPERGS\\DADOS\\1_dados_limpos\\completo_ativos_com_sobreviveu.csv\", r\"D:\\____________________FAPERGS\\DADOS\\1_dados_limpos\\completo_inativos_nao_sobreviveu_pipe.csv\", separador='|')\n",
    "merger.executar_processo_completo()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# CRIAR ARQUIVOS DE EXEMPLO PARA TESTE\n",
    "# ============================================\n",
    "\n",
    "def criar_arquivos_exemplo():\n",
    "    \"\"\"Cria arquivos de exemplo para teste\"\"\"\n",
    "    \n",
    "    # Criar arquivo 1 - Ativos\n",
    "    df_ativos = pd.DataFrame({\n",
    "        'cnpj': ['06.303.914/0001-08', '11.222.333/0001-44', '99.888.777/0001-66'],\n",
    "        'username': ['gruporezor', 'empresa2', 'empresa3'],\n",
    "        'name': ['Grupo Rezor', 'Empresa 2', 'Empresa 3'],\n",
    "        'followers_count': [955, 1200, 450],\n",
    "        'status': ['ativo', 'ativo', 'ativo'],\n",
    "        'timestamp': ['2025-06-04T18:04:48+0000', '2025-06-05T10:00:00+0000', '2025-06-06T14:30:00+0000']\n",
    "    })\n",
    "    \n",
    "    # Criar arquivo 2 - Inativos\n",
    "    df_inativos = pd.DataFrame({\n",
    "        'cnpj': ['55.444.333/0001-22', '77.666.555/0001-11'],\n",
    "        'username': ['empresa4', 'empresa5'],\n",
    "        'name': ['Empresa 4', 'Empresa 5'],\n",
    "        'followers_count': [200, 150],\n",
    "        'status': ['inativo', 'inativo'],\n",
    "        'timestamp': ['2025-01-15T09:00:00+0000', '2025-02-20T11:30:00+0000']\n",
    "    })\n",
    "    \n",
    "    # Salvar arquivos de exemplo\n",
    "    df_ativos.to_csv('exemplo_ativos.csv', sep='|', index=False)\n",
    "    df_inativos.to_csv('exemplo_inativos.csv', sep='|', index=False)\n",
    "    \n",
    "    print(\"üìÅ Arquivos de exemplo criados:\")\n",
    "    print(\"   ‚Ä¢ exemplo_ativos.csv\")\n",
    "    print(\"   ‚Ä¢ exemplo_inativos.csv\")\n",
    "    print(\"\\nüöÄ Execute o c√≥digo abaixo para test√°-los:\")\n",
    "    print(\"merger = CSVMerger('exemplo_ativos.csv', 'exemplo_inativos.csv', '|')\")\n",
    "    print(\"merger.executar_processo_completo()\")\n",
    "    \n",
    "    return df_ativos, df_inativos\n",
    "\n",
    "# Descomentar para criar arquivos de exemplo\n",
    "# df_ativos, df_inativos = criar_arquivos_exemplo()\n",
    "\n",
    "print(\"\\n‚ú® C√≥digo pronto para uso! Substitua os nomes dos arquivos e execute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3ff0651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù EXEMPLO DE USO:\n",
      "============================================================\n",
      "\n",
      "# Para usar com seus arquivos, execute:\n",
      "\n",
      "merger = CSVMerger('arquivo1.csv', 'arquivo2.csv', separador='|')\n",
      "merger.executar_processo_completo()\n",
      "\n",
      "# Ou use a fun√ß√£o simplificada:\n",
      "df_unificado = verificar_e_unir_csvs(\n",
      "    'caminho/para/arquivo1.csv',\n",
      "    'caminho/para/arquivo2.csv',\n",
      "    separador='|',\n",
      "    nome_saida='ativos e inativos_full.csv'\n",
      ")\n",
      "\n",
      "# Para acessar o DataFrame resultado ap√≥s a execu√ß√£o:\n",
      "# df_resultado = merger.df_merged\n",
      "\n",
      "üöÄ INICIANDO PROCESSO DE VERIFICA√á√ÉO E UNI√ÉO DE CSVs\n",
      "============================================================\n",
      "üìÇ CARREGANDO ARQUIVOS...\n",
      "============================================================\n",
      "‚úÖ Arquivo 1 carregado com encoding utf-8\n",
      "   üìÅ completo_ativos_com_sobreviveu.csv\n",
      "   üìä Dimens√µes: 1,292,775 linhas x 15 colunas\n",
      "‚úÖ Arquivo 2 carregado com encoding utf-8\n",
      "   üìÅ completo_inativos_nao_sobreviveu_pipe.csv\n",
      "   üìä Dimens√µes: 239,713 linhas x 2 colunas\n",
      "\n",
      "üìã AN√ÅLISE DA ESTRUTURA\n",
      "============================================================\n",
      "\n",
      "üî§ COLUNAS:\n",
      "Arquivo 1: 15 colunas\n",
      "Arquivo 2: 2 colunas\n",
      "\n",
      "‚úÖ Colunas em comum: 1\n",
      "   ‚Ä¢ sobreviveu\n",
      "\n",
      "‚ö†Ô∏è Colunas exclusivas do Arquivo 1: 14\n",
      "   ‚Ä¢ arquivo_origem\n",
      "   ‚Ä¢ biography\n",
      "   ‚Ä¢ business_account\n",
      "   ‚Ä¢ caption\n",
      "   ‚Ä¢ cep\n",
      "   ‚Ä¢ cnae_primario\n",
      "   ‚Ä¢ cnpj\n",
      "   ‚Ä¢ followers_count\n",
      "   ‚Ä¢ like_count\n",
      "   ‚Ä¢ media_count\n",
      "   ... e mais 4 colunas\n",
      "\n",
      "‚ö†Ô∏è Colunas exclusivas do Arquivo 2: 1\n",
      "   ‚Ä¢ cnpj    cep cnae_primario   username    name    followers_count media_count profile_picture_url biography   caption like_count  timestamp   business_account\n",
      "\n",
      "üîç VERIFICA√á√ÉO DE COMPATIBILIDADE:\n",
      "----------------------------------------\n",
      "‚ö†Ô∏è Estruturas PARCIALMENTE compat√≠veis\n",
      "   1 colunas em comum podem ser unidas\n",
      "\n",
      "üìä TIPOS DE DADOS NAS COLUNAS COMUNS:\n",
      "============================================================\n",
      "‚úÖ Todos os tipos de dados s√£o compat√≠veis!\n",
      "\n",
      "üìà ESTAT√çSTICAS DETALHADAS:\n",
      "============================================================\n",
      "\n",
      "üìÅ ARQUIVO 1:\n",
      "----------------------------------------\n",
      "Total de linhas: 1,292,775\n",
      "Total de colunas: 15\n",
      "Total de c√©lulas: 19,391,625\n",
      "Mem√≥ria utilizada: 3445.28 MB\n",
      "Valores nulos: 95,783 (0.49%)\n",
      "Linhas duplicadas: 25 (0.00%)\n",
      "\n",
      "üîç Primeiras 3 linhas (primeiras 5 colunas):\n",
      "                 cnpj       cep  cnae_primario    username         name\n",
      "0  07.118.629/0001-80  95555000        4110700  grupopessi  GRUPO PESSI\n",
      "1  07.118.629/0001-80  95555000        4110700  grupopessi  GRUPO PESSI\n",
      "2  07.118.629/0001-80  95555000        4110700  grupopessi  GRUPO PESSI\n",
      "\n",
      "üìÅ ARQUIVO 2:\n",
      "----------------------------------------\n",
      "Total de linhas: 239,713\n",
      "Total de colunas: 2\n",
      "Total de c√©lulas: 479,426\n",
      "Mem√≥ria utilizada: 892.20 MB\n",
      "Linhas duplicadas: 447 (0.19%)\n",
      "\n",
      "üîç Primeiras 3 linhas (primeiras 5 colunas):\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              cnpj    cep cnae_primario   username    name    followers_count media_count profile_picture_url biography   caption like_count  timestamp   business_account  sobreviveu\n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                06.303.914/0001-08     gruporezor  Grupo Rezor  955   387   https://scontent.fpoa4-1.fna.fbcdn.net/v/t51.2885-15/468994426_958969389469326_1637646702774260357_n.jpg?_nc_cat=105&ccb=1-7&_nc_sid=7d201b&_nc_eui2=AeHROGuoCx5SVC9aJ1bxc7AsrH8ez1IMdGisfx7PUgx0aGlBqprB3Z0yqye7X6OZIj4haF-oT9isIq5IWkqI4keH&_nc_ohc=YJLPjcHk-rwQ7kNvwFL48i0&_nc_oc=AdnIqRmsEs4a8aBeRAAU8Jt5ka6uoR4e2fa4uD2RwOqr9gnWvwROqA-6G71y1G-Xk2Q&_nc_zt=23&_nc_ht=scontent.fpoa4-1.fna&edm=AL-3X8kEAAAA&oh=00_AfNygrmhOYijvlb3LHOIJPg0CBPKT2CYYLSndncMOEiybg&oe=684D04BF   üéØ Consultoria com contabilidade üå± Agro üöÄ Startups üí∞ BPO Financeiro üìù Educa√ß√£o  üìçResultados que constroem legado    Dessa vez, nem Madame Simone teve resposta. üòÇüî•  Apenas com expertise, estrat√©gia e determina√ß√£o, √© poss√≠vel tomar decis√µes assertivas para n√£o ter problemas com a contabilidade do seu neg√≥cio.  - #gruporezor #madamesimone #contabilidade #santamaria #contabil  40    2025-06-04T18:04:48+0000 true           0\n",
      "1  06.303.914/0001-08     gruporezor  Grupo Rezor  955   387   https://scontent.fpoa4-1.fna.fbcdn.net/v/t51.2885-15/468994426_958969389469326_1637646702774260357_n.jpg?_nc_cat=105&ccb=1-7&_nc_sid=7d201b&_nc_eui2=AeHROGuoCx5SVC9aJ1bxc7AsrH8ez1IMdGisfx7PUgx0aGlBqprB3Z0yqye7X6OZIj4haF-oT9isIq5IWkqI4keH&_nc_ohc=YJLPjcHk-rwQ7kNvwFL48i0&_nc_oc=AdnIqRmsEs4a8aBeRAAU8Jt5ka6uoR4e2fa4uD2RwOqr9gnWvwROqA-6G71y1G-Xk2Q&_nc_zt=23&_nc_ht=scontent.fpoa4-1.fna&edm=AL-3X8kEAAAA&oh=00_AfNygrmhOYijvlb3LHOIJPg0CBPKT2CYYLSndncMOEiybg&oe=684D04BF   üéØ Consultoria com contabilidade üå± Agro üöÄ Startups üí∞ BPO Financeiro üìù Educa√ß√£o  üìçResultados que constroem legado    üì£ Aten√ß√£o, empresas e profissionais da √°rea fiscal!  A partir de janeiro de 2026, notas fiscais sem o detalhamento de IBS e CBS ser√£o rejeitadas. As mudan√ßas fazem parte do novo layout da nota fiscal eletr√¥nica e exigem aten√ß√£o desde j√°!  üîé O que muda? ‚úî Detalhamento dos tributos por item ‚úî Total dos tributos e total geral da nota ‚úî Inclus√£o de novos campos obrigat√≥rios, como al√≠quota efetiva, cr√©dito presumido e nota de cr√©dito  üóì Cronograma: Homologa√ß√£o: julho/2025 Produ√ß√£o: outubro/2025 Obrigatoriedade: janeiro/2026  üö® Fique por dentro, prepare sua equipe e garanta conformidade com as novas exig√™ncias!  #GrupoRezor #ReformaTribut√°ria #NotaFiscalEletr√¥nica #IBS #CBS #Contabilidade #ComplianceFiscal #Brasil2026  9    2025-05-27T15:51:46+0000 true           0\n",
      "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                           06.303.914/0001-08     gruporezor  Grupo Rezor  955   387   https://scontent.fpoa4-1.fna.fbcdn.net/v/t51.2885-15/468994426_958969389469326_1637646702774260357_n.jpg?_nc_cat=105&ccb=1-7&_nc_sid=7d201b&_nc_eui2=AeHROGuoCx5SVC9aJ1bxc7AsrH8ez1IMdGisfx7PUgx0aGlBqprB3Z0yqye7X6OZIj4haF-oT9isIq5IWkqI4keH&_nc_ohc=YJLPjcHk-rwQ7kNvwFL48i0&_nc_oc=AdnIqRmsEs4a8aBeRAAU8Jt5ka6uoR4e2fa4uD2RwOqr9gnWvwROqA-6G71y1G-Xk2Q&_nc_zt=23&_nc_ht=scontent.fpoa4-1.fna&edm=AL-3X8kEAAAA&oh=00_AfNygrmhOYijvlb3LHOIJPg0CBPKT2CYYLSndncMOEiybg&oe=684D04BF   üéØ Consultoria com contabilidade üå± Agro üöÄ Startups üí∞ BPO Financeiro üìù Educa√ß√£o  üìçResultados que constroem legado    TU DEIXOU PARA A √öLTIMA HORA, N√ÉO √â? üò±üìÜ  O prazo para entrega do IRPF 2025 se encerra no dia 30 de maio.  Corra que ainda d√° tempo! Entre e contato com o Grupo Rezor que podemos te ajudar.   - #IRPF #impostoderenda #santamaria #gruporezor #contabil #contabilidade  25    2025-05-22T12:01:30+0000 true           0\n",
      "\n",
      "üîó UNINDO ARQUIVOS...\n",
      "============================================================\n",
      "üìå M√©todo: Uni√£o externa (outer join)\n",
      "‚úÖ Uni√£o conclu√≠da!\n",
      "   Total de linhas inicial: 1,532,488\n",
      "   Total de colunas: 17\n",
      "   üóëÔ∏è 472 linhas duplicadas removidas\n",
      "\n",
      "üìä RESULTADO FINAL:\n",
      "   Linhas do arquivo 1: 1,292,775\n",
      "   Linhas do arquivo 2: 239,713\n",
      "   Linhas no arquivo unificado: 1,532,016\n",
      "   Colunas no arquivo unificado: 17\n",
      "\n",
      "üìç DISTRIBUI√á√ÉO POR ARQUIVO DE ORIGEM:\n",
      "   ‚Ä¢ completo_ativos_com_sobreviveu.csv: 1,292,750 linhas (84.4%)\n",
      "   ‚Ä¢ completo_inativos_nao_sobreviveu_pipe.csv: 239,266 linhas (15.6%)\n",
      "\n",
      "üíæ SALVANDO ARQUIVO UNIFICADO...\n",
      "============================================================\n",
      "‚úÖ Arquivo salvo com sucesso!\n",
      "   üìÅ Nome: ativos e inativos_full.csv\n",
      "   üìä Tamanho: 1680.51 MB\n",
      "   üìù Linhas: 1,532,016\n",
      "   üìã Colunas: 17\n",
      "   üî§ Separador: '|'\n",
      "   üî† Encoding: UTF-8 com BOM\n",
      "\n",
      "‚ú® PROCESSO CONCLU√çDO COM SUCESSO!\n",
      "\n",
      "‚ú® C√≥digo pronto para uso! Substitua os nomes dos arquivos e execute.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# VERIFICADOR E UNIFICADOR DE ARQUIVOS CSV\n",
    "# ============================================\n",
    "\n",
    "class CSVMerger:\n",
    "    \"\"\"Classe para verificar e unir arquivos CSV\"\"\"\n",
    "    \n",
    "    def __init__(self, arquivo1, arquivo2, separador='|'):\n",
    "        \"\"\"\n",
    "        Inicializa o verificador/unificador\n",
    "        \n",
    "        Args:\n",
    "            arquivo1: Caminho do primeiro arquivo CSV\n",
    "            arquivo2: Caminho do segundo arquivo CSV\n",
    "            separador: Separador usado nos CSVs (padr√£o: |)\n",
    "        \"\"\"\n",
    "        self.arquivo1 = arquivo1\n",
    "        self.arquivo2 = arquivo2\n",
    "        self.separador = separador\n",
    "        self.df1 = None\n",
    "        self.df2 = None\n",
    "        self.df_merged = None\n",
    "        self.compativel = False\n",
    "        \n",
    "    def carregar_arquivos(self):\n",
    "        \"\"\"Carrega os dois arquivos CSV\"\"\"\n",
    "        print(\"üìÇ CARREGANDO ARQUIVOS...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        try:\n",
    "            # Tentar diferentes encodings se necess√°rio\n",
    "            encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']\n",
    "            \n",
    "            # Carregar primeiro arquivo\n",
    "            for encoding in encodings:\n",
    "                try:\n",
    "                    self.df1 = pd.read_csv(self.arquivo1, sep=self.separador, encoding=encoding, low_memory=False)\n",
    "                    print(f\"‚úÖ Arquivo 1 carregado com encoding {encoding}\")\n",
    "                    print(f\"   üìÅ {Path(self.arquivo1).name}\")\n",
    "                    print(f\"   üìä Dimens√µes: {len(self.df1):,} linhas x {len(self.df1.columns)} colunas\")\n",
    "                    break\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    if encoding == encodings[-1]:  # Se √© o √∫ltimo encoding\n",
    "                        print(f\"   ‚ùå Erro: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Carregar segundo arquivo        \n",
    "            for encoding in encodings:\n",
    "                try:\n",
    "                    self.df2 = pd.read_csv(self.arquivo2, sep=self.separador, encoding=encoding, low_memory=False)\n",
    "                    print(f\"‚úÖ Arquivo 2 carregado com encoding {encoding}\")\n",
    "                    print(f\"   üìÅ {Path(self.arquivo2).name}\")\n",
    "                    print(f\"   üìä Dimens√µes: {len(self.df2):,} linhas x {len(self.df2.columns)} colunas\")\n",
    "                    break\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    if encoding == encodings[-1]:  # Se √© o √∫ltimo encoding\n",
    "                        print(f\"   ‚ùå Erro: {e}\")\n",
    "                    continue\n",
    "                    \n",
    "            if self.df1 is None or self.df2 is None:\n",
    "                raise Exception(\"N√£o foi poss√≠vel carregar um ou ambos os arquivos\")\n",
    "                \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao carregar arquivos: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def analisar_estrutura(self):\n",
    "        \"\"\"Analisa e compara a estrutura dos dois CSVs\"\"\"\n",
    "        print(\"\\nüìã AN√ÅLISE DA ESTRUTURA\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Limpar nomes das colunas (remover espa√ßos extras)\n",
    "        self.df1.columns = self.df1.columns.str.strip()\n",
    "        self.df2.columns = self.df2.columns.str.strip()\n",
    "        \n",
    "        # Colunas\n",
    "        cols1 = set(self.df1.columns)\n",
    "        cols2 = set(self.df2.columns)\n",
    "        \n",
    "        print(f\"\\nüî§ COLUNAS:\")\n",
    "        print(f\"Arquivo 1: {len(cols1)} colunas\")\n",
    "        print(f\"Arquivo 2: {len(cols2)} colunas\")\n",
    "        \n",
    "        # Colunas em comum\n",
    "        colunas_comuns = cols1.intersection(cols2)\n",
    "        print(f\"\\n‚úÖ Colunas em comum: {len(colunas_comuns)}\")\n",
    "        if len(colunas_comuns) <= 20:\n",
    "            for col in sorted(colunas_comuns):\n",
    "                print(f\"   ‚Ä¢ {col}\")\n",
    "        else:\n",
    "            for i, col in enumerate(sorted(colunas_comuns)[:10]):\n",
    "                print(f\"   ‚Ä¢ {col}\")\n",
    "            print(f\"   ... e mais {len(colunas_comuns) - 10} colunas\")\n",
    "        \n",
    "        # Colunas exclusivas\n",
    "        exclusivas1 = cols1 - cols2\n",
    "        exclusivas2 = cols2 - cols1\n",
    "        \n",
    "        if exclusivas1:\n",
    "            print(f\"\\n‚ö†Ô∏è Colunas exclusivas do Arquivo 1: {len(exclusivas1)}\")\n",
    "            for col in list(sorted(exclusivas1))[:10]:\n",
    "                print(f\"   ‚Ä¢ {col}\")\n",
    "            if len(exclusivas1) > 10:\n",
    "                print(f\"   ... e mais {len(exclusivas1) - 10} colunas\")\n",
    "                \n",
    "        if exclusivas2:\n",
    "            print(f\"\\n‚ö†Ô∏è Colunas exclusivas do Arquivo 2: {len(exclusivas2)}\")\n",
    "            for col in list(sorted(exclusivas2))[:10]:\n",
    "                print(f\"   ‚Ä¢ {col}\")\n",
    "            if len(exclusivas2) > 10:\n",
    "                print(f\"   ... e mais {len(exclusivas2) - 10} colunas\")\n",
    "        \n",
    "        # Verificar compatibilidade\n",
    "        print(\"\\nüîç VERIFICA√á√ÉO DE COMPATIBILIDADE:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        if cols1 == cols2:\n",
    "            print(\"‚úÖ Estruturas ID√äNTICAS - Uni√£o simples poss√≠vel\")\n",
    "            self.compativel = True\n",
    "            self.tipo_uniao = \"concat\"\n",
    "        elif len(colunas_comuns) > 0:\n",
    "            print(f\"‚ö†Ô∏è Estruturas PARCIALMENTE compat√≠veis\")\n",
    "            print(f\"   {len(colunas_comuns)} colunas em comum podem ser unidas\")\n",
    "            self.compativel = True\n",
    "            self.tipo_uniao = \"merge\"\n",
    "        else:\n",
    "            print(\"‚ùå Estruturas INCOMPAT√çVEIS - Nenhuma coluna em comum\")\n",
    "            self.compativel = False\n",
    "            self.tipo_uniao = None\n",
    "            \n",
    "        return self.compativel\n",
    "    \n",
    "    def verificar_tipos_dados(self):\n",
    "        \"\"\"Verifica compatibilidade dos tipos de dados nas colunas comuns\"\"\"\n",
    "        print(\"\\nüìä TIPOS DE DADOS NAS COLUNAS COMUNS:\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        colunas_comuns = set(self.df1.columns).intersection(set(self.df2.columns))\n",
    "        \n",
    "        if not colunas_comuns:\n",
    "            print(\"‚ö†Ô∏è Nenhuma coluna em comum para verificar tipos\")\n",
    "            return []\n",
    "        \n",
    "        incompativeis = []\n",
    "        colunas_verificar = list(sorted(colunas_comuns))[:20]  # Verificar primeiras 20 colunas\n",
    "        \n",
    "        for col in colunas_verificar:\n",
    "            tipo1 = self.df1[col].dtype\n",
    "            tipo2 = self.df2[col].dtype\n",
    "            \n",
    "            if tipo1 != tipo2:\n",
    "                incompativeis.append({\n",
    "                    'coluna': col,\n",
    "                    'tipo_arquivo1': str(tipo1),\n",
    "                    'tipo_arquivo2': str(tipo2)\n",
    "                })\n",
    "                print(f\"‚ö†Ô∏è {col}:\")\n",
    "                print(f\"   Arquivo 1: {tipo1}\")\n",
    "                print(f\"   Arquivo 2: {tipo2}\")\n",
    "        \n",
    "        if not incompativeis:\n",
    "            print(\"‚úÖ Todos os tipos de dados s√£o compat√≠veis!\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è {len(incompativeis)} colunas com tipos diferentes\")\n",
    "            print(\"   (Ser√£o convertidas automaticamente na uni√£o)\")\n",
    "            \n",
    "        return incompativeis\n",
    "    \n",
    "    def estatisticas_detalhadas(self):\n",
    "        \"\"\"Mostra estat√≠sticas detalhadas dos dois arquivos\"\"\"\n",
    "        print(\"\\nüìà ESTAT√çSTICAS DETALHADAS:\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for i, df in enumerate([self.df1, self.df2], 1):\n",
    "            print(f\"\\nüìÅ ARQUIVO {i}:\")\n",
    "            print(\"-\" * 40)\n",
    "            print(f\"Total de linhas: {len(df):,}\")\n",
    "            print(f\"Total de colunas: {len(df.columns)}\")\n",
    "            print(f\"Total de c√©lulas: {df.size:,}\")\n",
    "            print(f\"Mem√≥ria utilizada: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
    "            \n",
    "            # Valores nulos\n",
    "            nulos = df.isnull().sum().sum()\n",
    "            if nulos > 0:\n",
    "                print(f\"Valores nulos: {nulos:,} ({nulos/df.size*100:.2f}%)\")\n",
    "            \n",
    "            # Valores duplicados\n",
    "            duplicados = df.duplicated().sum()\n",
    "            if duplicados > 0:\n",
    "                print(f\"Linhas duplicadas: {duplicados:,} ({duplicados/len(df)*100:.2f}%)\")\n",
    "            \n",
    "            # Amostra de dados\n",
    "            print(f\"\\nüîç Primeiras 3 linhas (primeiras 5 colunas):\")\n",
    "            colunas_amostra = list(df.columns)[:5]\n",
    "            print(df[colunas_amostra].head(3).to_string())\n",
    "    \n",
    "    def unir_arquivos(self, metodo='auto', remover_duplicados=True):\n",
    "        \"\"\"\n",
    "        Une os dois arquivos CSV\n",
    "        \n",
    "        Args:\n",
    "            metodo: 'auto', 'concat', 'merge', 'outer'\n",
    "            remover_duplicados: Remove linhas duplicadas ap√≥s uni√£o\n",
    "        \n",
    "        Returns:\n",
    "            Boolean indicando sucesso da opera√ß√£o\n",
    "        \"\"\"\n",
    "        if not self.compativel:\n",
    "            print(\"‚ùå Arquivos incompat√≠veis para uni√£o!\")\n",
    "            return False\n",
    "            \n",
    "        print(\"\\nüîó UNINDO ARQUIVOS...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        try:\n",
    "            if metodo == 'auto':\n",
    "                metodo = self.tipo_uniao\n",
    "            \n",
    "            # Fazer c√≥pias para n√£o modificar os originais\n",
    "            df1_copy = self.df1.copy()\n",
    "            df2_copy = self.df2.copy()\n",
    "            \n",
    "            # Adicionar coluna de origem\n",
    "            df1_copy['origem_arquivo'] = Path(self.arquivo1).name\n",
    "            df2_copy['origem_arquivo'] = Path(self.arquivo2).name\n",
    "            \n",
    "            if metodo == 'concat' or set(self.df1.columns) == set(self.df2.columns):\n",
    "                # Uni√£o simples (estruturas id√™nticas ou muito similares)\n",
    "                print(\"üìå M√©todo: Concatena√ß√£o simples (append)\")\n",
    "                self.df_merged = pd.concat([df1_copy, df2_copy], ignore_index=True, sort=False)\n",
    "                \n",
    "            else:\n",
    "                # Uni√£o com outer join (estruturas diferentes)\n",
    "                print(\"üìå M√©todo: Uni√£o externa (outer join)\")\n",
    "                self.df_merged = pd.concat([df1_copy, df2_copy], ignore_index=True, sort=False)\n",
    "            \n",
    "            print(f\"‚úÖ Uni√£o conclu√≠da!\")\n",
    "            print(f\"   Total de linhas inicial: {len(self.df_merged):,}\")\n",
    "            print(f\"   Total de colunas: {len(self.df_merged.columns)}\")\n",
    "            \n",
    "            # Remover duplicados se solicitado\n",
    "            if remover_duplicados:\n",
    "                antes = len(self.df_merged)\n",
    "                # N√£o considerar a coluna origem_arquivo para detectar duplicados\n",
    "                cols_para_verificar = [col for col in self.df_merged.columns if col != 'origem_arquivo']\n",
    "                \n",
    "                if cols_para_verificar:  # Se houver colunas para verificar\n",
    "                    self.df_merged = self.df_merged.drop_duplicates(subset=cols_para_verificar, keep='first')\n",
    "                    depois = len(self.df_merged)\n",
    "                    if antes > depois:\n",
    "                        print(f\"   üóëÔ∏è {antes - depois:,} linhas duplicadas removidas\")\n",
    "            \n",
    "            # Estat√≠sticas finais\n",
    "            print(f\"\\nüìä RESULTADO FINAL:\")\n",
    "            print(f\"   Linhas do arquivo 1: {len(self.df1):,}\")\n",
    "            print(f\"   Linhas do arquivo 2: {len(self.df2):,}\")\n",
    "            print(f\"   Linhas no arquivo unificado: {len(self.df_merged):,}\")\n",
    "            print(f\"   Colunas no arquivo unificado: {len(self.df_merged.columns)}\")\n",
    "            \n",
    "            # Mostrar distribui√ß√£o por arquivo de origem\n",
    "            print(f\"\\nüìç DISTRIBUI√á√ÉO POR ARQUIVO DE ORIGEM:\")\n",
    "            contagem_origem = self.df_merged['origem_arquivo'].value_counts()\n",
    "            for origem, count in contagem_origem.items():\n",
    "                print(f\"   ‚Ä¢ {origem}: {count:,} linhas ({count/len(self.df_merged)*100:.1f}%)\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao unir arquivos: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "    \n",
    "    def salvar_resultado(self, nome_arquivo=\"ativos e inativos_full.csv\", separador='|'):\n",
    "        \"\"\"\n",
    "        Salva o arquivo unificado\n",
    "        \n",
    "        Args:\n",
    "            nome_arquivo: Nome do arquivo de sa√≠da\n",
    "            separador: Separador a usar no arquivo de sa√≠da\n",
    "        \"\"\"\n",
    "        if self.df_merged is None:\n",
    "            print(\"‚ùå Nenhum arquivo unificado para salvar!\")\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            print(f\"\\nüíæ SALVANDO ARQUIVO UNIFICADO...\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            # Salvar com encoding UTF-8\n",
    "            self.df_merged.to_csv(nome_arquivo, sep=separador, index=False, encoding='utf-8-sig')\n",
    "            \n",
    "            # Verificar arquivo salvo\n",
    "            if Path(nome_arquivo).exists():\n",
    "                tamanho = Path(nome_arquivo).stat().st_size / 1024 / 1024\n",
    "                \n",
    "                print(f\"‚úÖ Arquivo salvo com sucesso!\")\n",
    "                print(f\"   üìÅ Nome: {nome_arquivo}\")\n",
    "                print(f\"   üìä Tamanho: {tamanho:.2f} MB\")\n",
    "                print(f\"   üìù Linhas: {len(self.df_merged):,}\")\n",
    "                print(f\"   üìã Colunas: {len(self.df_merged.columns)}\")\n",
    "                print(f\"   üî§ Separador: '{separador}'\")\n",
    "                print(f\"   üî† Encoding: UTF-8 com BOM\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao salvar arquivo: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "    \n",
    "    def executar_processo_completo(self):\n",
    "        \"\"\"Executa o processo completo de verifica√ß√£o e uni√£o\"\"\"\n",
    "        print(\"üöÄ INICIANDO PROCESSO DE VERIFICA√á√ÉO E UNI√ÉO DE CSVs\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        try:\n",
    "            # 1. Carregar arquivos\n",
    "            if not self.carregar_arquivos():\n",
    "                return False\n",
    "            \n",
    "            # 2. Analisar estrutura\n",
    "            if not self.analisar_estrutura():\n",
    "                print(\"\\n‚ùå Processo interrompido: arquivos incompat√≠veis\")\n",
    "                return False\n",
    "            \n",
    "            # 3. Verificar tipos de dados\n",
    "            self.verificar_tipos_dados()\n",
    "            \n",
    "            # 4. Mostrar estat√≠sticas\n",
    "            self.estatisticas_detalhadas()\n",
    "            \n",
    "            # 5. Unir arquivos\n",
    "            sucesso_uniao = self.unir_arquivos()\n",
    "            \n",
    "            if sucesso_uniao:\n",
    "                # 6. Salvar resultado\n",
    "                self.salvar_resultado()\n",
    "                print(\"\\n‚ú® PROCESSO CONCLU√çDO COM SUCESSO!\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"\\n‚ùå Falha na uni√£o dos arquivos\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Erro no processo: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "\n",
    "# ============================================\n",
    "# FUN√á√ïES AUXILIARES\n",
    "# ============================================\n",
    "\n",
    "def verificar_e_unir_csvs(arquivo1, arquivo2, separador='|', nome_saida=\"ativos_e_inativos_full.csv\"):\n",
    "    \"\"\"\n",
    "    Fun√ß√£o simplificada para verificar e unir dois CSVs\n",
    "    \n",
    "    Args:\n",
    "        arquivo1: Caminho do primeiro arquivo\n",
    "        arquivo2: Caminho do segundo arquivo\n",
    "        separador: Separador usado nos CSVs\n",
    "        nome_saida: Nome do arquivo de sa√≠da\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame unificado ou None se incompat√≠vel\n",
    "    \"\"\"\n",
    "    merger = CSVMerger(arquivo1, arquivo2, separador)\n",
    "    if merger.executar_processo_completo():\n",
    "        return merger.df_merged\n",
    "    return None\n",
    "\n",
    "def analisar_multiplos_csvs(*arquivos, separador='|'):\n",
    "    \"\"\"\n",
    "    Analisa m√∫ltiplos arquivos CSV para verificar compatibilidade\n",
    "    \n",
    "    Args:\n",
    "        *arquivos: Lista de caminhos de arquivos\n",
    "        separador: Separador usado nos CSVs\n",
    "    \"\"\"\n",
    "    print(\"üìä AN√ÅLISE DE M√öLTIPLOS ARQUIVOS CSV\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    dataframes = []\n",
    "    estruturas = []\n",
    "    \n",
    "    for i, arquivo in enumerate(arquivos, 1):\n",
    "        try:\n",
    "            df = pd.read_csv(arquivo, sep=separador, low_memory=False)\n",
    "            dataframes.append(df)\n",
    "            estruturas.append(set(df.columns))\n",
    "            print(f\"‚úÖ Arquivo {i}: {Path(arquivo).name}\")\n",
    "            print(f\"   Linhas: {len(df):,}, Colunas: {len(df.columns)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro no arquivo {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not estruturas:\n",
    "        print(\"‚ùå Nenhum arquivo p√¥de ser carregado\")\n",
    "        return None\n",
    "    \n",
    "    # Verificar compatibilidade geral\n",
    "    colunas_comuns = set.intersection(*estruturas) if estruturas else set()\n",
    "    print(f\"\\nüìã Colunas em comum em TODOS os arquivos: {len(colunas_comuns)}\")\n",
    "    \n",
    "    if len(colunas_comuns) > 0:\n",
    "        print(\"‚úÖ Arquivos podem ser unidos!\")\n",
    "        return dataframes\n",
    "    else:\n",
    "        print(\"‚ùå Arquivos incompat√≠veis para uni√£o\")\n",
    "        return None\n",
    "\n",
    "# ============================================\n",
    "# EXEMPLO DE USO\n",
    "# ============================================\n",
    "\n",
    "# Exemplo b√°sico - SUBSTITUA COM SEUS ARQUIVOS\n",
    "print(\"üìù EXEMPLO DE USO:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "# Para usar com seus arquivos, execute:\n",
    "\n",
    "merger = CSVMerger('arquivo1.csv', 'arquivo2.csv', separador='|')\n",
    "merger.executar_processo_completo()\n",
    "\n",
    "# Ou use a fun√ß√£o simplificada:\n",
    "df_unificado = verificar_e_unir_csvs(\n",
    "    'caminho/para/arquivo1.csv',\n",
    "    'caminho/para/arquivo2.csv',\n",
    "    separador='|',\n",
    "    nome_saida='ativos e inativos_full.csv'\n",
    ")\n",
    "\n",
    "# Para acessar o DataFrame resultado ap√≥s a execu√ß√£o:\n",
    "# df_resultado = merger.df_merged\n",
    "\"\"\")\n",
    "merger = CSVMerger(r\"D:\\____________________FAPERGS\\DADOS\\1_dados_limpos\\completo_ativos_com_sobreviveu.csv\", r\"D:\\____________________FAPERGS\\DADOS\\1_dados_limpos\\completo_inativos_nao_sobreviveu_pipe.csv\", separador='|')\n",
    "merger.executar_processo_completo()\n",
    "\n",
    "# ============================================\n",
    "# CRIAR ARQUIVOS DE EXEMPLO PARA TESTE\n",
    "# ============================================\n",
    "\n",
    "def criar_arquivos_exemplo():\n",
    "    \"\"\"Cria arquivos de exemplo para teste\"\"\"\n",
    "    \n",
    "    # Criar arquivo 1 - Ativos\n",
    "    df_ativos = pd.DataFrame({\n",
    "        'cnpj': ['06.303.914/0001-08', '11.222.333/0001-44', '99.888.777/0001-66'],\n",
    "        'username': ['gruporezor', 'empresa2', 'empresa3'],\n",
    "        'name': ['Grupo Rezor', 'Empresa 2', 'Empresa 3'],\n",
    "        'followers_count': [955, 1200, 450],\n",
    "        'status': ['ativo', 'ativo', 'ativo'],\n",
    "        'timestamp': ['2025-06-04T18:04:48+0000', '2025-06-05T10:00:00+0000', '2025-06-06T14:30:00+0000']\n",
    "    })\n",
    "    \n",
    "    # Criar arquivo 2 - Inativos\n",
    "    df_inativos = pd.DataFrame({\n",
    "        'cnpj': ['55.444.333/0001-22', '77.666.555/0001-11'],\n",
    "        'username': ['empresa4', 'empresa5'],\n",
    "        'name': ['Empresa 4', 'Empresa 5'],\n",
    "        'followers_count': [200, 150],\n",
    "        'status': ['inativo', 'inativo'],\n",
    "        'timestamp': ['2025-01-15T09:00:00+0000', '2025-02-20T11:30:00+0000']\n",
    "    })\n",
    "    \n",
    "    # Salvar arquivos de exemplo\n",
    "    df_ativos.to_csv('exemplo_ativos.csv', sep='|', index=False)\n",
    "    df_inativos.to_csv('exemplo_inativos.csv', sep='|', index=False)\n",
    "    \n",
    "    print(\"üìÅ Arquivos de exemplo criados:\")\n",
    "    print(\"   ‚Ä¢ exemplo_ativos.csv\")\n",
    "    print(\"   ‚Ä¢ exemplo_inativos.csv\")\n",
    "    print(\"\\nüöÄ Execute o c√≥digo abaixo para test√°-los:\")\n",
    "    print(\"merger = CSVMerger('exemplo_ativos.csv', 'exemplo_inativos.csv', '|')\")\n",
    "    print(\"merger.executar_processo_completo()\")\n",
    "    \n",
    "    return df_ativos, df_inativos\n",
    "\n",
    "# Descomentar para criar arquivos de exemplo\n",
    "# df_ativos, df_inativos = criar_arquivos_exemplo()\n",
    "\n",
    "print(\"\\n‚ú® C√≥digo pronto para uso! Substitua os nomes dos arquivos e execute.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
