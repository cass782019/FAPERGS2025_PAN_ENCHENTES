{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2030b25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 15:37:00,233 - INFO - ================================================================================\n",
      "2025-11-13 15:37:00,233 - INFO - EXECUTANDO PIPELINE COMPLETO\n",
      "2025-11-13 15:37:00,234 - INFO - ================================================================================\n",
      "2025-11-13 15:37:00,234 - INFO - Início: 2025-11-13 15:37:00.234555\n",
      "2025-11-13 15:37:00,235 - INFO - ================================================================================\n",
      "2025-11-13 15:37:00,236 - INFO - ETAPA 1: CARREGAMENTO DE DADOS\n",
      "2025-11-13 15:37:00,236 - INFO - ================================================================================\n",
      "2025-11-13 15:37:00,237 - INFO - Carregando dados de posts: 7_dados_unidos.csv\n",
      "2025-11-13 15:37:00,238 - INFO -   Tamanho do arquivo: 2012.86 MB\n",
      "2025-11-13 15:37:00,239 - INFO -   Usando leitura em chunks (chunk_size=10000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ╔══════════════════════════════════════════════════════════════╗\n",
      "    ║         PIPELINE ESCALÁVEL DE MACHINE LEARNING              ║\n",
      "    ║     Processamento de Datasets Grandes com e sem Posts       ║\n",
      "    ╚══════════════════════════════════════════════════════════════╝\n",
      "    \n",
      "    Este pipeline suporta 3 estratégias:\n",
      "    \n",
      "    1. UNIFIED: Cria um único dataset com imputação\n",
      "       - Melhor para: Datasets balanceados\n",
      "       - Vantagem: Simplicidade\n",
      "    \n",
      "    2. SEPARATE: Mantém datasets separados\n",
      "       - Melhor para: Extreme imbalance (seu caso)\n",
      "       - Vantagem: Modelos especializados\n",
      "    \n",
      "    3. HYBRID: Cria múltiplas versões\n",
      "       - Melhor para: Ensemble e máxima performance\n",
      "       - Vantagem: Robustez\n",
      "    \n",
      "    Executando com dados de exemplo...\n",
      "    \n",
      "\n",
      "    \n",
      "    ╔══════════════════════════════════════════════════════════════╗\n",
      "    ║                     PIPELINE CONCLUÍDO!                      ║\n",
      "    ╚══════════════════════════════════════════════════════════════╝\n",
      "    \n",
      "    Para usar com seus próprios dados:\n",
      "    \n",
      "    1. Atualize os caminhos dos arquivos no código\n",
      "    2. Escolha a estratégia (unified/separate/hybrid)\n",
      "    3. Execute o pipeline\n",
      "    \n",
      "    Exemplo:\n",
      "    \n",
      "    pipeline = MLPipelineEscalavel()\n",
      "    pipeline.executar_pipeline_completo(\n",
      "        path_posts='seus_posts.csv',\n",
      "        path_empresas='suas_empresas.csv',\n",
      "        strategy='hybrid'\n",
      "    )\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 15:37:28,477 - INFO -   Posts carregados: 2,068,457 linhas\n",
      "2025-11-13 15:37:28,478 - INFO - Carregando dados de empresas: 6_empresas_rs_porte_sobreviveu_pandemia_enchente.csv\n",
      "2025-11-13 15:37:28,478 - INFO -   Tamanho do arquivo: 197.17 MB\n",
      "2025-11-13 15:37:31,315 - INFO -   Empresas carregadas: 2,685,868 linhas\n",
      "2025-11-13 15:37:31,315 - INFO - \n",
      "Análise dos dados carregados:\n",
      "2025-11-13 15:37:33,591 - INFO -   CNPJs únicos com posts: 4,065\n",
      "2025-11-13 15:37:33,591 - INFO -   Média de posts por CNPJ: 508.8\n",
      "2025-11-13 15:37:33,592 - INFO -   Total de empresas: 2,685,868\n",
      "2025-11-13 15:37:33,594 - INFO -   Taxa sobrevivência pandemia: 35.8%\n",
      "2025-11-13 15:37:33,597 - INFO -   Taxa sobrevivência enchente: 49.6%\n",
      "2025-11-13 15:37:33,597 - INFO - \n",
      "================================================================================\n",
      "2025-11-13 15:37:33,598 - INFO - ETAPA 2: FEATURE ENGINEERING\n",
      "2025-11-13 15:37:33,598 - INFO - ================================================================================\n",
      "2025-11-13 15:37:33,599 - INFO - \n",
      "Gerando features de posts...\n",
      "2025-11-13 15:37:42,174 - INFO -   Features de posts geradas: 27 features para 4065 CNPJs\n",
      "2025-11-13 15:37:42,176 - INFO - \n",
      "Gerando features de empresas...\n",
      "2025-11-13 15:37:46,019 - INFO -   Features de empresas geradas: 15 features para 2685868 empresas\n",
      "2025-11-13 15:37:46,055 - INFO - \n",
      "================================================================================\n",
      "2025-11-13 15:37:46,056 - INFO - ETAPA 3: COMBINAÇÃO DE DATASETS\n",
      "2025-11-13 15:37:46,057 - INFO - ================================================================================\n",
      "2025-11-13 15:37:46,058 - INFO - Estratégia selecionada: HYBRID\n",
      "2025-11-13 15:37:46,059 - INFO - \n",
      "Criando datasets híbridos para ensemble...\n",
      "2025-11-13 15:37:46,060 - INFO - \n",
      "Criando dataset unificado...\n",
      "2025-11-13 15:37:48,802 - INFO -   Dataset unificado: 2685868 empresas, 43 features\n",
      "2025-11-13 15:37:48,804 - INFO -   Empresas com posts: 2,638\n",
      "2025-11-13 15:37:48,810 - INFO -   Empresas sem posts: 2,683,230\n",
      "2025-11-13 15:38:15,590 - INFO -   Salvo em: outputs\\processed_data\\dataset_unificado.csv\n",
      "2025-11-13 15:38:15,591 - INFO - \n",
      "Mantendo datasets separados...\n",
      "2025-11-13 15:38:16,055 - INFO -   Dataset COM posts: (2638, 43)\n",
      "2025-11-13 15:38:16,056 - INFO -   Dataset SEM posts: (2683230, 16)\n",
      "2025-11-13 15:38:16,112 - INFO -   Salvo: outputs\\processed_data\\dataset_com_posts.csv\n",
      "2025-11-13 15:38:26,938 - INFO -   Salvo: outputs\\processed_data\\dataset_sem_posts.csv\n",
      "2025-11-13 15:38:27,329 - INFO -   Dataset BASE (só empresas): (2685868, 16)\n",
      "2025-11-13 15:38:27,330 - INFO - \n",
      "================================================================================\n",
      "2025-11-13 15:38:27,330 - INFO - ETAPA 4: TREINAMENTO DE MODELOS\n",
      "2025-11-13 15:38:27,331 - INFO - ================================================================================\n",
      "2025-11-13 15:38:27,331 - INFO - Target: sobreviveu_pandemia\n",
      "2025-11-13 15:38:29,517 - INFO - \n",
      "--- Treinando modelo para dataset: unificado ---\n",
      "2025-11-13 15:38:30,248 - INFO -   Features após limpeza: 13\n",
      "2025-11-13 15:38:37,237 - INFO -   Dataset grande: usando LightGBM\n",
      "2025-11-13 15:38:48,012 - INFO -   AUC-ROC: 0.9998\n",
      "2025-11-13 15:38:48,023 - INFO -   Modelo salvo: outputs\\models\\modelo_unificado_sobreviveu_pandemia.joblib\n",
      "2025-11-13 15:38:48,024 - INFO - \n",
      "--- Treinando modelo para dataset: com_posts ---\n",
      "2025-11-13 15:38:48,044 - INFO -   Features após limpeza: 40\n",
      "2025-11-13 15:38:48,096 - INFO -   Dataset médio: usando XGBoost\n",
      "2025-11-13 15:38:48,316 - INFO -   AUC-ROC: 0.9913\n",
      "2025-11-13 15:38:48,322 - INFO -   Modelo salvo: outputs\\models\\modelo_com_posts_sobreviveu_pandemia.joblib\n",
      "2025-11-13 15:38:48,322 - INFO - \n",
      "--- Treinando modelo para dataset: sem_posts ---\n",
      "2025-11-13 15:38:48,836 - INFO -   Features após limpeza: 13\n",
      "2025-11-13 15:38:55,275 - INFO -   Dataset grande: usando LightGBM\n",
      "2025-11-13 15:39:03,078 - INFO -   AUC-ROC: 0.9998\n",
      "2025-11-13 15:39:03,087 - INFO -   Modelo salvo: outputs\\models\\modelo_sem_posts_sobreviveu_pandemia.joblib\n",
      "2025-11-13 15:39:03,088 - INFO - \n",
      "--- Treinando modelo para dataset: base ---\n",
      "2025-11-13 15:39:03,509 - INFO -   Features após limpeza: 13\n",
      "2025-11-13 15:39:10,197 - INFO -   Dataset grande: usando LightGBM\n",
      "2025-11-13 15:39:18,377 - INFO -   AUC-ROC: 0.9998\n",
      "2025-11-13 15:39:18,393 - INFO -   Modelo salvo: outputs\\models\\modelo_base_sobreviveu_pandemia.joblib\n",
      "2025-11-13 15:39:18,435 - INFO - \n",
      "================================================================================\n",
      "2025-11-13 15:39:18,438 - INFO - ETAPA 4: TREINAMENTO DE MODELOS\n",
      "2025-11-13 15:39:18,439 - INFO - ================================================================================\n",
      "2025-11-13 15:39:18,439 - INFO - Target: sobreviveu_enchente\n",
      "2025-11-13 15:39:18,440 - INFO - \n",
      "--- Treinando modelo para dataset: unificado ---\n",
      "2025-11-13 15:39:19,068 - INFO -   Features após limpeza: 13\n",
      "2025-11-13 15:39:25,123 - INFO -   Dataset grande: usando LightGBM\n",
      "2025-11-13 15:39:34,495 - INFO -   AUC-ROC: 0.9995\n",
      "2025-11-13 15:39:34,504 - INFO -   Modelo salvo: outputs\\models\\modelo_unificado_sobreviveu_enchente.joblib\n",
      "2025-11-13 15:39:34,504 - INFO - \n",
      "--- Treinando modelo para dataset: com_posts ---\n",
      "2025-11-13 15:39:34,521 - INFO -   Features após limpeza: 40\n",
      "2025-11-13 15:39:34,579 - INFO -   Dataset médio: usando XGBoost\n",
      "2025-11-13 15:39:34,685 - INFO -   AUC-ROC: 0.9836\n",
      "2025-11-13 15:39:34,690 - INFO -   Modelo salvo: outputs\\models\\modelo_com_posts_sobreviveu_enchente.joblib\n",
      "2025-11-13 15:39:34,691 - INFO - \n",
      "--- Treinando modelo para dataset: sem_posts ---\n",
      "2025-11-13 15:39:35,271 - INFO -   Features após limpeza: 13\n",
      "2025-11-13 15:39:41,576 - INFO -   Dataset grande: usando LightGBM\n",
      "2025-11-13 15:39:50,168 - INFO -   AUC-ROC: 0.9995\n",
      "2025-11-13 15:39:50,178 - INFO -   Modelo salvo: outputs\\models\\modelo_sem_posts_sobreviveu_enchente.joblib\n",
      "2025-11-13 15:39:50,178 - INFO - \n",
      "--- Treinando modelo para dataset: base ---\n",
      "2025-11-13 15:39:50,612 - INFO -   Features após limpeza: 13\n",
      "2025-11-13 15:39:57,530 - INFO -   Dataset grande: usando LightGBM\n",
      "2025-11-13 15:40:06,712 - INFO -   AUC-ROC: 0.9995\n",
      "2025-11-13 15:40:06,740 - INFO -   Modelo salvo: outputs\\models\\modelo_base_sobreviveu_enchente.joblib\n",
      "2025-11-13 15:40:06,774 - INFO - \n",
      "Fim: 2025-11-13 15:40:06.774489\n",
      "2025-11-13 15:40:06,775 - INFO - Pipeline executado com sucesso!\n",
      "2025-11-13 15:40:06,776 - INFO - \n",
      "================================================================================\n",
      "2025-11-13 15:40:06,777 - INFO - RESUMO DA EXECUÇÃO\n",
      "2025-11-13 15:40:06,778 - INFO - ================================================================================\n",
      "2025-11-13 15:40:06,778 - INFO - \n",
      "Modelos treinados: 4\n",
      "2025-11-13 15:40:06,779 - INFO -   - unificado_sobreviveu_enchente: AUC=0.9995\n",
      "2025-11-13 15:40:06,781 - INFO -   - com_posts_sobreviveu_enchente: AUC=0.9836\n",
      "2025-11-13 15:40:06,782 - INFO -   - sem_posts_sobreviveu_enchente: AUC=0.9995\n",
      "2025-11-13 15:40:06,782 - INFO -   - base_sobreviveu_enchente: AUC=0.9995\n",
      "2025-11-13 15:40:06,784 - INFO - \n",
      "Arquivos salvos em:\n",
      "2025-11-13 15:40:06,785 - INFO -   - Modelos: outputs\\models\n",
      "2025-11-13 15:40:06,787 - INFO -   - Datasets: outputs\\processed_data\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Pipeline Escalável de Machine Learning para Datasets Grandes\n",
    "Suporta processamento de datasets com e sem posts de qualquer tamanho\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import gc\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Configuração de logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURAÇÕES GLOBAIS\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configurações do pipeline\"\"\"\n",
    "    \n",
    "    # Caminhos dos arquivos\n",
    "    PATH_POSTS = '7_dados_unidos.csv'  # Atualizar com seu arquivo\n",
    "    PATH_EMPRESAS = '6_empresas_rs_porte_sobreviveu_pandemia_enchente.csv'  # Atualizar com seu arquivo\n",
    "    \n",
    "    # Configurações de processamento\n",
    "    CHUNK_SIZE = 10000  # Processar em chunks para economizar memória\n",
    "    USE_DASK = True  # Usar Dask para datasets muito grandes (> 1GB)\n",
    "    \n",
    "    # Estratégia de modelagem\n",
    "    STRATEGY = 'separate'  # 'unified', 'separate', 'hybrid'\n",
    "    \n",
    "    # Features\n",
    "    DROP_FEATURES = ['cnpj_basico_str', 'cnpj', 'profile_picture_url']\n",
    "    TARGET_COLS = ['sobreviveu_pandemia', 'sobreviveu_enchente']\n",
    "    \n",
    "    # Modelo\n",
    "    TEST_SIZE = 0.2\n",
    "    CV_FOLDS = 5\n",
    "    RANDOM_STATE = 42\n",
    "    \n",
    "    # Output\n",
    "    OUTPUT_DIR = Path('./outputs')\n",
    "    MODEL_DIR = OUTPUT_DIR / 'models'\n",
    "    DATA_DIR = OUTPUT_DIR / 'processed_data'\n",
    "\n",
    "# ============================================================================\n",
    "# CLASSE PRINCIPAL DO PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "class MLPipelineEscalavel:\n",
    "    \"\"\"Pipeline escalável para processar datasets grandes\"\"\"\n",
    "    \n",
    "    def __init__(self, config=Config()):\n",
    "        self.config = config\n",
    "        self.config.MODEL_DIR.mkdir(exist_ok=True, parents=True)\n",
    "        self.config.DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        self.dataset_posts = None\n",
    "        self.dataset_empresas = None\n",
    "        self.dataset_final = None\n",
    "        self.models = {}\n",
    "        \n",
    "    # ------------------------------------------------------------------------\n",
    "    # ETAPA 1: CARREGAMENTO DE DADOS\n",
    "    # ------------------------------------------------------------------------\n",
    "    \n",
    "    def carregar_dados(self, path_posts=None, path_empresas=None, sample_size=None):\n",
    "        \"\"\"\n",
    "        Carrega os datasets de forma eficiente\n",
    "        \n",
    "        Args:\n",
    "            path_posts: Caminho para arquivo com dados de posts\n",
    "            path_empresas: Caminho para arquivo com dados de empresas\n",
    "            sample_size: Se especificado, carrega apenas uma amostra (útil para testes)\n",
    "        \"\"\"\n",
    "        logger.info(\"=\" * 80)\n",
    "        logger.info(\"ETAPA 1: CARREGAMENTO DE DADOS\")\n",
    "        logger.info(\"=\" * 80)\n",
    "        \n",
    "        # Usar caminhos fornecidos ou padrão\n",
    "        path_posts = path_posts or self.config.PATH_POSTS\n",
    "        path_empresas = path_empresas or self.config.PATH_EMPRESAS\n",
    "        \n",
    "        # Carregar dados de posts\n",
    "        if Path(path_posts).exists():\n",
    "            logger.info(f\"Carregando dados de posts: {path_posts}\")\n",
    "            if sample_size:\n",
    "                self.dataset_posts = pd.read_csv(path_posts, nrows=sample_size, low_memory=False)\n",
    "            else:\n",
    "                # Carregar em chunks se arquivo muito grande\n",
    "                self.dataset_posts = self._carregar_csv_grande(path_posts)\n",
    "            logger.info(f\"  Posts carregados: {len(self.dataset_posts):,} linhas\")\n",
    "        \n",
    "        # Carregar dados de empresas\n",
    "        if Path(path_empresas).exists():\n",
    "            logger.info(f\"Carregando dados de empresas: {path_empresas}\")\n",
    "            if sample_size:\n",
    "                self.dataset_empresas = pd.read_csv(path_empresas, nrows=sample_size, low_memory=False)\n",
    "            else:\n",
    "                self.dataset_empresas = self._carregar_csv_grande(path_empresas)\n",
    "            logger.info(f\"  Empresas carregadas: {len(self.dataset_empresas):,} linhas\")\n",
    "        \n",
    "        # Análise inicial\n",
    "        self._analisar_dados_carregados()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _carregar_csv_grande(self, filepath, chunk_size=None):\n",
    "        \"\"\"Carrega CSV grande usando chunks\"\"\"\n",
    "        chunk_size = chunk_size or self.config.CHUNK_SIZE\n",
    "        \n",
    "        # Estimar tamanho do arquivo\n",
    "        file_size_mb = Path(filepath).stat().st_size / (1024 * 1024)\n",
    "        logger.info(f\"  Tamanho do arquivo: {file_size_mb:.2f} MB\")\n",
    "        \n",
    "        if file_size_mb > 500:  # Se maior que 500MB, usar chunks\n",
    "            logger.info(f\"  Usando leitura em chunks (chunk_size={chunk_size})\")\n",
    "            chunks = []\n",
    "            for chunk in pd.read_csv(filepath, chunksize=chunk_size, low_memory=False):\n",
    "                chunks.append(chunk)\n",
    "            return pd.concat(chunks, ignore_index=True)\n",
    "        else:\n",
    "            return pd.read_csv(filepath, low_memory=False)\n",
    "    \n",
    "    def _analisar_dados_carregados(self):\n",
    "        \"\"\"Análise inicial dos dados carregados\"\"\"\n",
    "        logger.info(\"\\nAnálise dos dados carregados:\")\n",
    "        \n",
    "        if self.dataset_posts is not None:\n",
    "            # Preparar CNPJs\n",
    "            self.dataset_posts['cnpj_limpo'] = (self.dataset_posts['cnpj']\n",
    "                                                .astype(str)\n",
    "                                                .str.replace('[^0-9]', '', regex=True))\n",
    "            self.dataset_posts['cnpj_basico'] = self.dataset_posts['cnpj_limpo'].str[:8]\n",
    "            \n",
    "            cnpjs_unicos_posts = self.dataset_posts['cnpj_basico'].nunique()\n",
    "            logger.info(f\"  CNPJs únicos com posts: {cnpjs_unicos_posts:,}\")\n",
    "            logger.info(f\"  Média de posts por CNPJ: {len(self.dataset_posts)/cnpjs_unicos_posts:.1f}\")\n",
    "        \n",
    "        if self.dataset_empresas is not None:\n",
    "            logger.info(f\"  Total de empresas: {len(self.dataset_empresas):,}\")\n",
    "            if 'sobreviveu_pandemia' in self.dataset_empresas.columns:\n",
    "                taxa_sobrev_pandemia = self.dataset_empresas['sobreviveu_pandemia'].mean()\n",
    "                logger.info(f\"  Taxa sobrevivência pandemia: {taxa_sobrev_pandemia:.1%}\")\n",
    "            if 'sobreviveu_enchente' in self.dataset_empresas.columns:\n",
    "                taxa_sobrev_enchente = self.dataset_empresas['sobreviveu_enchente'].mean()\n",
    "                logger.info(f\"  Taxa sobrevivência enchente: {taxa_sobrev_enchente:.1%}\")\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # ETAPA 2: FEATURE ENGINEERING\n",
    "    # ------------------------------------------------------------------------\n",
    "    \n",
    "    def gerar_features(self):\n",
    "        \"\"\"Gera features dos datasets\"\"\"\n",
    "        logger.info(\"\\n\" + \"=\" * 80)\n",
    "        logger.info(\"ETAPA 2: FEATURE ENGINEERING\")\n",
    "        logger.info(\"=\" * 80)\n",
    "        \n",
    "        features_list = []\n",
    "        \n",
    "        # Features de posts\n",
    "        if self.dataset_posts is not None:\n",
    "            features_posts = self._gerar_features_posts()\n",
    "            features_list.append(('posts', features_posts))\n",
    "        \n",
    "        # Features de empresas\n",
    "        if self.dataset_empresas is not None:\n",
    "            features_empresas = self._gerar_features_empresas()\n",
    "            features_list.append(('empresas', features_empresas))\n",
    "        \n",
    "        return features_list\n",
    "    \n",
    "    def _gerar_features_posts(self):\n",
    "        \"\"\"Gera features agregadas dos posts\"\"\"\n",
    "        logger.info(\"\\nGerando features de posts...\")\n",
    "        \n",
    "        # Converter colunas numéricas para o tipo correto\n",
    "        numeric_cols = ['followers_count', 'media_count', 'like_count']\n",
    "        for col in numeric_cols:\n",
    "            if col in self.dataset_posts.columns:\n",
    "                self.dataset_posts[col] = pd.to_numeric(self.dataset_posts[col], errors='coerce')\n",
    "        \n",
    "        # Criar features derivadas se as colunas base existirem\n",
    "        if 'caption' in self.dataset_posts.columns:\n",
    "            self.dataset_posts['caption_length'] = self.dataset_posts['caption'].fillna('').astype(str).apply(len)\n",
    "            self.dataset_posts['caption_words'] = self.dataset_posts['caption'].fillna('').astype(str).apply(lambda x: len(x.split()))\n",
    "        \n",
    "        if 'followers_count' in self.dataset_posts.columns and 'like_count' in self.dataset_posts.columns:\n",
    "            # Calcular engagement_rate se não existir\n",
    "            if 'engagement_rate' not in self.dataset_posts.columns:\n",
    "                # Garantir que as colunas são numéricas\n",
    "                followers = pd.to_numeric(self.dataset_posts['followers_count'], errors='coerce')\n",
    "                likes = pd.to_numeric(self.dataset_posts['like_count'], errors='coerce')\n",
    "                self.dataset_posts['engagement_rate'] = (\n",
    "                    (likes / (followers + 1)) * 100\n",
    "                ).replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        # Definir agregações apenas para colunas que existem\n",
    "        agg_dict = {}\n",
    "        \n",
    "        # Colunas obrigatórias (verificar se existem)\n",
    "        required_cols = {\n",
    "            'followers_count': ['mean', 'max', 'min', 'std'],\n",
    "            'media_count': ['mean', 'max', 'min'],\n",
    "            'like_count': ['sum', 'mean', 'median', 'std', 'max'],\n",
    "            'engagement_rate': ['mean', 'median', 'std', 'max'],\n",
    "            'caption_length': ['mean', 'median', 'std', 'max'],\n",
    "            'caption_words': ['mean', 'median', 'std', 'max'],\n",
    "        }\n",
    "        \n",
    "        for col, funcs in required_cols.items():\n",
    "            if col in self.dataset_posts.columns:\n",
    "                agg_dict[col] = funcs\n",
    "        \n",
    "        # Verificar qual coluna usar para agrupamento e contagem\n",
    "        groupby_col = None\n",
    "        count_col = None\n",
    "        if 'cnpj_basico' in self.dataset_posts.columns:\n",
    "            groupby_col = 'cnpj_basico'\n",
    "            count_col = 'cnpj_basico'\n",
    "        elif 'cnpj' in self.dataset_posts.columns:\n",
    "            groupby_col = 'cnpj'\n",
    "            count_col = 'cnpj'\n",
    "        else:\n",
    "            logger.warning(\"  Nenhuma coluna CNPJ encontrada para agrupamento\")\n",
    "            return None\n",
    "        \n",
    "        # Adicionar contagem de posts\n",
    "        agg_dict[count_col] = 'count'\n",
    "        \n",
    "        # Adicionar colunas opcionais se existirem\n",
    "        optional_cols = ['likes_per_media', 'biography_length', 'biography_words', \n",
    "                        'has_emoji_bio', 'year', 'month', 'day_of_week', 'hour']\n",
    "        \n",
    "        for col in optional_cols:\n",
    "            if col in self.dataset_posts.columns:\n",
    "                if col in ['year', 'month']:\n",
    "                    agg_dict[col] = ['min', 'max', 'nunique']\n",
    "                elif col in ['day_of_week', 'hour']:\n",
    "                    agg_dict[col] = ['mean', 'std']\n",
    "                elif col == 'has_emoji_bio':\n",
    "                    agg_dict[col] = ['mean']\n",
    "                else:\n",
    "                    agg_dict[col] = ['mean', 'std']\n",
    "        \n",
    "        # Verificar se há colunas para agregar\n",
    "        if len(agg_dict) == 0:\n",
    "            logger.warning(\"  Nenhuma feature pôde ser gerada\")\n",
    "            return None\n",
    "        \n",
    "        # Agregar por CNPJ\n",
    "        features = self.dataset_posts.groupby(groupby_col).agg(agg_dict)\n",
    "        \n",
    "        # Renomear colunas\n",
    "        features.columns = ['_'.join(col).replace('cnpj_count', 'total_posts').replace('cnpj_basico_count', 'total_posts') \n",
    "                           for col in features.columns]\n",
    "        \n",
    "        # Features derivadas\n",
    "        if 'total_posts' in features.columns and 'media_count_mean' in features.columns:\n",
    "            features['posts_per_media'] = features['total_posts'] / (features['media_count_mean'] + 1)\n",
    "        \n",
    "        if 'engagement_rate_std' in features.columns and 'engagement_rate_mean' in features.columns:\n",
    "            features['engagement_consistency'] = (features['engagement_rate_std'] / \n",
    "                                                 (features['engagement_rate_mean'] + 0.001))\n",
    "        \n",
    "        # Calcular tendências se possível\n",
    "        if 'year_min' in features.columns and 'year_max' in features.columns:\n",
    "            features['temporal_span_years'] = features['year_max'] - features['year_min']\n",
    "        \n",
    "        logger.info(f\"  Features de posts geradas: {features.shape[1]} features para {features.shape[0]} CNPJs\")\n",
    "        \n",
    "        # Resetar index para merge\n",
    "        features = features.reset_index()\n",
    "        if groupby_col == 'cnpj_basico':\n",
    "            features.rename(columns={'cnpj_basico': 'cnpj_basico_str'}, inplace=True)\n",
    "        elif groupby_col == 'cnpj':\n",
    "            features.rename(columns={'cnpj': 'cnpj_basico_str'}, inplace=True)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _gerar_features_empresas(self):\n",
    "        \"\"\"Gera features das empresas\"\"\"\n",
    "        logger.info(\"\\nGerando features de empresas...\")\n",
    "        \n",
    "        df = self.dataset_empresas.copy()\n",
    "        features = pd.DataFrame()\n",
    "        \n",
    "        # Identificador\n",
    "        if 'cnpj_basico' in df.columns:\n",
    "            features['cnpj_basico_str'] = df['cnpj_basico'].astype(str)\n",
    "        \n",
    "        # Features básicas\n",
    "        for col in ['porte', 'situacao_cadastral', 'motivo_situacao_cadastral']:\n",
    "            if col in df.columns:\n",
    "                features[col] = df[col]\n",
    "        \n",
    "        # CNAE\n",
    "        if 'cnae_fiscal_principal' in df.columns:\n",
    "            features['cnae_2_digitos'] = df['cnae_fiscal_principal'].astype(str).str[:2]\n",
    "            features['cnae_3_digitos'] = df['cnae_fiscal_principal'].astype(str).str[:3]\n",
    "        \n",
    "        # Features temporais\n",
    "        if 'data_inicio_atividade' in df.columns:\n",
    "            df['data_inicio_atividade'] = pd.to_datetime(df['data_inicio_atividade'], errors='coerce')\n",
    "            features['idade_empresa_anos'] = (datetime.now() - df['data_inicio_atividade']).dt.days / 365.25\n",
    "        \n",
    "        if 'data_situacao_cadastral' in df.columns:\n",
    "            df['data_situacao_cadastral'] = pd.to_datetime(\n",
    "                df['data_situacao_cadastral'].astype(str), \n",
    "                format='%Y%m%d', \n",
    "                errors='coerce'\n",
    "            )\n",
    "            features['tempo_situacao_anos'] = (datetime.now() - df['data_situacao_cadastral']).dt.days / 365.25\n",
    "        \n",
    "        # Features geográficas\n",
    "        if 'municipio' in df.columns:\n",
    "            features['municipio_codigo'] = df['municipio']\n",
    "        \n",
    "        if 'cep' in df.columns:\n",
    "            features['cep_3_digitos'] = df['cep'].astype(str).str[:3]\n",
    "        \n",
    "        # Indicadores\n",
    "        if 'situacao_cadastral' in df.columns:\n",
    "            features['empresa_ativa'] = (df['situacao_cadastral'] == 2).astype(int)\n",
    "            features['empresa_baixada'] = (df['situacao_cadastral'] == 8).astype(int)\n",
    "            features['empresa_suspensa'] = (df['situacao_cadastral'] == 4).astype(int)\n",
    "        \n",
    "        # Targets\n",
    "        for target in self.config.TARGET_COLS:\n",
    "            if target in df.columns:\n",
    "                features[target] = df[target]\n",
    "        \n",
    "        logger.info(f\"  Features de empresas geradas: {features.shape[1]} features para {features.shape[0]} empresas\")\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # ETAPA 3: COMBINAÇÃO DE DATASETS\n",
    "    # ------------------------------------------------------------------------\n",
    "    \n",
    "    def combinar_datasets(self, features_posts=None, features_empresas=None, strategy=None):\n",
    "        \"\"\"\n",
    "        Combina datasets usando diferentes estratégias\n",
    "        \n",
    "        Estratégias:\n",
    "        - 'unified': Um único dataset com imputação\n",
    "        - 'separate': Mantém datasets separados\n",
    "        - 'hybrid': Cria múltiplas versões para ensemble\n",
    "        \"\"\"\n",
    "        logger.info(\"\\n\" + \"=\" * 80)\n",
    "        logger.info(\"ETAPA 3: COMBINAÇÃO DE DATASETS\")\n",
    "        logger.info(\"=\" * 80)\n",
    "        \n",
    "        strategy = strategy or self.config.STRATEGY\n",
    "        logger.info(f\"Estratégia selecionada: {strategy.upper()}\")\n",
    "        \n",
    "        if strategy == 'unified':\n",
    "            return self._combinar_unificado(features_posts, features_empresas)\n",
    "        elif strategy == 'separate':\n",
    "            return self._combinar_separado(features_posts, features_empresas)\n",
    "        elif strategy == 'hybrid':\n",
    "            return self._combinar_hibrido(features_posts, features_empresas)\n",
    "        else:\n",
    "            raise ValueError(f\"Estratégia desconhecida: {strategy}\")\n",
    "    \n",
    "    def _combinar_unificado(self, features_posts, features_empresas):\n",
    "        \"\"\"Cria dataset unificado com imputação\"\"\"\n",
    "        logger.info(\"\\nCriando dataset unificado...\")\n",
    "        \n",
    "        # Fazer merge\n",
    "        if features_posts is not None and features_empresas is not None:\n",
    "            # Identificar empresas com posts\n",
    "            cnpjs_com_posts = set(features_posts['cnpj_basico_str'])\n",
    "            features_empresas['tem_posts'] = features_empresas['cnpj_basico_str'].isin(cnpjs_com_posts).astype(int)\n",
    "            \n",
    "            # Left join\n",
    "            dataset = features_empresas.merge(\n",
    "                features_posts,\n",
    "                on='cnpj_basico_str',\n",
    "                how='left',\n",
    "                suffixes=('', '_posts')\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"  Dataset unificado: {dataset.shape[0]} empresas, {dataset.shape[1]} features\")\n",
    "            logger.info(f\"  Empresas com posts: {dataset['tem_posts'].sum():,}\")\n",
    "            logger.info(f\"  Empresas sem posts: {(~dataset['tem_posts'].astype(bool)).sum():,}\")\n",
    "            \n",
    "            # Salvar\n",
    "            output_path = self.config.DATA_DIR / 'dataset_unificado.csv'\n",
    "            dataset.to_csv(output_path, index=False)\n",
    "            logger.info(f\"  Salvo em: {output_path}\")\n",
    "            \n",
    "            return dataset\n",
    "        \n",
    "        return features_empresas or features_posts\n",
    "    \n",
    "    def _combinar_separado(self, features_posts, features_empresas):\n",
    "        \"\"\"Mantém datasets separados\"\"\"\n",
    "        logger.info(\"\\nMantendo datasets separados...\")\n",
    "        \n",
    "        datasets = {}\n",
    "        \n",
    "        if features_posts is not None and features_empresas is not None:\n",
    "            # Dataset com posts\n",
    "            cnpjs_com_posts = set(features_posts['cnpj_basico_str'])\n",
    "            empresas_com_posts = features_empresas[\n",
    "                features_empresas['cnpj_basico_str'].isin(cnpjs_com_posts)\n",
    "            ]\n",
    "            \n",
    "            dataset_com_posts = empresas_com_posts.merge(\n",
    "                features_posts,\n",
    "                on='cnpj_basico_str',\n",
    "                how='inner'\n",
    "            )\n",
    "            \n",
    "            # Dataset sem posts\n",
    "            dataset_sem_posts = features_empresas[\n",
    "                ~features_empresas['cnpj_basico_str'].isin(cnpjs_com_posts)\n",
    "            ]\n",
    "            \n",
    "            datasets['com_posts'] = dataset_com_posts\n",
    "            datasets['sem_posts'] = dataset_sem_posts\n",
    "            \n",
    "            logger.info(f\"  Dataset COM posts: {dataset_com_posts.shape}\")\n",
    "            logger.info(f\"  Dataset SEM posts: {dataset_sem_posts.shape}\")\n",
    "            \n",
    "            # Salvar\n",
    "            for nome, df in datasets.items():\n",
    "                output_path = self.config.DATA_DIR / f'dataset_{nome}.csv'\n",
    "                df.to_csv(output_path, index=False)\n",
    "                logger.info(f\"  Salvo: {output_path}\")\n",
    "        \n",
    "        return datasets\n",
    "    \n",
    "    def _combinar_hibrido(self, features_posts, features_empresas):\n",
    "        \"\"\"Cria múltiplas versões para ensemble\"\"\"\n",
    "        logger.info(\"\\nCriando datasets híbridos para ensemble...\")\n",
    "        \n",
    "        datasets = {}\n",
    "        \n",
    "        # 1. Dataset unificado\n",
    "        datasets['unificado'] = self._combinar_unificado(features_posts, features_empresas)\n",
    "        \n",
    "        # 2. Datasets separados\n",
    "        separados = self._combinar_separado(features_posts, features_empresas)\n",
    "        if isinstance(separados, dict):\n",
    "            datasets.update(separados)\n",
    "        \n",
    "        # 3. Dataset apenas com features comuns\n",
    "        if features_empresas is not None:\n",
    "            datasets['base'] = features_empresas.copy()\n",
    "            logger.info(f\"  Dataset BASE (só empresas): {datasets['base'].shape}\")\n",
    "        \n",
    "        return datasets\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # ETAPA 4: TREINAMENTO DE MODELOS\n",
    "    # ------------------------------------------------------------------------\n",
    "    \n",
    "    def treinar_modelos(self, datasets, target='sobreviveu_pandemia'):\n",
    "        \"\"\"\n",
    "        Treina modelos de acordo com a estratégia\n",
    "        \"\"\"\n",
    "        logger.info(\"\\n\" + \"=\" * 80)\n",
    "        logger.info(\"ETAPA 4: TREINAMENTO DE MODELOS\")\n",
    "        logger.info(\"=\" * 80)\n",
    "        logger.info(f\"Target: {target}\")\n",
    "        \n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        from sklearn.impute import SimpleImputer\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        import xgboost as xgb\n",
    "        import lightgbm as lgb\n",
    "        from sklearn.metrics import roc_auc_score, classification_report\n",
    "        \n",
    "        modelos_treinados = {}\n",
    "        \n",
    "        # Se datasets é um DataFrame único\n",
    "        if isinstance(datasets, pd.DataFrame):\n",
    "            datasets = {'unico': datasets}\n",
    "        \n",
    "        # Treinar modelo para cada dataset\n",
    "        for nome_dataset, df in datasets.items():\n",
    "            if df is None or len(df) == 0:\n",
    "                continue\n",
    "                \n",
    "            logger.info(f\"\\n--- Treinando modelo para dataset: {nome_dataset} ---\")\n",
    "            \n",
    "            # Preparar dados\n",
    "            features_to_drop = self.config.DROP_FEATURES + self.config.TARGET_COLS\n",
    "            features_to_drop = [col for col in features_to_drop if col in df.columns]\n",
    "            \n",
    "            X = df.drop(columns=features_to_drop, errors='ignore')\n",
    "            \n",
    "            if target not in df.columns:\n",
    "                logger.warning(f\"  Target '{target}' não encontrado no dataset {nome_dataset}\")\n",
    "                continue\n",
    "            \n",
    "            y = df[target]\n",
    "            \n",
    "            # Remover colunas com muitos valores faltantes (> 90%)\n",
    "            missing_threshold = 0.9\n",
    "            missing_ratio = X.isnull().sum() / len(X)\n",
    "            cols_to_keep = missing_ratio[missing_ratio < missing_threshold].index\n",
    "            X = X[cols_to_keep]\n",
    "            \n",
    "            logger.info(f\"  Features após limpeza: {X.shape[1]}\")\n",
    "            \n",
    "            # Encoding de categóricas\n",
    "            from sklearn.preprocessing import LabelEncoder\n",
    "            le = LabelEncoder()\n",
    "            for col in X.select_dtypes(include=['object']).columns:\n",
    "                X[col] = X[col].fillna('missing')\n",
    "                X[col] = le.fit_transform(X[col].astype(str))\n",
    "            \n",
    "            # Imputação\n",
    "            imputer = SimpleImputer(strategy='median')\n",
    "            X = pd.DataFrame(\n",
    "                imputer.fit_transform(X),\n",
    "                columns=X.columns\n",
    "            )\n",
    "            \n",
    "            # Split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=self.config.TEST_SIZE, \n",
    "                random_state=self.config.RANDOM_STATE, \n",
    "                stratify=y if len(np.unique(y)) > 1 else None\n",
    "            )\n",
    "            \n",
    "            # Normalização\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Escolher modelo baseado no tamanho do dataset\n",
    "            if len(X_train) < 100:\n",
    "                logger.info(\"  Dataset pequeno: usando RandomForest\")\n",
    "                modelo = RandomForestClassifier(\n",
    "                    n_estimators=50,\n",
    "                    max_depth=3,\n",
    "                    random_state=self.config.RANDOM_STATE\n",
    "                )\n",
    "            elif len(X_train) < 10000:\n",
    "                logger.info(\"  Dataset médio: usando XGBoost\")\n",
    "                modelo = xgb.XGBClassifier(\n",
    "                    n_estimators=100,\n",
    "                    max_depth=5,\n",
    "                    random_state=self.config.RANDOM_STATE,\n",
    "                    use_label_encoder=False,\n",
    "                    eval_metric='logloss'\n",
    "                )\n",
    "            else:\n",
    "                logger.info(\"  Dataset grande: usando LightGBM\")\n",
    "                modelo = lgb.LGBMClassifier(\n",
    "                    n_estimators=200,\n",
    "                    max_depth=7,\n",
    "                    learning_rate=0.05,\n",
    "                    random_state=self.config.RANDOM_STATE,\n",
    "                    verbose=-1\n",
    "                )\n",
    "            \n",
    "            # Treinar\n",
    "            modelo.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Avaliar\n",
    "            y_pred = modelo.predict(X_test_scaled)\n",
    "            y_pred_proba = modelo.predict_proba(X_test_scaled)[:, 1]\n",
    "            \n",
    "            auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            logger.info(f\"  AUC-ROC: {auc:.4f}\")\n",
    "            \n",
    "            # Salvar modelo\n",
    "            modelo_info = {\n",
    "                'modelo': modelo,\n",
    "                'scaler': scaler,\n",
    "                'imputer': imputer,\n",
    "                'features': X.columns.tolist(),\n",
    "                'auc': auc,\n",
    "                'dataset': nome_dataset,\n",
    "                'target': target\n",
    "            }\n",
    "            \n",
    "            modelos_treinados[f\"{nome_dataset}_{target}\"] = modelo_info\n",
    "            \n",
    "            # Salvar em disco\n",
    "            model_path = self.config.MODEL_DIR / f\"modelo_{nome_dataset}_{target}.joblib\"\n",
    "            joblib.dump(modelo_info, model_path)\n",
    "            logger.info(f\"  Modelo salvo: {model_path}\")\n",
    "        \n",
    "        self.models = modelos_treinados\n",
    "        return modelos_treinados\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # ETAPA 5: PIPELINE COMPLETO\n",
    "    # ------------------------------------------------------------------------\n",
    "    \n",
    "    def executar_pipeline_completo(self, path_posts, path_empresas, strategy='hybrid'):\n",
    "        \"\"\"\n",
    "        Executa o pipeline completo de ponta a ponta\n",
    "        \"\"\"\n",
    "        logger.info(\"=\" * 80)\n",
    "        logger.info(\"EXECUTANDO PIPELINE COMPLETO\")\n",
    "        logger.info(\"=\" * 80)\n",
    "        logger.info(f\"Início: {datetime.now()}\")\n",
    "        \n",
    "        try:\n",
    "            # 1. Carregar dados\n",
    "            self.carregar_dados(path_posts, path_empresas)\n",
    "            \n",
    "            # 2. Gerar features\n",
    "            features = self.gerar_features()\n",
    "            features_dict = dict(features) if features else {}\n",
    "            \n",
    "            # 3. Combinar datasets\n",
    "            datasets = self.combinar_datasets(\n",
    "                features_posts=features_dict.get('posts'),\n",
    "                features_empresas=features_dict.get('empresas'),\n",
    "                strategy=strategy\n",
    "            )\n",
    "            \n",
    "            # 4. Treinar modelos para cada target\n",
    "            for target in self.config.TARGET_COLS:\n",
    "                if isinstance(datasets, pd.DataFrame):\n",
    "                    if target in datasets.columns:\n",
    "                        self.treinar_modelos(datasets, target)\n",
    "                elif isinstance(datasets, dict):\n",
    "                    # Verificar se algum dataset tem o target\n",
    "                    has_target = any(target in df.columns for df in datasets.values() if isinstance(df, pd.DataFrame))\n",
    "                    if has_target:\n",
    "                        self.treinar_modelos(datasets, target)\n",
    "            \n",
    "            logger.info(f\"\\nFim: {datetime.now()}\")\n",
    "            logger.info(\"Pipeline executado com sucesso!\")\n",
    "            \n",
    "            # Resumo\n",
    "            self._gerar_resumo()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erro no pipeline: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _gerar_resumo(self):\n",
    "        \"\"\"Gera resumo da execução\"\"\"\n",
    "        logger.info(\"\\n\" + \"=\" * 80)\n",
    "        logger.info(\"RESUMO DA EXECUÇÃO\")\n",
    "        logger.info(\"=\" * 80)\n",
    "        \n",
    "        if self.models:\n",
    "            logger.info(f\"\\nModelos treinados: {len(self.models)}\")\n",
    "            for nome, info in self.models.items():\n",
    "                logger.info(f\"  - {nome}: AUC={info['auc']:.4f}\")\n",
    "        \n",
    "        logger.info(f\"\\nArquivos salvos em:\")\n",
    "        logger.info(f\"  - Modelos: {self.config.MODEL_DIR}\")\n",
    "        logger.info(f\"  - Datasets: {self.config.DATA_DIR}\")\n",
    "\n",
    "# ============================================================================\n",
    "# FUNÇÕES AUXILIARES\n",
    "# ============================================================================\n",
    "\n",
    "def exemplo_uso_basico():\n",
    "    \"\"\"Exemplo de uso básico do pipeline\"\"\"\n",
    "    \n",
    "    # Configurar caminhos dos seus arquivos\n",
    "    PATH_POSTS = '/mnt/user-data/uploads/8_DADOS_UNIDOS_dataset_processado_10000.csv'\n",
    "    PATH_EMPRESAS = '/mnt/user-data/uploads/6_empresas_rs_porte_sobreviveu_pandemia_enchente_100000.csv'\n",
    "    \n",
    "    # Criar e executar pipeline\n",
    "    pipeline = MLPipelineEscalavel()\n",
    "    pipeline.executar_pipeline_completo(\n",
    "        path_posts=PATH_POSTS,\n",
    "        path_empresas=PATH_EMPRESAS,\n",
    "        strategy='hybrid'  # ou 'unified' ou 'separate'\n",
    "    )\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "def exemplo_uso_customizado():\n",
    "    \"\"\"Exemplo de uso customizado com controle fino\"\"\"\n",
    "    \n",
    "    # Configuração customizada\n",
    "    config = Config()\n",
    "    config.CHUNK_SIZE = 50000  # Chunks maiores para processamento mais rápido\n",
    "    config.TEST_SIZE = 0.3  # Mais dados para teste\n",
    "    \n",
    "    # Criar pipeline com configuração customizada\n",
    "    pipeline = MLPipelineEscalavel(config)\n",
    "    \n",
    "    # Executar etapas manualmente\n",
    "    pipeline.carregar_dados(\n",
    "        path_posts='seu_arquivo_posts.csv',\n",
    "        path_empresas='seu_arquivo_empresas.csv',\n",
    "        sample_size=10000  # Usar amostra para teste rápido\n",
    "    )\n",
    "    \n",
    "    # Gerar features\n",
    "    features = pipeline.gerar_features()\n",
    "    \n",
    "    # Combinar com estratégia específica\n",
    "    datasets = pipeline.combinar_datasets(strategy='unified')\n",
    "    \n",
    "    # Treinar modelos\n",
    "    modelos = pipeline.treinar_modelos(datasets, target='sobreviveu_pandemia')\n",
    "    \n",
    "    return pipeline, modelos\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUÇÃO PRINCIPAL\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\"\"\n",
    "    ╔══════════════════════════════════════════════════════════════╗\n",
    "    ║         PIPELINE ESCALÁVEL DE MACHINE LEARNING              ║\n",
    "    ║     Processamento de Datasets Grandes com e sem Posts       ║\n",
    "    ╚══════════════════════════════════════════════════════════════╝\n",
    "    \n",
    "    Este pipeline suporta 3 estratégias:\n",
    "    \n",
    "    1. UNIFIED: Cria um único dataset com imputação\n",
    "       - Melhor para: Datasets balanceados\n",
    "       - Vantagem: Simplicidade\n",
    "    \n",
    "    2. SEPARATE: Mantém datasets separados\n",
    "       - Melhor para: Extreme imbalance (seu caso)\n",
    "       - Vantagem: Modelos especializados\n",
    "    \n",
    "    3. HYBRID: Cria múltiplas versões\n",
    "       - Melhor para: Ensemble e máxima performance\n",
    "       - Vantagem: Robustez\n",
    "    \n",
    "    Executando com dados de exemplo...\n",
    "    \"\"\")\n",
    "    \n",
    "    # Executar pipeline\n",
    "    #pipeline = exemplo_uso_basico()\n",
    "    \n",
    "    print(\"\"\"\n",
    "    \n",
    "    ╔══════════════════════════════════════════════════════════════╗\n",
    "    ║                     PIPELINE CONCLUÍDO!                      ║\n",
    "    ╚══════════════════════════════════════════════════════════════╝\n",
    "    \n",
    "    Para usar com seus próprios dados:\n",
    "    \n",
    "    1. Atualize os caminhos dos arquivos no código\n",
    "    2. Escolha a estratégia (unified/separate/hybrid)\n",
    "    3. Execute o pipeline\n",
    "    \n",
    "    Exemplo:\n",
    "    \n",
    "    pipeline = MLPipelineEscalavel()\n",
    "    pipeline.executar_pipeline_completo(\n",
    "        path_posts='seus_posts.csv',\n",
    "        path_empresas='suas_empresas.csv',\n",
    "        strategy='hybrid'\n",
    "    )\n",
    "    \"\"\")\n",
    "    pipeline = MLPipelineEscalavel()\n",
    "    pipeline.executar_pipeline_completo(\n",
    "        path_posts='7_dados_unidos.csv',\n",
    "        path_empresas='6_empresas_rs_porte_sobreviveu_pandemia_enchente.csv',\n",
    "        strategy='hybrid'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
